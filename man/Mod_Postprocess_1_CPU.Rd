% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Mod_Postprocess.R
\name{Mod_Postprocess_1_CPU}
\alias{Mod_Postprocess_1_CPU}
\title{Model pipeline for Hmsc analysis (using CPU)}
\usage{
Mod_Postprocess_1_CPU(
  ModelDir = NULL,
  Hab_Abb = NULL,
  NCores = 8L,
  FromHPC = TRUE,
  EnvFile = ".env",
  Path_Hmsc = NULL,
  MemPerCpu = NULL,
  Time = NULL,
  FromJSON = FALSE,
  GPP_Dist = NULL,
  Tree = "Tree",
  Samples = 1000L,
  Thin = NULL,
  NOmega = 1000L,
  CVName = c("CV_Dist", "CV_Large"),
  N_Grid = 50L,
  UseTF = TRUE,
  TF_use_single = FALSE,
  LF_NCores = NCores,
  LF_Temp_Cleanup = TRUE,
  LF_Check = FALSE,
  Temp_Cleanup = TRUE,
  TF_Environ = NULL,
  Pred_Clamp = TRUE,
  Fix_Efforts = "q90",
  Fix_Rivers = "q90",
  Pred_NewSites = TRUE,
  NCores_VP = 3
)
}
\arguments{
\item{ModelDir}{String. Path to the root directory of the fitted models
without the trailing slash. Two folders will be created \code{Model_Fitted} and
\code{Model_Coda} to store merged model and coda objects, respectively.}

\item{Hab_Abb}{Character. Habitat abbreviation indicating the specific
\href{https://www.preslia.cz/article/pdf?id=11548}{SynHab} habitat type for
which data will be prepared. Valid values are \code{0}, \code{1}, \code{2}, \code{3}, \verb{4a},
\verb{4b}, \code{10}, \verb{12a}, \verb{12b}. For more details, see \href{https://doi.org/10.23855/preslia.2022.447}{Pysek et al.}.}

\item{NCores}{Integer specifying the number of parallel cores for
parallelization. Default: 8 cores.}

\item{FromHPC}{Logical indicating whether the work is being done from HPC, to
adjust file paths accordingly. Default: \code{TRUE}.}

\item{EnvFile}{Character. Path to the environment file containing paths to
data sources. Defaults to \code{.env}.}

\item{Path_Hmsc}{String. Path for the Hmsc-HPC.}

\item{MemPerCpu}{String. Memory per CPU allocation for the SLURM job.
Example: \verb{32G} for 32 gigabytes. Defaults to \code{NULL}. If not provided, the
function will throw an error.}

\item{Time}{String. Duration for which the job should run. Example:
\code{01:00:00} for one hour. If not provided, the function will throw an error.}

\item{FromJSON}{Logical. Indicates whether to convert loaded models from JSON
format before reading. Defaults to \code{FALSE}.}

\item{GPP_Dist}{Integer specifying the distance in \emph{kilometers} between knots
for the selected model.}

\item{Tree}{Character string specifying if phylogenetic tree was used in the
selected model. Valid values are "Tree" or "NoTree". Default is "Tree".}

\item{Samples}{Integer specifying number of MCMC samples in the selected
model. Defaults to 1000.}

\item{Thin}{Integer specifying the value for thinning in the selected model.}

\item{NOmega}{An integer specifying the number of species to be sampled for
the \code{Omega} parameter transformation. Defaults to 100.}

\item{CVName}{Character vector specifying the name of the column(s) in the
model input data (see \link{Mod_PrepData} and \link{GetCV}) to be
used to cross-validate the models. The function allows the possibility of
using more than one way of assigning grid cells into cross-validation
folders. If multiple names are provided, separate cross-validation models
will be fitted for each column. Currently, there are three cross-validation
strategies, created using the \link{Mod_PrepData}: \code{CV_SAC}, \code{CV_Dist},
and \code{CV_Large} (see \link{GetCV}). Defaults to \code{c("CV_Dist", "CV_Large")}.}

\item{N_Grid}{Integer specifying the number of points along the gradient for
continuous focal variables. Defaults to 50. See \link[Hmsc:constructGradient]{Hmsc::constructGradient}
for more details.}

\item{UseTF}{Logical indicating whether to use TensorFlow for calculations.
Defaults to TRUE.}

\item{TF_use_single}{Logical indicating whether to use single precision for
the TF calculations. Defaults to \code{FALSE}.}

\item{LF_NCores}{Integer specifying the number of cores to use for parallel
processing. Defaults to 8.}

\item{LF_Temp_Cleanup}{Logical indicating whether to delete temporary files
in the \code{Temp_Dir} after finishing the LF predictions.}

\item{LF_Check}{Logical. If TRUE, the function checks if the output files are
already created and valid. If FALSE, the function will only check if the
files exist without checking their integrity. Default is \code{FALSE}.}

\item{Temp_Cleanup}{logical, indicating whether to clean up temporary files.
Defaults to \code{TRUE}.}

\item{TF_Environ}{Character string specifying the path to the Python
environment. Defaults to NULL. This argument is required if \code{UseTF} is
TRUE.}

\item{Pred_Clamp}{Logical indicating whether to clamp the sampling efforts at
a single value. Defaults to \code{TRUE}. If \code{TRUE}, the \code{Fix_Efforts} argument
must be provided.}

\item{Fix_Efforts}{Numeric or character. Defines the value to fix sampling
efforts less than the provided value. If numeric, the value is directly
used (log\if{html}{\out{<sub>}}10\if{html}{\out{</sub>}} scale). If character, it can be \code{median}, \code{mean},
\code{max}, or \code{q90} (q0\% Quantile). Using \code{max} can reflect extreme values
caused by rare, highly sampled locations (e.g., urban centers or popular
natural reserves). While using 90\% quantile avoid such extreme grid cells
while still capturing areas with high sampling effort. This argument is
mandatory when \code{Pred_Clamp} is set to \code{TRUE}.}

\item{Fix_Rivers}{Numeric or character. Similar to \code{Fix_Efforts}, but for
fixing the length of rivers. If numeric, the value is directly used
(log\if{html}{\out{<sub>}}10\if{html}{\out{</sub>}} scale). If character, it can be \code{median}, \code{mean}, \code{max},
\code{q90} (90\% quantile). It can be also \code{NULL} for not fixing the river length
predictor. Defaults to \code{q90}.}

\item{Pred_NewSites}{Logical indicating whether to predict habitat
suitability at new sites. Default: \code{TRUE}. Note: This parameter is
temporary and will be removed in future updates.}

\item{NCores_VP}{Integer specifying the number of cores to use for variance
partitioning. Defaults to 3.}
}
\description{
This function sets up and runs an analysis pipeline for postprocessing Hmsc
models using CPU. This includes the following
\itemize{
\item Check unsuccessful models: \link{Mod_SLURM_Refit}
\item Merge chains and saving RData files: \link{Merge_Chains}
\item Convergence plots for fitted models: \link{Convergence_Plot_All},
\link{PlotGelman}, \link{Convergence_Plot}
\item Model summary: \link{Mod_Summary}
\item Plotting model parameters: \link{PlotOmegaGG}, \link{PlotBetaGG}
\item Prepare data for cross-validation and fit initial cross-validated models:
\link{Mod_CV_Fit}
\item Prepare scripts for GPU processing, including: predicting latent factors
for response curves and at new sampling units, and computing variance
partitioning: \link{RespCurv_PrepData}, \link{Predict_Maps}, \link{VarPar_Compute}.
}
}
\details{
This function must be followed by
\itemize{
\item running the \link{Mod_Prep_TF} function to prepare GPU-based TensorFlow
batch scripts for variance partitioning and latent factor prediction,
\item fitting cross-validated models by submitting respective SLURM commands.
}
}
\author{
Ahmed El-Gabbas
}
