% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mod_prepare_HPC.R, R/mod_prepare_data.R
\name{mod_inputs}
\alias{mod_inputs}
\alias{mod_prepare_HPC}
\alias{mod_prepare_data}
\title{Prepare initial models for model fitting with Hmsc-HPC}
\usage{
mod_prepare_HPC(
  hab_abb = NULL,
  directory_name = NULL,
  min_efforts_n_species = 100L,
  n_pres_per_species = 80L,
  env_file = ".env",
  GPP = TRUE,
  GPP_dists = NULL,
  GPP_save = TRUE,
  GPP_plot = TRUE,
  min_LF = NULL,
  max_LF = NULL,
  alphapw = list(Prior = NULL, Min = 20, Max = 1200, Samples = 200),
  bio_variables = c("bio3", "bio4", "bio11", "bio18", "bio19", "npp"),
  quadratic_variables = bio_variables,
  efforts_as_predictor = TRUE,
  road_rail_as_predictor = TRUE,
  habitat_as_predictor = TRUE,
  river_as_predictor = TRUE,
  n_species_per_grid = 0L,
  exclude_cultivated = TRUE,
  exclude_0_habitat = TRUE,
  CV_n_folds = 4L,
  CV_n_grids = 20L,
  CV_n_rows = 2L,
  CV_n_columns = 2L,
  CV_plot = TRUE,
  CV_SAC = FALSE,
  use_phylo_tree = TRUE,
  no_phylo_tree = FALSE,
  overwrite_rds = TRUE,
  n_cores = 8L,
  strategy = "multisession",
  MCMC_n_chains = 4L,
  MCMC_thin = NULL,
  MCMC_samples = 1000L,
  MCMC_transient_factor = 500L,
  MCMC_verbose = 200L,
  skip_fitted = TRUE,
  n_array_jobs = 210L,
  model_country = NULL,
  verbose_progress = TRUE,
  SLURM_prepare = TRUE,
  memory_per_cpu = "64G",
  job_runtime = NULL,
  job_name = NULL,
  path_Hmsc = NULL,
  check_python = FALSE,
  to_JSON = FALSE,
  precision = 64L,
  ...
)

mod_prepare_data(
  hab_abb = NULL,
  directory_name = NULL,
  min_efforts_n_species = 100L,
  exclude_cultivated = TRUE,
  exclude_0_habitat = TRUE,
  n_pres_per_species = 80L,
  env_file = ".env",
  verbose_progress = TRUE
)
}
\arguments{
\item{hab_abb}{Character. Abbreviation for the habitat type (based on
\href{https://www.preslia.cz/article/pdf?id=11548}{SynHab}) for which to prepare
data. Valid values are \code{0}, \code{1}, \code{2}, \code{3}, \verb{4a}, \verb{4b}, \code{10}, \verb{12a}, \verb{12b}.
If \code{hab_abb} = \code{0}, data is prepared irrespective of the habitat type. For
more details, see \href{https://doi.org/10.23855/preslia.2022.447}{Pysek et al.}.}

\item{directory_name}{Character. Directory name, without its parents, where
the models will be saved. This directory will be created.}

\item{min_efforts_n_species}{Integer. Minimum number of vascular plant
species per grid cell (from GBIF data) required for inclusion in the
models. This is to exclude grid cells with very little sampling efforts.
Defaults to \code{100}.}

\item{n_pres_per_species}{Integer. The minimum number of presence grid cells
for a species to be included in the analysis. The number of presence grid
cells per species is calculated after discarding grid cells with low
sampling efforts (\code{min_efforts_n_species}) and zero percentage habitat
coverage \code{exclude_0_habitat}. Defaults to \code{80}.}

\item{env_file}{Character. Path to the environment file containing paths to
data sources. Defaults to \code{.env}.}

\item{GPP}{Logical. Whether to fit spatial random effect using Gaussian
Predictive Process. Defaults to \code{TRUE}. If \code{FALSE}, non-spatial models will
be fitted.}

\item{GPP_dists}{Integer. Spacing (in kilometres) between GPP knots, as well
as the minimum allowable distance between a knot and the nearest sampling
point. The knots are generated using the \link{prepare_knots} function, and this
value is used for both \code{knotDist} and \code{minKnotDist} in
\link[Hmsc:constructKnots]{Hmsc::constructKnots}.}

\item{GPP_save}{Logical. Whether \code{RData} file. Default: \code{TRUE}.}

\item{GPP_plot}{Logical. Whether to plot the coordinates of the sampling
units and the knots in a pdf file. Default: \code{TRUE}.}

\item{min_LF, max_LF}{Integer. Minimum and maximum number of latent factors to
be used. Both default to \code{NULL} which means that the number of latent
factors will be estimated from the data. If either is provided, the
respective values will be used as arguments to \link[Hmsc:setPriors]{Hmsc::setPriors}.}

\item{alphapw}{Prior for the alpha parameter. Defaults to a list with \code{Prior = NULL}, \code{Min = 20}, \code{Max = 1200}, and \code{Samples = 200}. If \code{alphapw} is
\code{NULL} or a list with all \code{NULL} list items, the default prior will be
used. If \code{Prior} is a matrix, it will be used as the prior. If \code{Prior = NULL}, the prior will be generated using \code{Min}, \code{Max}, and \code{Samples}. \code{Min}
and \code{Max} are the minimum and maximum values of the alpha parameter (in
kilometre). \code{Samples} is the number of samples to be used in the prior.}

\item{bio_variables}{Character vector. Variables from CHELSA (bioclimatic
variables (bio1-bio19) and additional predictors (e.g., Net Primary
Productivity, npp)) to be used in the model. By default, six ecologically
relevant and minimally correlated variables are selected: c("bio3", "bio4",
"bio11", "bio18", "bio19", "npp").}

\item{quadratic_variables}{Character vector for variables for which quadratic
terms are used. Defaults to all variables of the \code{bio_variables}. If
\code{quadratic_variables} is \code{NULL}, no quadratic terms will be used.}

\item{efforts_as_predictor}{Logical. Whether to include the
(log\if{html}{\out{<sub>}}10\if{html}{\out{</sub>}}) sampling efforts as predictor to the model. Default:
\code{TRUE}.}

\item{road_rail_as_predictor}{Logical. Whether to include the
(log\if{html}{\out{<sub>}}10\if{html}{\out{</sub>}}) sum of road and railway intensity as predictor to the
model. Default: \code{TRUE}.}

\item{habitat_as_predictor}{Logical. Whether to include the
(log\if{html}{\out{<sub>}}10\if{html}{\out{</sub>}}) percentage coverage of respective habitat type per grid
cell as predictor to the model. Default: \code{TRUE}. Only valid if \code{hab_abb}
not equals to \code{0}.}

\item{river_as_predictor}{Logical. Whether to include the (log\if{html}{\out{<sub>}}10\if{html}{\out{</sub>}})
total length of rivers per grid cell as predictor to the model. Default:
\code{TRUE}. See \link{river_length} for more details.}

\item{n_species_per_grid}{Integer. Minimum number of species required for a
grid cell to be included in the analysis. This filtering occurs after
applying \code{min_efforts_n_species} (sampling effort thresholds),
\code{n_pres_per_species} (minimum species presence thresholds), and
\code{exclude_0_habitat} (exclude 0\% habitat coverage). Default (0): Includes
all grid cells. Positive value (>0): Includes only grid cells where at
least \code{n_species_per_grid} species are present.}

\item{exclude_cultivated}{Logical. Whether to exclude countries with
cultivated or casual observations per species. Defaults to \code{TRUE}.}

\item{exclude_0_habitat}{Logical. Whether to exclude grid cells with zero
percentage habitat coverage. Defaults to \code{TRUE}.}

\item{CV_n_folds}{Integer. Number of cross-validation folds. Default: 4L.}

\item{CV_n_grids}{Integer. For \code{CV_Dist} cross-validation strategy (see
\link{mod_CV_prepare}), this argument determines the size of the blocks (how
many grid cells in both directions).}

\item{CV_n_rows, CV_n_columns}{Integer. Number of rows and columns used in the
\code{CV_Large} cross-validation strategy  (see \link{mod_CV_prepare}), in which the
study area is divided into large blocks given the provided \code{CV_n_rows} and
\code{CV_n_columns} values. Both default to 2 which means to split the study
area into four large blocks at the median latitude and longitude.}

\item{CV_plot}{Logical. Indicating whether to plot the block cross-validation
folds.}

\item{CV_SAC}{Logical. Whether to use the spatial autocorrelation to
determine the block size. Defaults to \code{FALSE},}

\item{use_phylo_tree, no_phylo_tree}{Logical. Whether to fit models with
(use_phylo_tree) or without (no_phylo_tree) phylogenetic trees. Defaults
are \code{use_phylo_tree = TRUE} and \code{no_phylo_tree = FALSE}, meaning only
models with phylogenetic trees are fitted by default. At least one of
\code{use_phylo_tree} and \code{no_phylo_tree} should be \code{TRUE}.}

\item{overwrite_rds}{Logical. Whether to overwrite previously exported RDS
files for initial models. Default: \code{TRUE}.}

\item{n_cores}{Integer. Number of CPU cores to use for parallel processing.
Default: 8.}

\item{strategy}{Character. The parallel processing strategy to use. Valid
options are "sequential", "multisession" (default), "multicore", and
"cluster". See \code{\link[future:plan]{future::plan()}} and \code{\link[ecokit:set_parallel]{ecokit::set_parallel()}} for details.}

\item{MCMC_n_chains}{Integer. Number of model chains. Default: 4.}

\item{MCMC_thin}{Integer vector. Thinning value(s) in MCMC sampling. If more
than one value is provided, a separate model will be fitted at each value
of thinning.}

\item{MCMC_samples}{Integer vector. Value(s) for the number of MCMC samples.
If more than one value is provided, a separate model will be fitted at each
value of number of samples. Defaults to 1000.}

\item{MCMC_transient_factor}{Integer. Transient multiplication factor. The
value of \code{transient} will equal the multiplication of
\code{MCMC_transient_factor} and \code{MCMC_thin}. Default: 500.}

\item{MCMC_verbose}{Integer. Interval at which MCMC sampling progress is
reported. Default: \code{200}.}

\item{skip_fitted}{Logical. Whether to skip already fitted models. Default:
\code{TRUE}.}

\item{n_array_jobs}{Integer. Number of jobs per SLURM script file. In LUMI
HPC, there is a limit of 210 submitted jobs per user for the \code{small-g}
partition. This argument is used to split the jobs into multiple SLURM
scripts if needed. Default: 210. See \href{https://docs.lumi-supercomputer.eu/runjobs/scheduled-jobs/partitions}{LUMI documentation}
for more details.}

\item{model_country}{Character. Country or countries to filter observations
by. Default: \code{NULL}, which means prepare data for the whole Europe.}

\item{verbose_progress}{Logical. Whether to print a message upon successful
saving of files. Defaults to \code{FALSE}.}

\item{SLURM_prepare}{Logical. Whether to prepare SLURM command files. If
\code{TRUE} (default), the SLURM commands will be saved to disk using the
\link{mod_SLURM} function.}

\item{memory_per_cpu}{Character. Memory per CPU for the SLURM job. This value
will be assigned to the \verb{#SBATCH --mem-per-cpu=} SLURM argument. Example:
"32G" to request 32 gigabyte. Only effective if \code{SLURM_prepare = TRUE}.
Defaults to "64G".}

\item{job_runtime}{Character. Requested time for each job in the SLURM bash
arrays. Example: "01:00:00" to request an hour. Only effective if
\code{SLURM_prepare = TRUE}.}

\item{job_name}{Character. Name of the submitted job(s) for SLURM. If \code{NULL}
(Default), the job name will be prepared based on the folder path and the
\code{hab_abb} value. Only effective if \code{SLURM_prepare = TRUE}.}

\item{path_Hmsc}{Character. Directory path to \code{Hmsc-HPC} extension
installation. This will be provided as the \code{path_Hmsc} argument of the
\link{mod_SLURM} function.}

\item{check_python}{Logical. Whether to check if the Python executable
exists.}

\item{to_JSON}{Logical. Whether to convert unfitted models to JSON before
saving to RDS file. Default: \code{FALSE}.}

\item{precision}{Integer (either 32 or 64). Defines the floating-point
precision mode for \code{Hmsc-HPC} sampling (--fp 32 or --fp 64). The default is
64, which is the default precision in \code{Hmsc-HPC}.}

\item{...}{Additional parameters provided to the \link{mod_SLURM} function.}
}
\description{
The \strong{\code{mod_prepare_HPC}} function prepares input data and initialises models
for fitting with \href{https://doi.org/10.1371/journal.pcbi.1011914}{Hmsc-HPC}. It
performs multiple tasks, including data preparation, defining spatial block
cross-validation folds, generating Gaussian Predictive Process (GPP) knots
(\href{https://doi.org/10.1002/ecy.2929}{Tikhonov et al.}), initialising models,
and creating HPC execution commands. The function supports parallel
processing and offers the option to include or exclude phylogenetic tree
data.\if{html}{\out{<br/>}}\if{html}{\out{<br/>}} The \code{mod_prepare_data} function is used to prepare
habitat-specific data for Hmsc models. This function processes environmental
and species presence data, reads environment variables from a file, verifies
paths, loads and filters species data based on habitat type and minimum
presence grid cells per species, and merges various environmental layers
(e.g., CHELSA Bioclimatic variables, habitat coverage, road and railway
intensity, sampling efforts) into a single dataset. Processed data is saved
to disk as an \verb{*.RData} file.
}
\author{
Ahmed El-Gabbas
}
