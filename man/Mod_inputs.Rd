% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mod_prepare_data.R, R/mod_prepare_hpc.R
\name{mod_inputs}
\alias{mod_inputs}
\alias{mod_prepare_data}
\alias{mod_prepare_hpc}
\title{Prepare initial models for model fitting with Hmsc-HPC}
\usage{
mod_prepare_data(
  hab_abb = NULL,
  directory_name = NULL,
  min_efforts_n_species = 100L,
  exclude_cultivated = TRUE,
  exclude_0_habitat = TRUE,
  n_pres_per_species = 80L,
  env_file = ".env",
  verbose_progress = TRUE
)

mod_prepare_hpc(
  hab_abb = NULL,
  directory_name = NULL,
  min_efforts_n_species = 100L,
  n_pres_per_species = 80L,
  env_file = ".env",
  gpp = TRUE,
  gpp_dists = NULL,
  min_lf = NULL,
  max_lf = NULL,
  alphapw = list(Prior = NULL, Min = 10, Max = 1500, Samples = 101),
  efforts_as_predictor = TRUE,
  road_rail_as_predictor = TRUE,
  habitat_as_predictor = TRUE,
  river_as_predictor = FALSE,
  soil_as_predictor = TRUE,
  wetness_as_predictor = TRUE,
  bio_variables = c("bio3", "bio4", "bio11", "bio18", "bio19", "npp"),
  quadratic_variables = c("bio4", "bio11"),
  n_species_per_grid = 0L,
  exclude_cultivated = TRUE,
  exclude_0_habitat = TRUE,
  cv_n_folds = 4L,
  cv_n_grids = 20L,
  cv_n_rows = 2L,
  cv_n_columns = 2L,
  cv_sac = FALSE,
  cv_fit = list(cv_type = NULL, cv_fold = NULL, inherit_dir = NULL),
  use_phylo_tree = TRUE,
  no_phylo_tree = FALSE,
  overwrite_rds = TRUE,
  n_cores = 8L,
  strategy = "multisession",
  mcmc_n_chains = 4L,
  mcmc_thin = NULL,
  mcmc_samples = 1000L,
  mcmc_transient_factor = 500L,
  mcmc_verbose = 200L,
  skip_fitted = TRUE,
  n_array_jobs = 210L,
  model_country = NULL,
  verbose_progress = TRUE,
  slurm_prepare = TRUE,
  memory_per_cpu = "64G",
  job_runtime = NULL,
  job_name = NULL,
  path_hmsc = NULL,
  check_python = FALSE,
  to_json = FALSE,
  precision = 64L,
  ...
)
}
\arguments{
\item{hab_abb}{Character. Abbreviation for the habitat type (based on
\href{https://www.preslia.cz/article/pdf?id=11548}{SynHab}) for which to prepare
data. Valid values are \code{0}, \code{1}, \code{2}, \code{3}, \verb{4a}, \verb{4b}, \code{10}, \verb{12a}, \verb{12b}.
If \code{hab_abb} = \code{0}, data is prepared irrespective of the habitat type. For
more details, see \href{https://doi.org/10.23855/preslia.2022.447}{Pysek et al.}.}

\item{directory_name}{Character. Directory name, without its parents, where
the models will be saved. This directory will be created.}

\item{min_efforts_n_species}{Integer. Minimum number of vascular plant
species per grid cell (from GBIF data) required for inclusion in the
models. This is to exclude grid cells with very little sampling efforts.
Defaults to \code{100}.}

\item{exclude_cultivated}{Logical. Whether to exclude countries with
cultivated or casual observations per species. Defaults to \code{TRUE}.}

\item{exclude_0_habitat}{Logical. Whether to exclude grid cells with zero
percentage habitat coverage. Defaults to \code{TRUE}.}

\item{n_pres_per_species}{Integer. The minimum number of presence grid cells
for a species to be included in the analysis. The number of presence grid
cells per species is calculated after discarding grid cells with low
sampling efforts (\code{min_efforts_n_species}) and zero percentage habitat
coverage \code{exclude_0_habitat}. Defaults to \code{80}.}

\item{env_file}{Character. Path to the environment file containing paths to
data sources. Defaults to \code{.env}.}

\item{verbose_progress}{Logical. Whether to print a message upon successful
saving of files. Defaults to \code{FALSE}.}

\item{gpp}{Logical. Whether to fit spatial random effect using Gaussian
Predictive Process. Defaults to \code{TRUE}. If \code{FALSE}, non-spatial models will
be fitted.}

\item{gpp_dists}{Integer. Spacing (in kilometres) between GPP knots, as well
as the minimum allowable distance between a knot and the nearest sampling
point. The knots are generated using the \link{prepare_knots} function, and this
value is used for both \code{knotDist} and \code{minKnotDist} in
\link[Hmsc:constructKnots]{Hmsc::constructKnots}.}

\item{min_lf, max_lf}{Integer. Minimum and maximum number of latent factors to
be used. Both default to \code{NULL} which means that the number of latent
factors will be estimated from the data. If either is provided, the
respective values will be used as arguments to \link[Hmsc:setPriors]{Hmsc::setPriors}.}

\item{alphapw}{Prior for the alpha parameter. Defaults to a list with \code{Prior = NULL}, \code{Min = 10}, \code{Max = 1500}, and \code{Samples = 101}. If \code{alphapw} is
\code{NULL} or a list with all \code{NULL} list items, the default prior will be
used. If \code{Prior} is a matrix, it will be used as the prior. If \code{Prior = NULL}, the prior will be generated using the minimum and maximum values of
the alpha parameter (\code{min} and \code{max}, respectively; in kilometre) and  the
number of samples (\code{Samples}). Defaults to a prior with 101 samples ranging
from 10 to 1500 km, with the first value in the second column set to 0.5.}

\item{efforts_as_predictor}{Logical. Whether to include the
(log\if{html}{\out{<sub>}}10\if{html}{\out{</sub>}}) sampling efforts as predictor to the model. Default:
\code{TRUE}.}

\item{road_rail_as_predictor}{Logical. Whether to include the
(log\if{html}{\out{<sub>}}10\if{html}{\out{</sub>}}) sum of road and railway intensity as predictor to the
model. Default: \code{TRUE}.}

\item{habitat_as_predictor}{Logical. Whether to include the
(log\if{html}{\out{<sub>}}10\if{html}{\out{</sub>}}) percentage coverage of respective habitat type per grid
cell as predictor to the model. Default: \code{TRUE}. Only valid if \code{hab_abb}
not equals to \code{0}.}

\item{river_as_predictor}{Logical. Whether to include the (log\if{html}{\out{<sub>}}10\if{html}{\out{</sub>}})
total length of rivers per grid cell as predictor to the model. Default:
\code{FALSE}. See \link{river_length} for more details.}

\item{soil_as_predictor}{Logical. Whether to include soil bulk density at
depth of 15-30 cm as predictor to the model. Default: \code{TRUE}. See
\link{soil_density_process}.}

\item{wetness_as_predictor}{Logical. Whether to include topographic wetness
index as predictor to the model. Default: \code{TRUE}. See
\link{wetness_index_process}.}

\item{bio_variables}{Character vector. Variables from CHELSA (bioclimatic
variables (bio1-bio19) and additional predictors (e.g., Net Primary
Productivity, npp)) to be used in the model. By default, six ecologically
relevant and minimally correlated variables are selected: \code{c("bio3", "bio4", "bio11", "bio18", "bio19", "npp")}.}

\item{quadratic_variables}{Character vector for variables for which quadratic
terms are used. Defaults to \code{c("bio4", "bio11")}. If \code{quadratic_variables}
is \code{NULL}, no quadratic terms will be used.}

\item{n_species_per_grid}{Integer. Minimum number of species required for a
grid cell to be included in the analysis. This filtering occurs after
applying \code{min_efforts_n_species} (sampling effort thresholds),
\code{n_pres_per_species} (minimum species presence thresholds), and
\code{exclude_0_habitat} (exclude 0\% habitat coverage). Default (0): Includes
all grid cells. Positive value (>0): Includes only grid cells where at
least \code{n_species_per_grid} species are present.}

\item{cv_n_folds}{Integer. Number of cross-validation folds. Default: 4L.}

\item{cv_n_grids}{Integer. For \code{cv_dist} cross-validation strategy (see
\link{mod_cv_prepare}), this argument determines the size of the blocks (how
many grid cells in both directions).}

\item{cv_n_rows, cv_n_columns}{Integer. Number of rows and columns used in the
\code{cv_large} cross-validation strategy  (see \link{mod_cv_prepare}), in which the
study area is divided into large blocks given the provided \code{cv_n_rows} and
\code{cv_n_columns} values. Both default to 2 which means to split the study
area into four large blocks at the median latitude and longitude.}

\item{cv_sac}{Logical. Whether to use the spatial autocorrelation to
determine the block size. Defaults to \code{FALSE},}

\item{cv_fit}{A list with three elements determining if the current model is
for a specific cross-validation fold or for full dataset.
\itemize{
\item \code{cv_type} (character): the type of cross-validation to use. Valid
options are "cv_dist", "cv_large", and "cv_sac". Default: \code{NULL}, which
means fit models on the full dataset. This can not be \code{NULL} if \code{cv_fold}
is provided.
\item \code{cv_fold} (integer): the id of the cross-validation fold to fit.
For example, \code{cv_fold = 4} means use the fourth fold for testing. Default:
\code{NULL} which means no cross-validation is performed. This can not be \code{NULL}
if \code{cv_type} is provided.
\item \code{inherit_dir} (character): the name of the directory (without its
parents) to inherit (copy) species and cross-validation data from. Defaults
to \code{NULL}, which means that a data on species and cross-validation will be
calculated.
}}

\item{use_phylo_tree, no_phylo_tree}{Logical. Whether to fit models with
(use_phylo_tree) or without (no_phylo_tree) phylogenetic trees. Defaults
are \code{use_phylo_tree = TRUE} and \code{no_phylo_tree = FALSE}, meaning only
models with phylogenetic trees are fitted by default. At least one of
\code{use_phylo_tree} and \code{no_phylo_tree} should be \code{TRUE}.}

\item{overwrite_rds}{Logical. Whether to overwrite previously exported RDS
files for initial models. Default: \code{TRUE}.}

\item{n_cores}{Integer. Number of CPU cores to use for parallel processing.
Default: 8.}

\item{strategy}{Character. The parallel processing strategy to use. Valid
options are "sequential", "multisession" (default), "multicore", and
"cluster". See \code{\link[future:plan]{future::plan()}} and \code{\link[ecokit:set_parallel]{ecokit::set_parallel()}} for details.}

\item{mcmc_n_chains}{Integer. Number of model chains. Default: 4L.}

\item{mcmc_thin}{Integer vector. Thinning value(s) in MCMC sampling. If more
than one value is provided, a separate model will be fitted at each value
of thinning.}

\item{mcmc_samples}{Integer vector. Value(s) for the number of MCMC samples.
If more than one value is provided, a separate model will be fitted at each
value of number of samples. Defaults to 1000.}

\item{mcmc_transient_factor}{Integer. Transient multiplication factor. The
value of \code{transient} will equal the multiplication of
\code{mcmc_transient_factor} and \code{mcmc_thin}. Default: 500.}

\item{mcmc_verbose}{Integer. Interval at which MCMC sampling progress is
reported. Default: \code{200}.}

\item{skip_fitted}{Logical. Whether to skip already fitted models. Default:
\code{TRUE}.}

\item{n_array_jobs}{Integer. Number of jobs per SLURM script file. In LUMI
HPC, there is a limit of 210 submitted jobs per user for the \code{small-g}
partition. This argument is used to split the jobs into multiple SLURM
scripts if needed. Default: 210. See \href{https://docs.lumi-supercomputer.eu/runjobs/scheduled-jobs/partitions}{LUMI documentation}
for more details.}

\item{model_country}{Character. Country or countries to filter observations
by. Default: \code{NULL}, which means prepare data for the whole Europe.}

\item{slurm_prepare}{Logical. Whether to prepare SLURM command files. If
\code{TRUE} (default), the SLURM commands will be saved to disk using the
\link{mod_slurm} function.}

\item{memory_per_cpu}{Character. Memory per CPU for the SLURM job. This value
will be assigned to the \verb{#SBATCH --mem-per-cpu=} SLURM argument. Example:
"32G" to request 32 gigabyte. Only effective if \code{slurm_prepare = TRUE}.
Defaults to "64G".}

\item{job_runtime}{Character. Requested time for each job in the SLURM bash
arrays. Example: "01:00:00" to request an hour. Only effective if
\code{slurm_prepare = TRUE}.}

\item{job_name}{Character. Name of the submitted job(s) for SLURM. If \code{NULL}
(Default), the job name will be prepared based on the folder path and the
\code{hab_abb} value. Only effective if \code{slurm_prepare = TRUE}.}

\item{path_hmsc}{Character. Directory path to \code{Hmsc-HPC} extension
installation. This will be provided as the \code{path_hmsc} argument of the
\link{mod_slurm} function.}

\item{check_python}{Logical. Whether to check if the Python executable
exists.}

\item{to_json}{Logical. Whether to convert unfitted models to JSON before
saving to RDS file. Default: \code{FALSE}.}

\item{precision}{Integer. Must be either 32 or 64 (default). Defines the
floating-point precision mode for \code{Hmsc-HPC} sampling (--fp 32 or --fp 64).}

\item{...}{Additional parameters provided to the \link{mod_slurm} function.}
}
\description{
The \strong{\code{mod_prepare_hpc}} function prepares input data and initialises models
for fitting with \href{https://doi.org/10.1371/journal.pcbi.1011914}{Hmsc-HPC}. It
performs multiple tasks, including data preparation, defining spatial block
cross-validation folds, generating Gaussian Predictive Process (GPP) knots
(\href{https://doi.org/10.1002/ecy.2929}{Tikhonov et al.}), initialising models,
and creating HPC execution commands. The function supports parallel
processing and offers the option to include or exclude phylogenetic tree
data.\if{html}{\out{<br/>}}\if{html}{\out{<br/>}} The \code{mod_prepare_data} function is used to prepare
habitat-specific data for Hmsc models. This function processes environmental
and species presence data, reads environment variables from a file, verifies
paths, loads and filters species data based on habitat type and minimum
presence grid cells per species, and merges various environmental layers
(e.g., CHELSA Bioclimatic variables, habitat coverage, road and railway
intensity, sampling efforts) into a single dataset. Processed data is saved
to disk as an \verb{*.RData} file.
}
\author{
Ahmed El-Gabbas
}
