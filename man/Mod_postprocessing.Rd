% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mod_postprocess.R, R/mod_CV_postprocess.R
\name{mod_postprocessing}
\alias{mod_postprocessing}
\alias{mod_postprocess_1_CPU}
\alias{mod_prepare_TF}
\alias{mod_postprocess_2_CPU}
\alias{mod_CV_postprocess_1_CPU}
\alias{mod_CV_postprocess_2_CPU}
\title{Model pipeline for post-processing fitted Hmsc models}
\usage{
mod_postprocess_1_CPU(
  model_dir = NULL,
  hab_abb = NULL,
  n_cores = 8L,
  env_file = ".env",
  path_Hmsc = NULL,
  memory_per_cpu = "64G",
  job_runtime = NULL,
  from_JSON = FALSE,
  GPP_dist = NULL,
  use_trees = "Tree",
  MCMC_n_samples = 1000L,
  MCMC_thin = NULL,
  n_omega = 1000L,
  CV_name = c("CV_Dist", "CV_Large"),
  n_grid = 50L,
  use_TF = TRUE,
  TF_use_single = FALSE,
  LF_n_cores = n_cores,
  LF_temp_cleanup = TRUE,
  LF_check = FALSE,
  temp_cleanup = TRUE,
  TF_environ = NULL,
  clamp_pred = TRUE,
  fix_efforts = "q90",
  fix_rivers = "q90",
  pred_new_sites = TRUE,
  n_cores_VP = 3L,
  width_omega = 26,
  height_omega = 22.5,
  width_beta = 25,
  height_beta = 35
)

mod_prepare_TF(
  n_batch_files = 210L,
  env_file = ".env",
  working_directory = NULL,
  partition_name = "small-g",
  LF_runtime = "01:00:00",
  model_prefix = NULL,
  VP_runtime = "02:00:00"
)

mod_postprocess_2_CPU(
  model_dir = NULL,
  hab_abb = NULL,
  n_cores = 8L,
  env_file = ".env",
  GPP_dist = NULL,
  use_trees = "Tree",
  MCMC_n_samples = 1000L,
  MCMC_thin = NULL,
  use_TF = TRUE,
  TF_environ = NULL,
  TF_use_single = FALSE,
  LF_n_cores = n_cores,
  LF_check = FALSE,
  LF_temp_cleanup = TRUE,
  temp_cleanup = TRUE,
  n_grid = 50L,
  CC_models = c("GFDL-ESM4", "IPSL-CM6A-LR", "MPI-ESM1-2-HR", "MRI-ESM2-0",
    "UKESM1-0-LL"),
  CC_scenario = c("ssp126", "ssp370", "ssp585"),
  RC_n_cores = 8L,
  clamp_pred = TRUE,
  fix_efforts = "q90",
  fix_rivers = "q90",
  pred_new_sites = TRUE,
  tar_predictions = TRUE,
  RC_prepare = TRUE,
  RC_plot = TRUE,
  VP_prepare = TRUE,
  VP_plot = TRUE,
  predict_suitability = TRUE,
  plot_predictions = TRUE,
  plot_LF = TRUE,
  plot_internal_evaluation = TRUE
)

mod_CV_postprocess_1_CPU(
  model_dir = NULL,
  CV_names = NULL,
  n_cores = 8L,
  env_file = ".env",
  from_JSON = FALSE,
  use_TF = TRUE,
  TF_use_single = FALSE,
  TF_environ = NULL,
  LF_n_cores = n_cores,
  LF_only = TRUE,
  LF_temp_cleanup = TRUE,
  LF_check = FALSE,
  LF_runtime = "01:00:00",
  temp_cleanup = TRUE,
  n_batch_files = 210L,
  working_directory = NULL,
  partition_name = "small-g"
)

mod_CV_postprocess_2_CPU(
  model_dir = NULL,
  CV_names = NULL,
  n_cores = 8L,
  env_file = ".env",
  use_TF = TRUE,
  TF_use_single = FALSE,
  temp_cleanup = TRUE,
  LF_temp_cleanup = TRUE,
  TF_environ = NULL,
  LF_n_cores = n_cores,
  LF_check = FALSE
)
}
\arguments{
\item{model_dir}{Character. Path to the root directory of the fitted model.}

\item{hab_abb}{Character. Habitat abbreviation indicating the specific
\href{https://www.preslia.cz/article/pdf?id=11548}{SynHab} habitat type for
which data will be prepared. Valid values are \code{0}, \code{1}, \code{2}, \code{3}, \verb{4a},
\verb{4b}, \code{10}, \verb{12a}, \verb{12b}. For more details, see \href{https://doi.org/10.23855/preslia.2022.447}{Pysek et al.}.}

\item{n_cores}{Integer. Number of CPU cores to use for parallel processing.
Default: 8.}

\item{env_file}{Character. Path to the environment file containing paths to
data sources. Defaults to \code{.env}.}

\item{path_Hmsc}{Character. Path to the Hmsc-HPC installation.}

\item{memory_per_cpu}{Character. Memory allocation per CPU core. Example:
"32G" for 32 gigabytes. Defaults to "64G".}

\item{job_runtime}{Character. Maximum allowed runtime for the job. Example:
"01:00:00" for one hour. Required --- if not provided, the function throws
an error.}

\item{from_JSON}{Logical. Whether to convert loaded models from JSON format
before reading. Defaults to \code{FALSE}.}

\item{GPP_dist}{Integer. Distance in \emph{kilometers} between knots for the
selected model.}

\item{use_trees}{Character. Whether a phylogenetic tree was used in the
selected model. Accepts "Tree" (default) or "NoTree".}

\item{MCMC_thin, MCMC_n_samples}{Integer. Thinning value and the number of
MCMC samples of the selected model.}

\item{n_omega}{Integer. The number of species to be sampled for the \code{Omega}
parameter transformation. Defaults to 100.}

\item{CV_name}{Character vector. Column name(s) in the model input data to be
used to cross-validate the models (see \link{mod_prepare_data} and
\link{mod_CV_prepare}). The function allows the possibility of using more than
one way of assigning grid cells into cross-validation folders. If multiple
names are provided, separate cross-validation models will be fitted for
each cross-validation type. Currently, there are three cross-validation
strategies: \code{CV_SAC}, \code{CV_Dist}, and \code{CV_Large}. Defaults to \code{c("CV_Dist", "CV_Large")}.}

\item{n_grid}{Integer. Number of points along the gradient for continuous
focal variables. Higher values result in smoother curves. Default: 50. See
\link[Hmsc:constructGradient]{Hmsc::constructGradient} for details.}

\item{use_TF}{Logical. Whether to use TensorFlow for calculations. Defaults
to \code{TRUE}.}

\item{TF_use_single}{Logical. Whether to use single precision for the
TensorFlow calculations. Defaults to \code{FALSE}.}

\item{LF_n_cores}{Integer. Number of cores to use for parallel processing of
latent factor prediction. Defaults to 8L.}

\item{LF_temp_cleanup}{Logical. Whether to delete temporary files in the
\code{temp_dir} directory after finishing the LF predictions.}

\item{LF_check}{Logical. If \code{TRUE}, the function checks if the output files
are already created and valid. If \code{FALSE}, the function will only check if
the files exist without checking their integrity. Default is \code{FALSE}.}

\item{temp_cleanup}{Logical. Whether to clean up temporary files. Defaults to
\code{TRUE}.}

\item{TF_environ}{Character. Path to the Python environment. This argument is
required if \code{use_TF} is \code{TRUE} under Windows. Defaults to \code{NULL}.}

\item{clamp_pred}{Logical indicating whether to clamp the sampling efforts at
a single value. If \code{TRUE} (default), the \code{fix_efforts} argument must be
provided.}

\item{fix_efforts}{Numeric or character. If \code{clamp_pred = TRUE}, the sampling
efforts predictor with values U+02264 \code{fix_efforts} is fixed at
\code{fix_efforts} during predictions. If numeric, the value is directly used
(log\if{html}{\out{<sub>}}10\if{html}{\out{</sub>}} scale). If character, it can be one of \code{median}, \code{mean},
\code{max}, or \code{q90} (90\% Quantile). Using \code{max} can reflect extreme values
caused by rare, highly sampled locations (e.g., urban centers or popular
natural reserves). While using 90\% quantile avoid such extreme grid cells
while still capturing areas with high sampling effort. This argument is
mandatory when \code{clamp_pred} is set to \code{TRUE}.}

\item{fix_rivers}{Numeric or character. Similar to \code{fix_efforts}, but for
fixing the length of rivers. If numeric, the value is directly used
(log\if{html}{\out{<sub>}}10\if{html}{\out{</sub>}} scale). If character, it can be one of \code{median}, \code{mean},
\code{max}, \code{q90} (90\% quantile). It can be also \code{NULL} for not fixing the river
length predictor. Defaults to \code{q90}.}

\item{pred_new_sites}{Logical. Whether to predict habitat suitability at new
sites. Default: \code{TRUE}. Note: This parameter is temporary and will be
removed in future updates.}

\item{n_cores_VP}{Integer. Number of cores to use for variance partitioning.
Defaults to 3L.}

\item{width_omega, height_omega, width_beta, height_beta}{Integer. The width and
height of the generated heatmaps of the Omega and Beta parameters in
centimeters.}

\item{n_batch_files}{Integer. Number of output batch files to create. Must be
less than or equal to the maximum job limit of the HPC environment.}

\item{working_directory}{Character. Optionally sets the working directory in
batch scripts to this path. If \code{NULL}, the directory remains unchanged.}

\item{partition_name}{Character. Name of the partition to submit the SLURM
jobs to. Default is \code{small-g}.}

\item{LF_runtime, VP_runtime}{Character. Time limit for latent factor
prediction and variance partitioning processing jobs, respectively.
Defaults are \code{01:00:00} and \code{02:00:00} respectively.}

\item{model_prefix}{Character. Prefix for the model name. A directory named
\code{model_prefix_TF} is created in the \code{model_dir} to store the TensorFlow
running commands. Defaults to \code{NULL}. This can not be \code{NULL}.}

\item{CC_models}{Character vector. Climate models for future predictions.
Available options are \code{c("GFDL-ESM4", "IPSL-CM6A-LR", "MPI-ESM1-2-HR", "MRI-ESM2-0", "UKESM1-0-LL")} (default).}

\item{CC_scenario}{Character vector. Climate scenarios for future
predictions. Available options are: \code{c("ssp126", "ssp370", "ssp585")}
(default).}

\item{RC_n_cores}{Integer. The number of cores to use for response curve
prediction. Defaults to \code{8}.}

\item{tar_predictions}{Logical. Whether to compress the add files into a
single \verb{*.tar} file (without compression). Default: \code{TRUE}.}

\item{RC_prepare}{Logical. Whether to prepare the data for response curve
prediction (using \link{resp_curv_prepare_data}). Defaults to \code{TRUE}.}

\item{RC_plot}{Logical. Whether to plot the response curves as JPEG files
(using \link{resp_curv_plot_SR}, \link{resp_curv_plot_species}, and
\link{resp_curv_plot_species_all}). Defaults to \code{TRUE}.}

\item{VP_prepare}{Logical. Whether to prepare the data for variance
partitioning (using \link{variance_partitioning_compute}). Defaults to \code{TRUE}.}

\item{VP_plot}{Logical. Whether to plot the variance partitioning results
(using \link{variance_partitioning_plot}). Defaults to \code{TRUE}.}

\item{predict_suitability}{Logical. Whether to predict habitat suitability
across different climate options (using \link{predict_maps}). Defaults to
\code{TRUE}.}

\item{plot_predictions}{Logical. Whether to plot species and species richness
predictions as JPEG files (using \link{plot_prediction}). Defaults to \code{TRUE}.}

\item{plot_LF}{Logical. Whether to plot latent factors as JPEG files (using
\link{plot_latent_factor}). Defaults to \code{TRUE}.}

\item{plot_internal_evaluation}{Logical. Whether to compute and visualize
model internal evaluation (explanatory power) using \link{plot_evaluation}.
Defaults to \code{TRUE}.}

\item{CV_names}{Character vector. Names of cross-validation strategies to
merge, matching those used during model setup. Defaults to \code{c("CV_Dist", "CV_Large")}. The names should be one of \code{CV_Dist}, \code{CV_Large}, or
\code{CV_SAC}. Applies only to \code{mod_merge_chains_CV}.}

\item{LF_only}{Logical. Whether to predict only the latent factor. This is
useful for distributing processing load between GPU and CPU. When \code{LF_only = TRUE}, latent factor prediction needs to be computed separately on GPU.
When computations are finished on GPU, the function can later be rerun with
\code{LF_only = FALSE} (default) to predict habitat suitability using the
already-computed latent factor predictions.}
}
\description{
These functions post-process fitted Hmsc models on both CPU and GPU. The main
functions in the pipeline includes \code{mod_postprocess_1_CPU},
\code{mod_CV_postprocess_1_CPU}, \code{mod_prepare_TF}, and \code{mod_postprocess_2_CPU}.
See details for more information.
}
\details{
\strong{mod_postprocess_1_CPU}

This function performs the initial post-processing step for habitat-specific
fitted models, automating the following tasks:
\itemize{
\item check unsuccessful models: \link{mod_SLURM_refit}
\item merge chains and save R objects (fitted model object and coda object) to
\code{qs2} or \code{RData} files: \link{mod_merge_chains}
\item visualize the convergence of all model variants fitted
\link{convergence_plot_all}
\item visualize the convergence of selected model, including plotting
Gelman-Rubin-Brooks \link{plot_gelman} and \link{convergence_plot} for model
convergence diagnostics of the \code{rho}, \code{alpha}, \code{omega}, and \code{beta}
parameters.
\item extract and save model summary: \link{mod_summary}
\item plotting model parameters: \link{mod_heatmap_omega}, \link{mod_heatmap_beta}
\item prepare data for cross-validation and fit initial cross-validated models:
\link{mod_CV_fit}
\item Prepare scripts for GPU processing, including:
\itemize{
\item predicting latent factors of the response curves:
\link{resp_curv_prepare_data}
\item predicting latent factors for new sampling units: \link{predict_maps}
\item computing variance partitioning: \link{variance_partitioning_compute}
}
}\if{html}{\out{
<hr>
}}


\strong{mod_CV_postprocess_1_CPU}

This function is similar to \code{mod_postprocess_1_CPU}, but it is specifically
designed for cross-validated models. It automates merging fitted
cross-validated model chains into \code{Hmsc} model objects and prepare scripts
for latent factor prediction on \code{TensorFlow} using \link{predict_maps_CV}.\if{html}{\out{
<hr>
}}


\strong{mod_prepare_TF}

After running \code{mod_postprocess_1_CPU} for all habitat types, this function
prepares batch scripts for GPU computations of all habitat types:
\itemize{
\item for \if{html}{\out{<u>}}variance partitioning\if{html}{\out{</u>}}, the function matches all files with
the pattern \code{ "VP_.+Command.txt"} (created by \link{variance_partitioning_compute}
and merges their contents into a single file
(\code{model_prefix_TF/VP_Commands.txt}). Then, it prepares a SLURM script for
variance partitioning computations (\code{model_prefix_TF/VP_SLURM.slurm}).
\item for \if{html}{\out{<u>}}latent factor predictions\if{html}{\out{</u>}}, the function matches all files
with the pattern \code{"^LF_NewSites_Commands_.+.txt|^LF_RC_Commands_.+txt"} and
split their contents into multiple scripts at the \code{model_prefix_TF} directory
for processing as a batch job. The function prepares a SLURM script for
latent factor predictions (\code{LF_SLURM.slurm}).
}

This function is tailored for the LUMI HPC environment and assumes that the
\code{tensorflow} module is installed and correctly configured with all required
Python packages. On other HPC systems, users may need to modify the function
to load a Python virtual environment or install the required dependencies for
TensorFlow and related packages.\if{html}{\out{
<hr>
}}


\strong{mod_postprocess_2_CPU}

This function continues running the analysis pipeline for post-processing
Hmsc by automating the following steps:
\itemize{
\item process and visualize response curves: \link{response_curves}
\item predict habitat suitability across different climate options:
\link{predict_maps}
\item plot species & SR predictions as JPEG: \link{plot_prediction}
\item plot latent factors as JPEG: \link{plot_latent_factor}
\item process and visualize variance partitioning:
\link{variance_partitioning_compute} and \link{variance_partitioning_plot}
\item compute and visualizing model internal evaluation (explanatory power):
\link{plot_evaluation}
\item initiate post-processing of fitted cross-validated models: prepare
commands for latent factor predictions on GPU --- \strong{Ongoing}
}

This function should be run after:
\itemize{
\item completing \code{mod_postprocess_1_CPU} and \code{mod_prepare_TF} on CPU,
\item running \code{VP_SLURM.slurm} and \code{LF_SLURM.slurm} on GPU to process response
curves and latent factor predictions (both scripts are generated by
\code{mod_prepare_TF}).
\item submitting SLURM jobs for cross-validated model fitting.
}
}
\author{
Ahmed El-Gabbas
}
