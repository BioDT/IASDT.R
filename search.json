[{"path":"https://biodt.github.io/IASDT.R/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 Ahmed El-Gabbas, Helmholtz-Centre Environmental Research (UFZ) Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_1_overview.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"IASDT modelling workflow — 1. Overview","text":"document delineates workflow modelling distribution level invasion (species richness) naturalised alien plant species (NAPS) across Europe. invasive alien species (IAS) Digital Twin (IASDT) component European BioDT project, seeks establish Digital Twin framework biodiversity Europe. detailed exposition IASDT, refer Khan, El-Gabbas, et al. (2024). complete IASDT workflow documented Zenodo. IASDT leverages IASDT.R ecokit R packages execute comprehensive workflow encompassing data processing, model fitting, post-processing, preparation maps IASDT Shiny dashboard. IASDT.R package facilitates preparation abiotic data (e.g., climate land cover) biotic data (.e., species distribution). Model outputs IASDT made publicly available end-users stakeholders OPeNDAP cloud server.","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_1_overview.html","id":"models","dir":"Articles","previous_headings":"","what":"Models","title":"IASDT modelling workflow — 1. Overview","text":"Species distribution models constructed using Hierarchical Modelling Species Communities (HMSC) R package, hierarchical Bayesian framework incorporates spatial autocorrelation species associations. Spatial autocorrelation modelled via Gaussian Predictive Process (GPP; Tikhonov et al., 2019), offering flexible computationally efficient approach capturing spatial dependencies. Given substantial computational demands fitting spatial models European scale, utilise HMSC-HPC extension (Rahman et al., 2024) leverage GPU-based processing enhanced efficiency. Models fitted habitat level, distinct model fitted habitat type, incorporating invasive alien species (IAS) associated respective habitat type. employ habitat classification delineated Pyšek et al. (2022). fitted models eight habitat types (see table ). habitat type, predictions individual species distributions species richness generated across multiple climate scenarios; details provided abiotic data processing section. Model performance assessed using spatial block cross-validation ensure spatial independence training testing datasets.","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_1_overview.html","id":"environment-variables","dir":"Articles","previous_headings":"","what":"Environment variables","title":"IASDT modelling workflow — 1. Overview","text":"workflow necessitates configuration multiple environment variables ensure proper execution. Certain functions within IASDT.R package include env_file argument, defaults .env. table enumerates environment variables essential workflow, accompanied descriptions default values.  Next articles:↠2. Processing abiotic data↠3. Processing biotic data↠4. Model fitting↠5. Model post-processing","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_2_abiotic_data.html","id":"reference-grid","dir":"Articles","previous_headings":"","what":"Reference grid","title":"IASDT modelling workflow — 2. abiotic data","text":"workflow employs European Environment Agency (EEA) reference grid, standardized 10×10 km resolution across study area. grid utilises ETRS89-LAEA Europe coordinate reference system (CRS; EPSG:3035).","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_2_abiotic_data.html","id":"corine-land-cover-and-habitat-data","dir":"Articles","previous_headings":"","what":"Corine land cover and habitat data","title":"IASDT modelling workflow — 2. abiotic data","text":"clc_process() function manages processing Corine Land Cover (CLC) data within IASDT workflow. computes percentage coverage predominant classes per grid cell across three CLC levels, alongside EUNIS SynHab habitat classifications. custom crosswalk used transform CLC Level 3 data ecologically meaningful EUNIS SynHab habitat classes. resulting data serve following purposes: model grid selection: Grid cells least 15% land cover (default threshold) retained modelling. optionally excluding grid cells zero coverage relevant habitat type model fitting. serving potential predictor variable models.","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_2_abiotic_data.html","id":"biogeographical-regions","dir":"Articles","previous_headings":"","what":"Biogeographical regions","title":"IASDT modelling workflow — 2. abiotic data","text":"bioreg_process() function retrieves processes biogeographical regions dataset EEA. extracts names biogeographical regions corresponding reference grid cell, enabling quantification species presence across regions.","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_2_abiotic_data.html","id":"chelsa-climate-data","dir":"Articles","previous_headings":"","what":"CHELSA climate data","title":"IASDT modelling workflow — 2. abiotic data","text":"chelsa_process() function manages retrieval processing CHELSA (Climatologies High Resolution Earth’s Land Surface Areas) climate data study area across multiple climate scenarios. CHELSA delivers high-resolution global datasets encompassing range environmental variables current conditions future projections. processed data integrated species distribution models predictor variables. environmental variable, dataset encompasses 45 future scenarios, derived combination five CMIP6 climate models, three Shared Socioeconomic Pathways (SSPs), three future time periods (refer CHELSA technical specifications details). Additionally, future climate model outputs aggregated generate ensemble predictions SSP time period combination.","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_2_abiotic_data.html","id":"railways-and-roads-intensity","dir":"Articles","previous_headings":"","what":"Railways and roads intensity","title":"IASDT modelling workflow — 2. abiotic data","text":"railway_intensity() road_intensity() functions retrieve process railway data (sourced OpenRailwayMap) road data (sourced Global Roads Inventory Project; GRIP), respectively, study area. Road intensity reflects site accessibility, habitat disturbance levels, IAS dispersal potential, railway density serves proxy IAS dispersal routes. summed lengths railways roads per grid cell, transformed logarithmic scale (log10), incorporated models predictor variables.","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_2_abiotic_data.html","id":"river-length","dir":"Articles","previous_headings":"","what":"River length","title":"IASDT modelling workflow — 2. abiotic data","text":"river_length() function processes data EU-Hydro River Network Database compute river lengths categorized Strahler order grid cell. Strahler order, hierarchical classification river networks, assigns higher numbers larger, significant river segments. grid cell, function calculates cumulative length rivers given Strahler order (e.g., Strahler 5, includes rivers Strahler values 5 greater). total river length per grid cell Strahler order 5 , transformed logarithmic scale (log10), serves potential predictor variable species distribution models.","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_2_abiotic_data.html","id":"sampling-efforts","dir":"Articles","previous_headings":"","what":"Sampling efforts","title":"IASDT modelling workflow — 2. abiotic data","text":"address opportunistic bias inherent presence-data, total number vascular plant observations per grid cell Global Biodiversity Information Facility (GBIF) employed proxy sampling effort. efforts_process() function manages request, retrieval, processing sampling effort data, encompassing 275 million occurrences September 2025. Beyond total observations, function also determines number vascular plant species per grid cell. processed data support two primary applications: grid cell filtering: number species per grid cell enables optional filtering exclude areas insufficient sampling effort (e.g., grid cells fewer 100 observed vascular plant species excluded). sampling bias correction: total number observations per grid cell, logarithmic scale (log10), incorporated predictor variable models account sampling bias. mitigate bias predictions, predictor fixed constant value representing optimal sampling effort across study area, detailed Warton et al. (2013). Previous articles:↠1. OverviewNext articles:↠3. Processing biotic data↠4. Model fitting↠5. Model post-processing","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_3_biotic_data.html","id":"ias-species-list","dir":"Articles","previous_headings":"","what":"IAS species list","title":"IASDT modelling workflow — 3. biotic data","text":"recent checklist naturalised terrestrial alien plant species non-European origin retrieved FloraVeg.EU. checklist standardized GBIF taxonomic backbone ensure taxonomic consistency, resulting total 1,319 species (September, 2025). table summarises species count taxonomic class order. Occurrence data species compiled available sources detailed subsequent sections.","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_3_biotic_data.html","id":"gbif","dir":"Articles","previous_headings":"","what":"GBIF","title":"IASDT modelling workflow — 3. biotic data","text":"gbif_process() function manages retrieval, processing, visualisation current occurrence data invasive alien species (IAS) Global Biodiversity Information Facility (GBIF), totalling approximately 10.4 million occurrences September 2025. Occurrences deemed doubtful exhibiting high spatial uncertainty excluded ensure data quality. Additionally, function calculates number observations occupied grid cells per species. primary output consists gridded distribution maps species, delineating presence-data across reference grid.","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_3_biotic_data.html","id":"easin","dir":"Articles","previous_headings":"","what":"EASIN","title":"IASDT modelling workflow — 3. biotic data","text":"European Alien Species Information Network (EASIN) serves web-based platform providing spatial data approximately 14,000 alien species. easin_process() function oversees processing visualisation latest IAS occurrence data sourced EASIN. checklist vascular plants EASIN’s database retrieved via API standardized GBIF taxonomic backbone. Occurrence data taxa corresponding IAS checklist subsequently downloaded API. 34 partner organizations contributing EASIN (including GBIF), non-GBIF data incorporated models avoid redundancy. Following processing spatial filtering, dataset includes 427K observations 465 IAS September 2025. Consistent GBIF output, function generates gridded distribution maps species.","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_3_biotic_data.html","id":"elter","dir":"Articles","previous_headings":"","what":"eLTER","title":"IASDT modelling workflow — 3. biotic data","text":"Integrated European Long-Term Ecosystem, Critical Zone, Socio-Ecological Research Infrastructure (eLTER) comprises network sites dedicated collecting ecological data long-term research across European Union. Cleaned standardised data encompassed 4K observations 109 IAS (September 2025). elter_process() function processed standardised eLTER data format ready used models.","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_3_biotic_data.html","id":"species-distribution","dir":"Articles","previous_headings":"","what":"Species distribution","title":"IASDT modelling workflow — 3. biotic data","text":"naps_process() function integrates visualises species distribution data sourced GBIF, EASIN, eLTER. combines pre-processed datasets generates presence-absence rasters use species distribution models. Previous articles:↠1. Overview↠2. Processing abiotic dataNext articles:↠4. Model fitting↠5. Model post-processing","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_4_model_fitting.html","id":"model-input-data","dir":"Articles","previous_headings":"","what":"Model input data","title":"IASDT modelling workflow — 4. Model fitting","text":"primary function preparing model-fitting data initialising models mod_prepare_hpc(). structures data habitat-specific model distinct directories (e.g., datasets/processed/model_fitting/HabX, X represents habitat type). function orchestrates suite specialised sub-functions perform following tasks:  mod_prepare_data(): prepare input data modelling, key arguments including:  mod_cv_prepare(): prepare visualise options spatial-block cross-validation. cv_dist strategy, block size governed cv_n_grids argument, whereas cv_large strategy, study area partitioned larger blocks based cv_n_rows cv_n_columns arguments.  prepare_knots(): prepare visualise knot locations Gaussian Predictive Process (GPP) models, described Tikhonov et al. (2019).  mod_slurm(): generate SLURM scripts facilitate model fitting GPUs using Hmsc-HPC extension.  arguments: selection predictors:  model fitting options  n_species_per_grid: minimum number IAS per grid cell grid cell included analysis model_country: fit model specific country countries whether use phylogenetic trees: use_phylo_tree no_phylo_tree path_hmsc: directory path Hmsc-HPC extension installation slurm_prepare: whether prepare SLURM script model fitting GPU via mod_slurm()","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_4_model_fitting.html","id":"model-fitting-on-gpus","dir":"Articles","previous_headings":"","what":"Model fitting on GPUs","title":"IASDT modelling workflow — 4. Model fitting","text":"Following preparation model input data initialisation models, subsequent phase involves fitting models GPUs. habitat type, mod_prepare_hpc() function produces: python commands (commands_to_fit.txt) fitting model chains across model variants GPUs, line corresponding single chain.  one SLURM script files (bash_fit.slurm) designed submit model-fitting commands (commands_to_fit.txt) batch jobs high-performance computing (HPC) system.   Batch jobs model fitting can submitted using sbatch command, example: Previous articles:↠1. Overview↠2. Processing abiotic data↠3. Processing biotic dataNext articles:↠5. Model post-processing","code":"python3 -m hmsc.run_gibbs_sampler --input datasets/processed/model_fitting/Mod_Riv_Hab1/InitMod4HPC/InitMod_GPP120_Tree_samp1000_th200.rds --output datasets/processed/model_fitting/Mod_Riv_Hab1/model_fitting_hpc/GPP120_Tree_samp1000_th200_Chain1_post.rds --samples 1000 --transient 100000 --thin 200 --verbose 200 --chain 0 --fp 64 >& datasets/processed/model_fitting/Mod_Riv_Hab1/model_fitting_hpc/GPP120_Tree_samp1000_th200_Chain1_Progress.txt python3 -m hmsc.run_gibbs_sampler --input datasets/processed/model_fitting/Mod_Riv_Hab1/InitMod4HPC/InitMod_GPP120_Tree_samp1000_th200.rds --output datasets/processed/model_fitting/Mod_Riv_Hab1/model_fitting_hpc/GPP120_Tree_samp1000_th200_Chain2_post.rds --samples 1000 --transient 100000 --thin 200 --verbose 200 --chain 1 --fp 64 >& datasets/processed/model_fitting/Mod_Riv_Hab1/model_fitting_hpc/GPP120_Tree_samp1000_th200_Chain2_Progress.txt #!/bin/bash  # ----------------------------------------------------------- # Job array configuration # ----------------------------------------------------------- #SBATCH --job-name=Hab1 #SBATCH --ntasks=1 #SBATCH --output=datasets/processed/model_fitting/Mod_Riv_Hab1/model_fitting_hpc/JobsLog/%x-%A-%a.out #SBATCH --error=datasets/processed/model_fitting/Mod_Riv_Hab1/model_fitting_hpc/JobsLog/%x-%A-%a.out #SBATCH --account=project_465001588 #SBATCH --cpus-per-task=1 #SBATCH --gpus-per-node=1 #SBATCH --time=3-00:00:00 #SBATCH --partition=small-g #SBATCH --array=1-30  # ----------------------------------------------------------- # Job info # ----------------------------------------------------------- echo \"Start time = $(date)\" echo \"Submitting directory = \"$SLURM_SUBMIT_DIR echo \"working directory = \"$PWD echo \"Project name = \"$SLURM_JOB_ACCOUNT echo \"Job id = \"$SLURM_JOB_ID echo \"Job name = \"$SLURM_JOB_NAME echo \"memory per CPU = \"$SLURM_MEM_PER_CPU echo \"The GPU IDs of GPUs in the job allocation (if any) = \"$SLURM_JOB_GPUS echo \"Node running the job script = \"$SLURMD_NODENAME echo \"Process ID of the process started for the task = \"$SLURM_TASK_PID echo \"Dependency = \"$SLURM_JOB_DEPENDENCY echo \"Number of nodes assigned to a job = \"$SLURM_NNODES echo \"Number of tasks requested by the job = \"$SLURM_NTASKS echo \"Number of cpus per task = \"$SLURM_CPUS_PER_TASK echo \"Number of tasks in the array = \"$SLURM_ARRAY_TASK_COUNT echo \"Array's maximum ID (index) number = \"$SLURM_ARRAY_TASK_MIN echo \"Array's minimum ID (index) number = \"$SLURM_ARRAY_TASK_MAX  # ----------------------------------------------------------- # File contains bash commands for model fitting # ----------------------------------------------------------- File=datasets/processed/model_fitting/Mod_Riv_Hab1/Commands2Fit.txt  # ----------------------------------------------------------- # Loading Hmsc-HPC # ----------------------------------------------------------- source /pfs/lustrep4/scratch/project_465001857/elgabbas/Hmsc_simplify_io/setup-env.sh  # ----------------------------------------------------------- # Check GPU # ----------------------------------------------------------- export TF_CPP_MIN_LOG_LEVEL=3 PythonCheckGPU=references/LUMI_Check_GPU.py  # ----------------------------------------------------------- # Some checking # ----------------------------------------------------------- path_python=$(which python3) echo -e \"Some Checking:\\n  >>  Working directory: $PWD\\n  >>  Python path:       $path_python\\n  >>  Checking GPU:      $(python3 $PythonCheckGPU)\\n\"  # ----------------------------------------------------------- # Run array job # ----------------------------------------------------------- head -n $SLURM_ARRAY_TASK_ID $File | tail -n 1 | bash  echo \"End of program at `date`\"   # ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| # ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| # This script was created on: 2025-02-13 19:43 CET # ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| # ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| sbatch datasets/processed/model_fitting/Hab1/bash_fit.slurm"},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_5_model_postprocess.html","id":"step-1-cpu","dir":"Articles","previous_headings":"","what":"Step 1: CPU","title":"IASDT modelling workflow — 5. Model post-processing","text":"mod_postprocess_1_cpu() function begins post-processing phase habitat type automating following tasks: output files saved model_fitting_cv subdirectory. type cross-validation strategy controlled cv_name argument, defaults cv_dist cv_large. unfitted model objects saved model_init subdirectory. commands model fitting saved text files, separate file cross-validation strategy (e.g., commands_to_fit_cv_dist.txt, commands_to_fit_cv_large.txt). model fitting commands submitted batch jobs using SLURM scripts, separate script strategy (e.g., cv_bash_fit_dist.slurm, cv_bash_fit_large.slurm).","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_5_model_postprocess.html","id":"computationally-intensive-tasks-offloaded-to-gpu","dir":"Articles","previous_headings":"Step 1: CPU","what":"Computationally intensive tasks offloaded to GPU","title":"IASDT modelling workflow — 5. Model post-processing","text":"Previous attempts prepare response curve data, predict new sites, compute variance partitioning using R CPUs (UFZ Windows server LUMI HPC) limited memory constraints. result, tasks now offloaded GPU-based computations using Python TensorFlow. mod_postprocess_1_cpu() function calls following sub-functions generate necessary commands GPU execution:","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_5_model_postprocess.html","id":"preparing-commands-for-gpu-computations","dir":"Articles","previous_headings":"Step 1: CPU","what":"Preparing commands for GPU computations","title":"IASDT modelling workflow — 5. Model post-processing","text":"Predicting latent factors: Predictions latent factors response curves new sampling units performed using TensorFlow script located inst/crossprod_solve.py. tasks, corresponding R functions export multiple .qs2 .feather data files temp_pred subdirectory, essential GPU computations. Additionally, generate execution commands saved lf_rc_commands_.txt (response curves) lf_new_sites_commands_.txt (new sites).   rc_prepare_data() extends functionality Hmsc::constructGradient() Hmsc::plotGradient() enabling preparation response curve data GPUs lf_commands_only = TRUE. predictions mean coordinates (specified coordinates argument Hmsc::constructGradient()), latent factor predictions — typically memory-intensive using Hmsc::predictLatentFactor() — computed GPUs. predict_maps() sets GPU computations predictions new sites lf_only = TRUE lf_commands_only = TRUE.  Computing variance partitioning: Variance partitioning computations performed GPUs using TensorFlow scripts located inst/VP_geta.py, inst/VP_getf.py, inst/VP_gemu.py. scripts implement functionality derived Hmsc::computeVariancePartitioning(), specifically internal functions geta, getf, gemu. variance_partitioning_compute() function exports necessary files temp_vp subdirectory, including numerous .qs2 .feather files. also generates execution commands saved VP_A_Command.txt, VP_F_Command.txt, VP_mu_Command.txt.","code":"Error: File not found. Error: File not found."},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_5_model_postprocess.html","id":"combining-commands-for-gpu-computations","dir":"Articles","previous_headings":"Step 1: CPU","what":"Combining commands for GPU computations","title":"IASDT modelling workflow — 5. Model post-processing","text":"mod_postprocess_1_cpu() executed habitat types, mod_prepare_tf() function consolidates batch scripts GPU computations across habitat types: aggregates script files contain commands response curves latent factor predictions, splitting multiple scripts (tf_chunk_*.txt) batch processing. Additionally, generates SLURM script (lf_SLURM.slurm) executing latent factor predictions.   combines variance partitioning command files single VP_Commands.txt file prepares SLURM script (VP_SLURM.slurm) variance partitioning computations.","code":"#!/bin/bash  # Load TensorFlow module and configure environment ml use /appl/local/csc/modulefiles ml tensorflow export TF_CPP_MIN_LOG_LEVEL=3 export TF_ENABLE_ONEDNN_OPTS=0  # Verify GPU availability python3 -c \"import tensorflow as tf; print(\\\"Num GPUs Available:\\\", len(tf.config.list_physical_devices(\\\"GPU\\\")))\"  # 20 commands to be executed: python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch001.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch001.feather' --denom 50000 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch001.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch002.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch002.feather' --denom 1470707 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch002.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch003.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch003.feather' --denom 1485354 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch003.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch004.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch004.feather' --denom 1456061 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch004.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch005.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch005.feather' --denom 1500000 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch005.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch006.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch006.feather' --denom 1500000 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch006.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch007.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch007.feather' --denom 1426768 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch007.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch008.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch008.feather' --denom 1470707 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch008.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch009.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch009.feather' --denom 1485354 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch009.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch010.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch010.feather' --denom 1456061 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch010.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch011.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch011.feather' --denom 1470707 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch011.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch012.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch012.feather' --denom 1412121 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch012.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch013.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch013.feather' --denom 1426768 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch013.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch014.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch014.feather' --denom 1426768 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch014.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch015.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch015.feather' --denom 1441414 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch015.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch016.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch016.feather' --denom 1456061 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch016.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch017.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch017.feather' --denom 1441414 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch017.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch018.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch018.feather' --denom 1382828 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch018.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch019.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch019.feather' --denom 1485354 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch019.log 2>&1 python3 crossprod_solve.py --s1 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s1.feather' --s2 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_s2.feather' --post_eta 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_postEta_ch020.feather' --path_out 'datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch020.feather' --denom 1368182 --chunk_size 1000 --threshold_mb 2000 --solve_chunk_size 50 --verbose  >> datasets/processed/model_fitting/Mod_Riv_Hab1/temp_pred/lf_1_Test_etaPred_ch020.log 2>&1 Error: File not found. python3 VP_gemu.py --tr datasets/processed/model_fitting/Mod_Q_Hab1/temp_vp/VP_Tr.feather --gamma datasets/processed/model_fitting/Mod_Q_Hab1/temp_vp/VP_Gamma.feather --output datasets/processed/model_fitting/Mod_Q_Hab1/temp_vp/VP_Mu.feather --ncores 3 --chunk_size 50 >> datasets/processed/model_fitting/Mod_Q_Hab1/temp_vp/VP_Mu.log 2>&1 python3 VP_gemu.py --tr datasets/processed/model_fitting/Mod_Q_Hab2/temp_vp/VP_Tr.feather --gamma datasets/processed/model_fitting/Mod_Q_Hab2/temp_vp/VP_Gamma.feather --output datasets/processed/model_fitting/Mod_Q_Hab2/temp_vp/VP_Mu.feather --ncores 3 --chunk_size 50 >> datasets/processed/model_fitting/Mod_Q_Hab2/temp_vp/VP_Mu.log 2>&1 python3 VP_gemu.py --tr datasets/processed/model_fitting/Mod_Q_Hab3/temp_vp/VP_Tr.feather --gamma datasets/processed/model_fitting/Mod_Q_Hab3/temp_vp/VP_Gamma.feather --output datasets/processed/model_fitting/Mod_Q_Hab3/temp_vp/VP_Mu.feather --ncores 3 --chunk_size 50 >> datasets/processed/model_fitting/Mod_Q_Hab3/temp_vp/VP_Mu.log 2>&1 python3 VP_gemu.py --tr datasets/processed/model_fitting/Mod_Q_Hab4a/temp_vp/VP_Tr.feather --gamma datasets/processed/model_fitting/Mod_Q_Hab4a/temp_vp/VP_Gamma.feather --output datasets/processed/model_fitting/Mod_Q_Hab4a/temp_vp/VP_Mu.feather --ncores 3 --chunk_size 50 >> datasets/processed/model_fitting/Mod_Q_Hab4a/temp_vp/VP_Mu.log 2>&1 python3 VP_gemu.py --tr datasets/processed/model_fitting/Mod_Q_Hab4b/temp_vp/VP_Tr.feather --gamma datasets/processed/model_fitting/Mod_Q_Hab4b/temp_vp/VP_Gamma.feather --output datasets/processed/model_fitting/Mod_Q_Hab4b/temp_vp/VP_Mu.feather --ncores 3 --chunk_size 50 >> datasets/processed/model_fitting/Mod_Q_Hab4b/temp_vp/VP_Mu.log 2>&1 python3 VP_gemu.py --tr datasets/processed/model_fitting/Mod_Q_Hab10/temp_vp/VP_Tr.feather --gamma datasets/processed/model_fitting/Mod_Q_Hab10/temp_vp/VP_Gamma.feather --output datasets/processed/model_fitting/Mod_Q_Hab10/temp_vp/VP_Mu.feather --ncores 3 --chunk_size 50 >> datasets/processed/model_fitting/Mod_Q_Hab10/temp_vp/VP_Mu.log 2>&1 python3 VP_gemu.py --tr datasets/processed/model_fitting/Mod_Q_Hab12a/temp_vp/VP_Tr.feather --gamma datasets/processed/model_fitting/Mod_Q_Hab12a/temp_vp/VP_Gamma.feather --output datasets/processed/model_fitting/Mod_Q_Hab12a/temp_vp/VP_Mu.feather --ncores 3 --chunk_size 50 >> datasets/processed/model_fitting/Mod_Q_Hab12a/temp_vp/VP_Mu.log 2>&1 python3 VP_gemu.py --tr datasets/processed/model_fitting/Mod_Q_Hab12b/temp_vp/VP_Tr.feather --gamma datasets/processed/model_fitting/Mod_Q_Hab12b/temp_vp/VP_Gamma.feather --output datasets/processed/model_fitting/Mod_Q_Hab12b/temp_vp/VP_Mu.feather --ncores 3 --chunk_size 50 >> datasets/processed/model_fitting/Mod_Q_Hab12b/temp_vp/VP_Mu.log 2>&1 python3 VP_geta.py --tr datasets/processed/model_fitting/Mod_Q_Hab1/temp_vp/VP_Tr.feather --x datasets/processed/model_fitting/Mod_Q_Hab1/temp_vp/VP_X.feather --gamma datasets/processed/model_fitting/Mod_Q_Hab1/temp_vp/VP_Gamma.feather --output datasets/processed/model_fitting/Mod_Q_Hab1/temp_vp/VP_A.feather --ncores 3 --chunk_size 50 >> datasets/processed/model_fitting/Mod_Q_Hab1/temp_vp/VP_A.log 2>&1 python3 VP_geta.py --tr datasets/processed/model_fitting/Mod_Q_Hab2/temp_vp/VP_Tr.feather --x datasets/processed/model_fitting/Mod_Q_Hab2/temp_vp/VP_X.feather --gamma datasets/processed/model_fitting/Mod_Q_Hab2/temp_vp/VP_Gamma.feather --output datasets/processed/model_fitting/Mod_Q_Hab2/temp_vp/VP_A.feather --ncores 3 --chunk_size 50 >> datasets/processed/model_fitting/Mod_Q_Hab2/temp_vp/VP_A.log 2>&1 python3 VP_geta.py --tr datasets/processed/model_fitting/Mod_Q_Hab3/temp_vp/VP_Tr.feather --x datasets/processed/model_fitting/Mod_Q_Hab3/temp_vp/VP_X.feather --gamma datasets/processed/model_fitting/Mod_Q_Hab3/temp_vp/VP_Gamma.feather --output datasets/processed/model_fitting/Mod_Q_Hab3/temp_vp/VP_A.feather --ncores 3 --chunk_size 50 >> datasets/processed/model_fitting/Mod_Q_Hab3/temp_vp/VP_A.log 2>&1 python3 VP_geta.py --tr datasets/processed/model_fitting/Mod_Q_Hab4a/temp_vp/VP_Tr.feather --x datasets/processed/model_fitting/Mod_Q_Hab4a/temp_vp/VP_X.feather --gamma datasets/processed/model_fitting/Mod_Q_Hab4a/temp_vp/VP_Gamma.feather --output datasets/processed/model_fitting/Mod_Q_Hab4a/temp_vp/VP_A.feather --ncores 3 --chunk_size 50 >> datasets/processed/model_fitting/Mod_Q_Hab4a/temp_vp/VP_A.log 2>&1 python3 VP_geta.py --tr datasets/processed/model_fitting/Mod_Q_Hab4b/temp_vp/VP_Tr.feather --x datasets/processed/model_fitting/Mod_Q_Hab4b/temp_vp/VP_X.feather --gamma datasets/processed/model_fitting/Mod_Q_Hab4b/temp_vp/VP_Gamma.feather --output datasets/processed/model_fitting/Mod_Q_Hab4b/temp_vp/VP_A.feather --ncores 3 --chunk_size 50 >> datasets/processed/model_fitting/Mod_Q_Hab4b/temp_vp/VP_A.log 2>&1 python3 VP_geta.py --tr datasets/processed/model_fitting/Mod_Q_Hab10/temp_vp/VP_Tr.feather --x datasets/processed/model_fitting/Mod_Q_Hab10/temp_vp/VP_X.feather --gamma datasets/processed/model_fitting/Mod_Q_Hab10/temp_vp/VP_Gamma.feather --output datasets/processed/model_fitting/Mod_Q_Hab10/temp_vp/VP_A.feather --ncores 3 --chunk_size 50 >> datasets/processed/model_fitting/Mod_Q_Hab10/temp_vp/VP_A.log 2>&1 python3 VP_geta.py --tr datasets/processed/model_fitting/Mod_Q_Hab12a/temp_vp/VP_Tr.feather --x datasets/processed/model_fitting/Mod_Q_Hab12a/temp_vp/VP_X.feather --gamma datasets/processed/model_fitting/Mod_Q_Hab12a/temp_vp/VP_Gamma.feather --output datasets/processed/model_fitting/Mod_Q_Hab12a/temp_vp/VP_A.feather --ncores 3 --chunk_size 50 >> datasets/processed/model_fitting/Mod_Q_Hab12a/temp_vp/VP_A.log 2>&1 python3 VP_geta.py --tr datasets/processed/model_fitting/Mod_Q_Hab12b/temp_vp/VP_Tr.feather --x datasets/processed/model_fitting/Mod_Q_Hab12b/temp_vp/VP_X.feather --gamma datasets/processed/model_fitting/Mod_Q_Hab12b/temp_vp/VP_Gamma.feather --output datasets/processed/model_fitting/Mod_Q_Hab12b/temp_vp/VP_A.feather --ncores 3 --chunk_size 50 >> datasets/processed/model_fitting/Mod_Q_Hab12b/temp_vp/VP_A.log 2>&1 python3 VP_getf.py --x datasets/processed/model_fitting/Mod_Q_Hab1/temp_vp/VP_X.feather --beta_dir datasets/processed/model_fitting/Mod_Q_Hab1/temp_vp --output datasets/processed/model_fitting/Mod_Q_Hab1/temp_vp/VP_F.feather --ncores 3 >> datasets/processed/model_fitting/Mod_Q_Hab1/temp_vp/VP_F.log 2>&1 python3 VP_getf.py --x datasets/processed/model_fitting/Mod_Q_Hab2/temp_vp/VP_X.feather --beta_dir datasets/processed/model_fitting/Mod_Q_Hab2/temp_vp --output datasets/processed/model_fitting/Mod_Q_Hab2/temp_vp/VP_F.feather --ncores 3 >> datasets/processed/model_fitting/Mod_Q_Hab2/temp_vp/VP_F.log 2>&1 python3 VP_getf.py --x datasets/processed/model_fitting/Mod_Q_Hab3/temp_vp/VP_X.feather --beta_dir datasets/processed/model_fitting/Mod_Q_Hab3/temp_vp --output datasets/processed/model_fitting/Mod_Q_Hab3/temp_vp/VP_F.feather --ncores 3 >> datasets/processed/model_fitting/Mod_Q_Hab3/temp_vp/VP_F.log 2>&1 python3 VP_getf.py --x datasets/processed/model_fitting/Mod_Q_Hab4a/temp_vp/VP_X.feather --beta_dir datasets/processed/model_fitting/Mod_Q_Hab4a/temp_vp --output datasets/processed/model_fitting/Mod_Q_Hab4a/temp_vp/VP_F.feather --ncores 3 >> datasets/processed/model_fitting/Mod_Q_Hab4a/temp_vp/VP_F.log 2>&1 python3 VP_getf.py --x datasets/processed/model_fitting/Mod_Q_Hab4b/temp_vp/VP_X.feather --beta_dir datasets/processed/model_fitting/Mod_Q_Hab4b/temp_vp --output datasets/processed/model_fitting/Mod_Q_Hab4b/temp_vp/VP_F.feather --ncores 3 >> datasets/processed/model_fitting/Mod_Q_Hab4b/temp_vp/VP_F.log 2>&1 python3 VP_getf.py --x datasets/processed/model_fitting/Mod_Q_Hab10/temp_vp/VP_X.feather --beta_dir datasets/processed/model_fitting/Mod_Q_Hab10/temp_vp --output datasets/processed/model_fitting/Mod_Q_Hab10/temp_vp/VP_F.feather --ncores 3 >> datasets/processed/model_fitting/Mod_Q_Hab10/temp_vp/VP_F.log 2>&1 python3 VP_getf.py --x datasets/processed/model_fitting/Mod_Q_Hab12a/temp_vp/VP_X.feather --beta_dir datasets/processed/model_fitting/Mod_Q_Hab12a/temp_vp --output datasets/processed/model_fitting/Mod_Q_Hab12a/temp_vp/VP_F.feather --ncores 3 >> datasets/processed/model_fitting/Mod_Q_Hab12a/temp_vp/VP_F.log 2>&1 python3 VP_getf.py --x datasets/processed/model_fitting/Mod_Q_Hab12b/temp_vp/VP_X.feather --beta_dir datasets/processed/model_fitting/Mod_Q_Hab12b/temp_vp --output datasets/processed/model_fitting/Mod_Q_Hab12b/temp_vp/VP_F.feather --ncores 3 >> datasets/processed/model_fitting/Mod_Q_Hab12b/temp_vp/VP_F.log 2>&1 #!/bin/bash #SBATCH --job-name=VP_TF #SBATCH --ntasks=1 #SBATCH --ntasks-per-node=1 #SBATCH --account=project_465001588 #SBATCH --cpus-per-task=1 #SBATCH --gpus-per-node=1 #SBATCH --time=01:30:00 #SBATCH --partition=small-g #SBATCH --output=datasets/processed/model_fitting/Mod_Q_Hab_TF/log/%x-%A-%a.out #SBATCH --error=datasets/processed/model_fitting/Mod_Q_Hab_TF/log/%x-%A-%a.out #SBATCH --array=1-24  # File containing commands to be executed File=datasets/processed/model_fitting/Mod_Q_Hab_TF/VP_Commands.txt  # Load TensorFlow module and configure environment ml use /appl/local/csc/modulefiles ml tensorflow export TF_CPP_MIN_LOG_LEVEL=3 export TF_ENABLE_ONEDNN_OPTS=0  # Verify GPU availability python3 -c \"import tensorflow as tf; print(\\\"Num GPUs Available:\\\", len(tf.config.list_physical_devices(\\\"GPU\\\")))\"  # Run array job head -n $SLURM_ARRAY_TASK_ID $File | tail -n 1 | bash  echo End of program at `date`"},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_5_model_postprocess.html","id":"step-2-gpu","dir":"Articles","previous_headings":"","what":"Step 2: GPU","title":"IASDT modelling workflow — 5. Model post-processing","text":"step, latent factor predictions variance partitioning computed GPUs. batch jobs computations can submitted using sbatch command: Additionally, cross-validated models fitted submitting corresponding SLURM scripts cross-validation strategy:","code":"# Submit SLURM jobs for variance partitioning and latent factor predictions sbatch datasets/processed/model_fitting/Mod_Q_Hab_TF/VP_SLURM.slurm sbatch datasets/processed/model_fitting/Mod_Q_Hab_TF/lf_SLURM.slurm # Submit SLURM jobs for cross-validated model fitting # # cross-validation method \"cv_dist\" sbatch datasets/processed/model_fitting/HabX/model_fitting_cv/cv_bash_fit_dist.slurm # cross-validation method \"cv_large\" sbatch datasets/processed/model_fitting/HabX/model_fitting_cv/cv_bash_fit_large.slurm"},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_5_model_postprocess.html","id":"step-3-cpu","dir":"Articles","previous_headings":"","what":"Step 3: CPU","title":"IASDT modelling workflow — 5. Model post-processing","text":"continue post-processing fitted models CPUs, two functions need executed:","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_5_model_postprocess.html","id":"mod_postprocess_2_cpu","dir":"Articles","previous_headings":"Step 3: CPU","what":"1. mod_postprocess_2_cpu()","title":"IASDT modelling workflow — 5. Model post-processing","text":"mod_postprocess_2_cpu() function progresses post-processing pipeline HMSC models CPU automating following tasks: predicts habitat suitability across various climate scenarios. computes model’s explanatory power (internal evaluation without cross-validation) using four metrics: AUC (area ROC curve), RMSE (root mean square error), continuous Boyce index, Tjur R2. prepares GeoTIFF maps free access via IASDT OPeNDAP server IASDT Shiny app.","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_5_model_postprocess.html","id":"mod_postprocess_cv_1_cpu","dir":"Articles","previous_headings":"Step 3: CPU","what":"2. mod_postprocess_cv_1_cpu()","title":"IASDT modelling workflow — 5. Model post-processing","text":"mod_postprocess_cv_1_cpu() function begins post-processing cross-validated models CPU automating following tasks: predict_maps_cv() completed, function combines computation commands multiple text script files (tf_chunk_*.txt) model’s model_fitting_cv/lf_TF_commands subdirectory. scripts need executed GPUs using single batch job submitted via SLURM script directory, lf_SLURM.slurm.","code":""},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_5_model_postprocess.html","id":"step-4-gpu","dir":"Articles","previous_headings":"","what":"Step 4: GPU","title":"IASDT modelling workflow — 5. Model post-processing","text":"step, computation latent factors cross-validated models performed GPUs using SLURM scripts.","code":"sbatch datasets/processed/model_fitting/HabX/model_fitting_cv/lf_TF_commands/lf_SLURM.slurm"},{"path":"https://biodt.github.io/IASDT.R/articles/workflow_5_model_postprocess.html","id":"step-5-cpu","dir":"Articles","previous_headings":"","what":"Step 5: CPU","title":"IASDT modelling workflow — 5. Model post-processing","text":"final step post-processing pipeline carried CPUs using mod_postprocess_cv_2_cpu() function. function automates following tasks: Predicting habitat suitability testing cross-validation folds using predict_maps_cv() function. Computing model’s predictive power (using spatially independent testing data) function, based four metrics: AUC (area ROC curve); RMSE (root mean square error); continuous Boyce index; Tjur R2. Predictive power values evaluation metric versus mean number testing presences Explanatory versus predictive power evaluation metric Previous articles:↠1. Overview↠2. Processing abiotic data↠3. Processing biotic data↠4. Model fitting","code":""},{"path":"https://biodt.github.io/IASDT.R/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Ahmed El-Gabbas. Author, maintainer.","code":""},{"path":"https://biodt.github.io/IASDT.R/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"El-Gabbas (2025). IASDT.R: Modelling distribution invasive alien plant species Europe. doi:10.5281/zenodo.14834384, R package version 0.1.10, https://github.com/BioDT/IASDT.R.","code":"@Manual{,   title = {IASDT.R: Modelling the distribution of invasive alien plant species in Europe},   author = {Ahmed El-Gabbas},   year = {2025},   note = {R package version 0.1.10},   doi = {10.5281/zenodo.14834384},   url = {https://github.com/BioDT/IASDT.R}, }"},{"path":[]},{"path":"https://biodt.github.io/IASDT.R/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"IASDT.R","text":"IASDT.R supports invasive alien species (IAS) Digital Twin (IASDT), part EU-funded BioDT project. R package provides functions processing input data (biotic abiotic) modelling distribution naturalised alien plant species (NAPS) across Europe using joint species distribution models. information BioDT project can found link. overview IASDT, see Khan, El-Gabbas, et al. . Documentation package functions can found . Click workflow IASDT. package also depends ecokit general ecological data analysis visualisation.","code":""},{"path":"https://biodt.github.io/IASDT.R/index.html","id":"installing-the-package","dir":"","previous_headings":"","what":"Installing the package","title":"IASDT.R","text":"install -recent development version package, use following command: update package, use: using renv, update package :","code":"remotes::install_github(repo = \"BioDT/IASDT.R\", dependencies = TRUE) remotes::update_packages(\"IASDT.R\", dependencies = TRUE) renv::update(\"IASDT.R\", prompt = FALSE)"},{"path":"https://biodt.github.io/IASDT.R/index.html","id":"contribute-to-the-package","dir":"","previous_headings":"","what":"Contribute to the package","title":"IASDT.R","text":"Contributions, suggestions, bug reports welcome. Please make changes new branch submit pull request make issue . questions, please get touch elgabbas[]outlook[dot]com","code":""},{"path":"https://biodt.github.io/IASDT.R/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"IASDT.R","text":"use IASDT.R package, please cite : El-Gabbas, . (2025). IASDT.R: Modelling distribution invasive alien plant species Europe. 10.5281/zenodo.14834384, R package version 0.1.05; https://github.com/BioDT/IASDT.R; https://biodt.eu. Last update: 2025-10-13","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/BioReg_Process.html","id":null,"dir":"Reference","previous_headings":"","what":"Process biogeographical regions dataset — bioreg_process","title":"Process biogeographical regions dataset — bioreg_process","text":"Downloads processes Biogeographical Regions dataset (Europe 2016, v1) European Environment Agency. function extracts biogeographical region names per reference grid cell use counting species presence across biogeographical regions.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/BioReg_Process.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process biogeographical regions dataset — bioreg_process","text":"","code":"bioreg_process(env_file = \".env\")"},{"path":"https://biodt.github.io/IASDT.R/reference/BioReg_Process.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process biogeographical regions dataset — bioreg_process","text":"env_file Character. Path environment file containing paths data sources. Defaults .env.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/BioReg_Process.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process biogeographical regions dataset — bioreg_process","text":"Invisible NULL. Processed data saved disk raster, vector, RData files.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/BioReg_Process.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Process biogeographical regions dataset — bioreg_process","text":"Temporal coverage: 2011-2015 Spatial coverage: 28°E 81°E, 31.27°W 62°E CRS: EPSG:3035 file format: shapefile (compressed zip file) Requirements: curl (download) unzip (extraction)","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/BioReg_Process.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Process biogeographical regions dataset — bioreg_process","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/CHELSA_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Process CHELSA Climate Data for the IASDT — chelsa_data","title":"Process CHELSA Climate Data for the IASDT — chelsa_data","text":"Downloads, processes, projects CHELSA climate data European scale Invasive Alien Species Digital Twin (IASDT). Supports multiple climate scenarios, outputting data TIFF NetCDF formats. Orchestrated chelsa_process(), helper functions chelsa_prepare() chelsa_project().","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/CHELSA_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process CHELSA Climate Data for the IASDT — chelsa_data","text":"","code":"chelsa_process(   env_file = \".env\",   n_cores = 8L,   strategy = \"multisession\",   download = FALSE,   overwrite = FALSE,   download_attempts = 10L,   sleep = 5L,   other_variables = \"npp\",   download_n_cores = 4,   compression_level = 5,   overwrite_processed = FALSE )  chelsa_prepare(   env_file = \".env\",   download = FALSE,   n_cores = 8L,   strategy = \"multisession\",   overwrite = FALSE,   download_attempts = 10L,   sleep = 5L,   other_variables = \"npp\" )  chelsa_project(metadata = NULL, env_file = \".env\", compression_level = 5L)"},{"path":"https://biodt.github.io/IASDT.R/reference/CHELSA_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process CHELSA Climate Data for the IASDT — chelsa_data","text":"env_file Character. Path environment file containing paths data sources. Defaults .env. n_cores Integer. Number CPU cores use parallel processing. Default: 8. strategy Character. parallel processing strategy use. Valid options \"sequential\", \"multisession\" (default), \"multicore\", \"cluster\". See future::plan() ecokit::set_parallel() details. download Logical. TRUE, downloads CHELSA files. Default: FALSE. overwrite Logical. TRUE, re-downloads existing files. Default: FALSE. download_attempts Integer. Maximum download retries. Default: 10. sleep Integer. Seconds wait download attempts. Default: 5. other_variables Character. Additional variables process (e.g., \"npp\" Net Primary Productivity alongside 19 bioclimatic variables bio1-bio19). Use \"\" bioclimatic . See chelsa_variables details. Default: \"npp\". download_n_cores Integer. Number CPU cores use parallel downloading CHELSA data. valid download = TRUE. Defaults 4. compression_level Integer. NetCDF compression level (1 = least, 9 = ). Default: 5. overwrite_processed Logical. TRUE, overwrites processed files. Default: FALSE. metadata Tibble. Single-row metadata input files, prepared chelsa_prepare()","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/CHELSA_data.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Process CHELSA Climate Data for the IASDT — chelsa_data","text":"chelsa_prepare() chelsa_project() internal helpers, direct use. Processes 19 bioclimatic variables (bio1–bio19) plus optional variables (e.g., NPP) 46 scenarios (1 current, 45 future). Time-intensive; depends file size compute resources.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/CHELSA_data.html","id":"functions-details","dir":"Reference","previous_headings":"","what":"Functions details","title":"Process CHELSA Climate Data for the IASDT — chelsa_data","text":"chelsa_process(): Main function; optionally downloads CHELSA data, processes European scale reference grid, saves TIFF NetCDF outputs 46 climate scenarios. chelsa_prepare(): Extracts metadata local URL text files manages optional downloads. chelsa_project(): Projects data IASDT reference grid optional transformations.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/CHELSA_data.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Process CHELSA Climate Data for the IASDT — chelsa_data","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/CHELSA_variables.html","id":null,"dir":"Reference","previous_headings":"","what":"Detailed information on CHELSA climate variables — chelsa_variables","title":"Detailed information on CHELSA climate variables — chelsa_variables","text":"Detailed information CHELSA climate variables","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/CHELSA_variables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detailed information on CHELSA climate variables — chelsa_variables","text":"","code":"chelsa_variables"},{"path":"https://biodt.github.io/IASDT.R/reference/CHELSA_variables.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Detailed information on CHELSA climate variables — chelsa_variables","text":"object class tbl_df (inherits tbl, data.frame) 46 rows 6 columns.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/CHELSA_variables.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detailed information on CHELSA climate variables — chelsa_variables","text":"tibble containing detailed information various climate variables CHELSA climate data. row tibble represents different climate variable, columns providing additional details long name, unit, scale, offset, explanation variable","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/CHELSA_variables.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Detailed information on CHELSA climate variables — chelsa_variables","text":"","code":"ecokit::load_packages(dplyr, gt)  data(chelsa_variables) #> Warning: data set ‘chelsa_variables’ not found gt::gt(chelsa_variables)     variable       long_name       unit       scale       offset       explanation     bio1 mean annual air temperature deg C 0.1 -273.15 mean annual daily mean air temperatures averaged over 1 yearbio2 mean diurnal air temperature range deg C 0.1 0.00 mean diurnal range of temperatures averaged over 1 yearbio3 isothermality deg C 0.1 0.00 ratio of diurnal variation to annual variation in temperaturesbio4 temperature seasonality deg C/100 0.1 0.00 standard deviation of the monthly mean temperaturesbio5 mean daily maximum air temperature of the warmest month deg C 0.1 -273.15 The highest temperature of any monthly daily mean maximum temperaturebio6 mean daily minimum air temperature of the coldest month deg C 0.1 -273.15 The lowest temperature of any monthly daily mean minimum temperaturebio7 annual range of air temperature deg C 0.1 0.00 Difference between the Maximum Temperature of Warmest month and the Minimum Temperature of Coldest month (bio5-bio6)bio8 mean daily mean air temperatures of the wettest quarter deg C 0.1 -273.15 The wettest quarter of the year is determined (to the nearest month)bio9 mean daily mean air temperatures of the driest quarter deg C 0.1 -273.15 The driest quarter of the year is determined (to the nearest month)bio10 mean daily mean air temperatures of the warmest quarter deg C 0.1 -273.15 The warmest quarter of the year is determined (to the nearest month)bio11 mean daily mean air temperatures of the coldest quarter deg C 0.1 -273.15 The coldest quarter of the year is determined (to the nearest month)bio12 annual precipitation amount kg m-2 0.1 0.00 Accumulated precipitation amount over 1 yearbio13 precipitation amount of the wettest month kg m-2 0.1 0.00 The precipitation of the wettest month.bio14 precipitation amount of the driest month kg m-2 0.1 0.00 The precipitation of the driest month.bio15 precipitation seasonality kg m-2 0.1 0.00 The Coefficient of Variation = 100*standard deviation of the monthly precipitation / mean bio16 mean monthly precipitation amount of the wettest quarter kg m-2 0.1 0.00 The wettest quarter of the year is determined (to the nearest month)bio17 mean monthly precipitation amount of the driest quarter kg m-2 0.1 0.00 The driest quarter of the year is determined (to the nearest month)bio18 mean monthly precipitation amount of the warmest quarter kg m-2 0.1 0.00 The warmest quarter of the year is determined (to the nearest month)bio19 mean monthly precipitation amount of the coldest quarter kg m-2 0.1 0.00 The coldest quarter of the year is determined (to the nearest month)gdgfgd0 First growing degree day above 0 deg C julian day 1.0 0.00 First day of the year above 0 deg Cgdgfgd5 First growing degree day above 5 deg C julian day 1.0 0.00 First day of the year above 5 deg Cgdgfgd10 First growing degree day above 10 deg C julian day 1.0 0.00 First day of the year above 10 deg Cgddlgd0 Last growing degree day above 0 deg C julian day 1.0 0.00 Last day of the year above 0 deg Cgddlgd5 Last growing degree day above 5 deg C julian day 1.0 0.00 Last day of the year above 5 deg Cgddlgd10 Last growing degree day above 10 deg C julian day 1.0 0.00 Last day of the year above 10 deg Cgdd0 Growing degree days heat sum above 0 deg C deg C 0.1 0.00 heat sum of all days above the 0 deg C temperature accumulated over 1 year.gdd5 Growing degree days heat sum above 5 deg C deg C 0.1 0.00 heat sum of all days above the 5 deg C temperature accumulated over 1 year.gdd10 Growing degree days heat sum above 10 deg C deg C 0.1 0.00 heat sum of all days above the 10 deg C temperature accumulated over 1 year.ngd0 Number of growing degree days # days 1.0 0.00 Number of days at which tas > 0 deg Cngd5 Number of growing degree days # days 1.0 0.00 Number of days at which tas > 5 deg Cngd10 Number of growing degree days # days 1.0 0.00 Number of days at which tas > 10 deg Cgsl growing season length TREELIM # days 1.0 0.00 Length of the growing seasongst Mean temperature of the growing season TREELIM deg C 0.1 -273.15 Mean temperature of all growing season days based on TREELIMlgd last day of the growing season TREELIM julian day 1.0 0.00 Last day of the growing season according to TREELIMfgd first day of the growing season TREELIM julian day 1.0 0.00 first day of the growing season according to TREELIMgsp Accumulated precipitation amount on growing season days TREELIM kg m-2 0.1 0.00 precipitation sum accumulated on all days during the growing season based on TREELIMfcf Frost change frequency count 1.0 0.00 Number of events in which tmin or tmax go above, or below 0 deg Cscd Snow cover days count 1.0 0.00 Number of days with snowcover calculated using the snowpack model implementation in from TREELIMswe Snow water equivalent kg m-2 0.1 0.00 Amount of luquid water if snow is meltedkg0 Koeppen-Geiger climate classification category 1.0 0.00 Koeppen Geiger - Koeppen&Geiger (1936)kg1 Koeppen-Geiger climate classification category 1.0 0.00 Koeppen Geiger without As/Aw differentiation - Koeppen&Geiger (1936)kg2 Koeppen-Geiger climate classification category 1.0 0.00 Koeppen Geiger after Peel et al. 2007kg3 Koeppen-Geiger climate classification category 1.0 0.00 Wissmann 1939kg4 Koeppen-Geiger climate classification category 1.0 0.00 Thornthwaite 1931kg5 Koeppen-Geiger climate classification category 1.0 0.00 Troll-Pfaffen - Troll&Paffen (1964)npp Net primary productivity g C m-2 yr-1 0.1 0.00 Calculated based on the `Miami model`, Lieth, H., 1972."},{"path":"https://biodt.github.io/IASDT.R/reference/CLC_Process.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Corine Land Cover (CLC) data for the IASDT — clc_process","title":"Process Corine Land Cover (CLC) data for the IASDT — clc_process","text":"Processes Corine Land Cover (CLC) data Invasive Alien Species Digital Twin (IASDT). Calculates percentage coverage common classes per grid cell three CLC levels, plus eunis19 SynHab habitat types. Prepares reference grid optionally generates percentage coverage maps JPEG.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/CLC_Process.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Corine Land Cover (CLC) data for the IASDT — clc_process","text":"","code":"clc_process(   env_file = \".env\",   min_land_percent = 15L,   plot_clc = TRUE,   n_cores_plot = 5L,   strategy = \"multisession\" )"},{"path":"https://biodt.github.io/IASDT.R/reference/CLC_Process.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Corine Land Cover (CLC) data for the IASDT — clc_process","text":"env_file Character. Path environment file containing paths data sources. Defaults .env. min_land_percent Numeric. Minimum land percentage per grid cell reference grid. Default: 15. plot_clc Logical. TRUE, plots percentage coverage CLC levels habitat types. Default: TRUE. n_cores_plot Integer. Number CPU cores use parallel plotting. Default: 5L. strategy Character. parallel processing strategy use. Valid options \"sequential\", \"multisession\" (default), \"multicore\", \"cluster\". See future::plan() ecokit::set_parallel() details.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/CLC_Process.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process Corine Land Cover (CLC) data for the IASDT — clc_process","text":"Returns invisible(NULL); saves processed data optional plots disk.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/CLC_Process.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Process Corine Land Cover (CLC) data for the IASDT — clc_process","text":"Data source: https://land.copernicus.eu/pan-european/corine-land-cover/clc2018 Data citation: https://doi.org/10.2909/960998c1-1870-4e82-8051-6485205ebbac","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/CLC_Process.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Process Corine Land Cover (CLC) data for the IASDT — clc_process","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Coda_to_tibble.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a Coda object to a tibble with specified parameter transformations — coda_to_tibble","title":"Convert a Coda object to a tibble with specified parameter transformations — coda_to_tibble","text":"function converts Coda object (mcmc.list mcmc) tibble format, facilitating analysis visualisation. supports transformation specific parameter types: rho, alpha, omega, beta.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Coda_to_tibble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a Coda object to a tibble with specified parameter transformations — coda_to_tibble","text":"","code":"coda_to_tibble(   coda_object = NULL,   posterior_type = NULL,   env_file = \".env\",   n_omega = 100 )"},{"path":"https://biodt.github.io/IASDT.R/reference/Coda_to_tibble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a Coda object to a tibble with specified parameter transformations — coda_to_tibble","text":"coda_object object class mcmc.list mcmc, representing MCMC output. posterior_type Character. parameter type transform extract. Must one rho, alpha, omega, beta. env_file Character. Path environment file containing paths data sources. Defaults .env. n_omega Integer. number species sampled Omega parameter transformation. Defaults 100.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Coda_to_tibble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a Coda object to a tibble with specified parameter transformations — coda_to_tibble","text":"tibble containing transformed parameters based specified posterior_type. structure returned tibble varies depending posterior_type parameter.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Coda_to_tibble.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Convert a Coda object to a tibble with specified parameter transformations — coda_to_tibble","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Coda_to_tibble.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a Coda object to a tibble with specified parameter transformations — coda_to_tibble","text":"","code":"ecokit::load_packages(Hmsc, coda, dplyr)  coda_object <- Hmsc::convertToCodaObject(Hmsc::TD$m) ecokit::ht(   IASDT.R::coda_to_tibble(     coda_object = coda_object$Alpha[[1]], posterior_type = \"Alpha\")) #>                Alpha alpha_num  factor  chain  iter value #>               <fctr>    <fctr>  <fctr> <fctr> <int> <num> #>   1: Alpha1[factor1]    Alpha1 factor1      1    51     0 #>   2: Alpha1[factor1]    Alpha1 factor1      1    52     0 #>   3: Alpha1[factor1]    Alpha1 factor1      1    53     0 #>   4: Alpha1[factor1]    Alpha1 factor1      1    54     0 #>   5: Alpha1[factor1]    Alpha1 factor1      1    55     0 #>  ---                                                      #> 396: Alpha1[factor2]    Alpha1 factor2      2   146     0 #> 397: Alpha1[factor2]    Alpha1 factor2      2   147     0 #> 398: Alpha1[factor2]    Alpha1 factor2      2   148     0 #> 399: Alpha1[factor2]    Alpha1 factor2      2   149     0 #> 400: Alpha1[factor2]    Alpha1 factor2      2   150     0"},{"path":"https://biodt.github.io/IASDT.R/reference/Convergence_Plot_All.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot model convergence of multiple modelling alternatives — convergence_plot_all","title":"Plot model convergence of multiple modelling alternatives — convergence_plot_all","text":"function generates saves series diagnostic plots assess convergence Hmsc models across multiple modelling alternatives. checks model convergence using trace plots Gelman-Rubin diagnostics key model parameters.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Convergence_Plot_All.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot model convergence of multiple modelling alternatives — convergence_plot_all","text":"","code":"convergence_plot_all(   model_dir = NULL,   n_omega = 1000L,   margin_type = \"histogram\",   spatial_model = TRUE,   n_cores = NULL,   strategy = \"multisession\",   future_max_size = 1000L,   n_rc_alpha = c(2L, 3L) )"},{"path":"https://biodt.github.io/IASDT.R/reference/Convergence_Plot_All.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot model convergence of multiple modelling alternatives — convergence_plot_all","text":"model_dir Character. Path root directory fitted model. convergence outputs saved model_convergence_all subdirectory. n_omega Integer. Number species interactions sampled Omega parameter diagnostics. Default: 1000L margin_type Character. type marginal plot add main plot. Valid options \"histogram\" (default) \"density\". spatial_model Logical. Whether model spatial model. TRUE (default), function generate additional plots model's Alpha parameter. n_cores Integer. Number CPU cores use parallel processing. strategy Character. parallel processing strategy use. Valid options \"sequential\", \"multisession\" (default), \"multicore\", \"cluster\". See future::plan() ecokit::set_parallel() details. future_max_size Numeric. Maximum allowed total size (megabytes) global variables identified. See future.globals.maxSize argument future::future.options details. n_rc_alpha Numeric vector length 2. Number rows columns convergence plots alpha parameter. Default: c(2L, 3L).","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Convergence_Plot_All.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot model convergence of multiple modelling alternatives — convergence_plot_all","text":"function returns invisible(NULL) return value, saves series diagnostic plots specified path.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Convergence_Plot_All.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot model convergence of multiple modelling alternatives — convergence_plot_all","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Convergence_plots.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot model convergence of a selected model — convergence_plots","title":"Plot model convergence of a selected model — convergence_plots","text":"convergence_plot() function generates saves convergence diagnostics plots rho, alpha, omega, beta parameters Hmsc model. plots help assess whether MCMC chains reached stationarity. supports parallel processing can work models fitted HPC environments.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Convergence_plots.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot model convergence of a selected model — convergence_plots","text":"","code":"convergence_plot(   path_coda = NULL,   env_file = \".env\",   title = \" \",   n_omega = 1000L,   n_cores = 8L,   strategy = \"multisession\",   future_max_size = 2000L,   n_rc = list(alpha = c(2L, 3L), omega = c(2L, 2L), beta = c(3L, 3L)),   pages_per_file = 20L,   chain_colors = NULL,   margin_type = \"histogram\",   spatial_model = TRUE )  convergence_alpha(   posterior = NULL,   title = NULL,   n_rc_alpha = c(2L, 3L),   add_footer = TRUE,   add_title = TRUE,   chain_colors = NULL,   margin_type = \"histogram\",   n_chains = NULL,   n_samples = NULL )  convergence_rho(   posterior = NULL,   title = NULL,   chain_colors = NULL,   margin_type = \"histogram\",   n_chains = NULL,   n_samples = NULL )  convergence_beta_ranges(model_dir = NULL, beta_data = NULL, n_chains = NULL)"},{"path":"https://biodt.github.io/IASDT.R/reference/Convergence_plots.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot model convergence of a selected model — convergence_plots","text":"path_coda Character. Path saved coda object containing MCMC samples. env_file Character. Path environment file containing paths data sources. Defaults .env. title Character. title rho alpha convergence plots. Default: \" \" n_omega Integer. Number species interactions sampled Omega parameter diagnostics. Default: 1000L n_cores Integer. Number CPU cores use parallel processing. Default: 8. strategy Character. parallel processing strategy use. Valid options \"sequential\", \"multisession\" (default), \"multicore\", \"cluster\". See future::plan() ecokit::set_parallel() details. future_max_size Numeric. Maximum allowed total size (megabytes) global variables identified. See future.globals.maxSize argument future::future.options details. n_rc List 3 numeric vectors representing number rows columns grid layout convergence plots alpha, omega, beta parameters. . pages_per_file Integer. Number plots per page Omega parameter output. Default: 20L. chain_colors Character vector. MCMC chain colours (optional). Default: NULL. margin_type Character. type marginal plot add main plot. Valid options \"histogram\" (default) \"density\". spatial_model Logical. Whether model spatial model. TRUE (default), function generate additional plots model's Alpha parameter. posterior mcmc.list character. Either MCMC object (mcmc.list) containing posterior samples, file path saved coda object. n_rc_alpha Numeric vector length 2. Number rows columns convergence plots alpha parameter. Default: c(2L, 3L). add_footer Logical. TRUE (default), adds footer page numbers plot. add_title Logical. TRUE (default), adds main title (title) plot. n_chains Integer. Number MCMC chains. n_samples Integer. Number MCMC samples. model_dir Character. path model directory. beta_data Data frame. Beta parameter summary data frame.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Convergence_plots.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot model convergence of a selected model — convergence_plots","text":"convergence_alpha(), convergence_rho(), convergence_beta_ranges internal functions called directly. convergence_beta_ranges plots convergence range species beta parameters. can used check chains show convergence issues; .e., showing exceptionally high low beta values.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Convergence_plots.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot model convergence of a selected model — convergence_plots","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/EASIN_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Process EASIN data for the IASDT — easin_data","title":"Process EASIN data for the IASDT — easin_data","text":"Extracts, processes, visualises data European Alien Species Information Network (EASIN) Invasive Alien Species Digital Twin (IASDT). Manages taxonomy, occurrence data, plots, handling API pagination server limits. Orchestrated easin_process() helpers easin_taxonomy(), easin_download(), easin_plot().","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/EASIN_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process EASIN data for the IASDT — easin_data","text":"","code":"easin_process(   extract_taxa = TRUE,   extract_data = TRUE,   n_download_attempts = 10L,   n_cores = 6L,   strategy = \"multisession\",   sleep_time = 10L,   n_search = 1000L,   env_file = \".env\",   delete_chunks = TRUE,   start_year = 1981L,   plot = TRUE )  easin_taxonomy(   env_file = \".env\",   kingdom = \"Plantae\",   phylum = \"Tracheophyta\",   n_search = 100L )  easin_download(   species_key,   timeout = 200,   verbose = FALSE,   env_file = \".env\",   n_search = 1000,   n_attempts = 10,   sleep_time = 5,   delete_chunks = TRUE,   return_data = FALSE )  easin_plot(env_file = \".env\")"},{"path":"https://biodt.github.io/IASDT.R/reference/EASIN_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process EASIN data for the IASDT — easin_data","text":"extract_taxa Logical. TRUE, extracts taxonomy using easin_taxonomy(). Default: TRUE. extract_data Logical.TRUE, downloads occurrence data easin_download(). Default: TRUE. n_download_attempts Integer. Retry attempts downloads. Default: 10. n_cores Integer. Number CPU cores use parallel processing. Default: 6. maximum number allowed cores 8. strategy Character. parallel processing strategy use. Valid options \"sequential\", \"multisession\" (default), \"multicore\", \"cluster\". See future::plan() ecokit::set_parallel() details. sleep_time Integer. Number seconds pause data retrieval request prevent overloading server. Default: 5 second. n_search Integer. Number records attempt retrieve per request. Default: 1000, current maximum allowed API. env_file Character. Path environment file containing paths data sources. Defaults .env. delete_chunks Logical. Whether delete temporary files data chunks file_parts subdirectory. Defaults TRUE. start_year Integer. Earliest year occurrence data (excludes earlier records). Default: 1981 (aligned CHELSA climate data). plot Logical. TRUE, generates plots via easin_plot(). Default: TRUE. kingdom Character. Taxonomic kingdom query. Default: \"Plantae\". phylum Character. Taxonomic phylum within kingdom. Default: \"Tracheophyta\" species_key Character. EASIN taxon ID data retrieved. parameter NULL. timeout Integer. Download timeout seconds. Default: 200. verbose Logical. TRUE, prints progress messages. Default: FALSE. n_attempts Integer. Max download attempts per chunk. Default: 10. return_data Logical. TRUE, returns data dataframe; otherwise, saves disk returns invisible(NULL). Default: FALSE.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/EASIN_data.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Process EASIN data for the IASDT — easin_data","text":"Uses static RDS file EASIN-GBIF taxonomic standardization, prepared Marina Golivets (Feb 2024).","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/EASIN_data.html","id":"functions-details","dir":"Reference","previous_headings":"","what":"Functions details","title":"Process EASIN data for the IASDT — easin_data","text":"easin_process(): Orchestrates taxonomy extraction, data downloads, plotting EASIN species data. easin_taxonomy(): Fetches taxonomy data chunks via easin API, filtered kingdom phylum. Returns tibble. easin_download(): Downloads occurrence data given easin ID, handling pagination pauses. Returns dataframe return_data = TRUE, else invisible(NULL). easin_plot(): Creates summary plots (observations count, species count, distribution partner) JPEGs. Returns invisible(NULL).","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/EASIN_data.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Process EASIN data for the IASDT — easin_data","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Efforts_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Process GBIF sampling effort data for the IASDT — efforts_data","title":"Process GBIF sampling effort data for the IASDT — efforts_data","text":"Downloads processes GBIF sampling effort data vascular plants Europe, supporting Invasive Alien Species Digital Twin (IASDT). Orchestrated efforts_process(), uses helper functions request, download, split, summarise, visualise data Order level. functions prepares raster maps number vascular plant observations species per grid cell.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Efforts_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process GBIF sampling effort data for the IASDT — efforts_data","text":"","code":"efforts_process(   env_file = \".env\",   r_environ = \".Renviron\",   request = TRUE,   download = TRUE,   n_cores = 6L,   strategy = \"multisession\",   start_year = 1981L,   boundaries = c(-30, 50, 25, 75),   chunk_size = 100000L,   delete_chunks = TRUE,   delete_processed = TRUE )  efforts_request(   env_file = \".env\",   n_cores = 3L,   strategy = \"multisession\",   start_year = 1981L,   r_environ = \".Renviron\",   boundaries = c(-30, 50, 25, 75) )  efforts_download(n_cores = 6L, strategy = \"multisession\", env_file = \".env\")  efforts_summarize(   env_file = \".env\",   n_cores = 6L,   strategy = \"multisession\",   chunk_size = 100000L,   delete_chunks = TRUE )  efforts_split(path_zip = NULL, env_file = \".env\", chunk_size = 100000L)  efforts_plot(env_file = \".env\")"},{"path":"https://biodt.github.io/IASDT.R/reference/Efforts_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process GBIF sampling effort data for the IASDT — efforts_data","text":"env_file Character. Path environment file containing paths data sources. Defaults .env. r_environ Character. Path .Renviron file GBIF credentials (GBIF_EMAIL, GBIF_USER, GBIF_PWD). Default: \".Renviron\". credentials must format: GBIF_EMAIL=your_email GBIF_USER=your_username GBIF_PWD=your_password request Logical. TRUE (default), requests GBIF data; otherwise, loads existing data. download Logical. TRUE (default), downloads saves GBIF data; otherwise, skips download. Default: TRUE. n_cores Integer. Number CPU cores use parallel processing. Default: 6, except efforts_request, defaults 3 maximum 3. strategy Character. parallel processing strategy use. Valid options \"sequential\", \"multisession\" (default), \"multicore\", \"cluster\". See future::plan() ecokit::set_parallel() details. start_year Integer. Earliest year GBIF records (matches CHELSA climate data). Default: 1981. boundaries Numeric vector (length 4). GBIF data bounds (Left, Right, Bottom, Top). Default: c(-30, 50, 25, 75). chunk_size Integer. Rows per chunk file. Default: 100000. delete_chunks Logical. TRUE (default), deletes chunk files post-processing. delete_processed Logical. TRUE (default), removes raw GBIF files processing (>22 GB). path_zip Character. Path zip file CSV splitting.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Efforts_data.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Process GBIF sampling effort data for the IASDT — efforts_data","text":"efforts_process() main entry point processing sampling effort data. Time-intensive (>9 hours 6-core Windows PC; GBIF request ~5 hours). Detects processes new/missing data order.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Efforts_data.html","id":"functions-details","dir":"Reference","previous_headings":"","what":"Functions details","title":"Process GBIF sampling effort data for the IASDT — efforts_data","text":"efforts_process(): Manages workflow requesting, downloading, processing, plotting GBIF vascular plant data. efforts_request(): Requests GBIF data order parallel. Stores results disk. efforts_download(): Downloads GBIF data, validates files, loads existing data available. Returns dataframe (efforts_all_requests) paths. efforts_split(): Splits zipped CSV data order chunks, saving separately. efforts_summarize(): Processes summarises data RData TIFF rasters. efforts_plot(): Plots observation efforts (raw log10 scales).","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Efforts_data.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Process GBIF sampling effort data for the IASDT — efforts_data","text":"Data source: https://www.gbif.org","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Efforts_data.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Process GBIF sampling effort data for the IASDT — efforts_data","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/GBIF_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Process GBIF occurrence data for the IASDT — gbif_data","title":"Process GBIF occurrence data for the IASDT — gbif_data","text":"Extracts, processes, visualises occurrence data Global Biodiversity Information Facility (GBIF) Invasive Alien Species Digital Twin (IASDT). Orchestrated gbif_process(), requests, downloads, cleans, chunks, maps species data using helper functions.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/GBIF_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process GBIF occurrence data for the IASDT — gbif_data","text":"","code":"gbif_process(   env_file = \".env\",   r_environ = \".Renviron\",   n_cores = 6L,   strategy = \"multisession\",   request = TRUE,   download = TRUE,   split_chunks = TRUE,   overwrite = FALSE,   delete_chunks = TRUE,   chunk_size = 50000L,   boundaries = c(-30, 50, 25, 75),   start_year = 1981L )  gbif_download(   env_file = \".env\",   r_environ = \".Renviron\",   request = TRUE,   download = TRUE,   split_chunks = TRUE,   chunk_size = 50000L,   boundaries = c(-30L, 50L, 25L, 75L),   start_year = 1981L )  gbif_read_chunk(   chunk_file,   env_file = \".env\",   max_uncertainty = 10L,   start_year = 1981L,   save_rdata = TRUE,   return_data = FALSE,   overwrite = FALSE )  gbif_species_data(   species = NULL,   env_file = \".env\",   verbose = TRUE,   plot_tag = NULL )"},{"path":"https://biodt.github.io/IASDT.R/reference/GBIF_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process GBIF occurrence data for the IASDT — gbif_data","text":"env_file Character. Path environment file containing paths data sources. Defaults .env. r_environ Character. Path .Renviron file GBIF credentials (GBIF_EMAIL, GBIF_USER, GBIF_PWD). Default: \".Renviron\". credentials must format: GBIF_EMAIL=your_email GBIF_USER=your_username GBIF_PWD=your_password n_cores Integer. Number CPU cores use parallel processing. Default: 6. strategy Character. parallel processing strategy use. Valid options \"sequential\", \"multisession\" (default), \"multicore\", \"cluster\". See future::plan() ecokit::set_parallel() details. request Logical. TRUE (default), requests GBIF data; otherwise, loads disk. download Logical. TRUE (default), downloads saves GBIF data. split_chunks Logical. TRUE (default), splits data chunks easier processing. overwrite Logical. TRUE, reprocesses existing .RData chunks. Default: FALSE. helps continue working previously processed chunks previous try failed, e.g. due memory issue. delete_chunks Logical. TRUE (default), deletes chunk files. chunk_size Integer. Records per data chunk. Default: 50000. boundaries Numeric vector (length 4). GBIF data bounds (Left, Right, Bottom, Top). Default: c(-30, 50, 25, 75). start_year Integer. Earliest collection year included. Default 1981. chunk_file Character. Path chunk file processing. max_uncertainty Numeric. Maximum spatial uncertainty kilometres. Default: 10. save_rdata Logical. TRUE (default), saves chunk data .RData. return_data TRUE, returns chunk data; otherwise, invisible(NULL). Default: FALSE. species Character. Species name processing. verbose Logical. TRUE (default), prints progress messages. plot_tag Character. Tag plot titles.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/GBIF_data.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Process GBIF occurrence data for the IASDT — gbif_data","text":"Relies static RDS file listing IAS species, GBIF keys, metadata, standardized Marina Golivets (Feb 2024).","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/GBIF_data.html","id":"functions-details","dir":"Reference","previous_headings":"","what":"Functions details","title":"Process GBIF occurrence data for the IASDT — gbif_data","text":"gbif_process(): Orchestrates GBIF data requests, downloads, processing, mapping. Saves RData, Excel, JPEG summary files. gbif_download(): Requests downloads GBIF data (download = TRUE), using specified criteria (taxa, coordinates, time period, boundaries), splits small chunks (split_chunks = TRUE), saves metadata. Returns invisible(NULL). gbif_read_chunk(): Filters chunk data (spatial/temporal, e.g., spatial uncertainty, collection year, coordinate precision, taxonomic rank), select relevant columns, saves .RData (save_rdata = TRUE) returns (return_data = TRUE). Skips .RData exists overwrite = FALSE. gbif_species_data(): Converts species-specific data sf raster formats, generating distribution maps.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/GBIF_data.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Process GBIF occurrence data for the IASDT — gbif_data","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/IASDT.R-package.html","id":null,"dir":"Reference","previous_headings":"","what":"IASDT.R: Modelling the distribution of invasive alien plant species in Europe — IASDT.R-package","title":"IASDT.R: Modelling the distribution of invasive alien plant species in Europe — IASDT.R-package","text":"Helper R functions Invasive Alien Species (IAS) Digital Twin (IASDT), part EU-funded Biodiversity Digital Twin (BioDT) project.","code":""},{"path":[]},{"path":"https://biodt.github.io/IASDT.R/reference/IASDT.R-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"IASDT.R: Modelling the distribution of invasive alien plant species in Europe — IASDT.R-package","text":"Maintainer: Ahmed El-Gabbas ahmed.el-gabbas@ufz.de (ORCID)","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_CV_Fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare cross-validated Hmsc models for HPC fitting — mod_cv_fit","title":"Prepare cross-validated Hmsc models for HPC fitting — mod_cv_fit","text":"function prepares cross-validated Hmsc models fitting using HPC. handles data preparation, model initialisation, generation SLURM commands.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_CV_Fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare cross-validated Hmsc models for HPC fitting — mod_cv_fit","text":"","code":"mod_cv_fit(   path_model = NULL,   cv_name = c(\"cv_dist\", \"cv_large\"),   partitions = NULL,   env_file = \".env\",   init_par = NULL,   job_name = \"cv_models\",   updater = list(Gamma2 = FALSE, GammaEta = FALSE),   align_posterior = TRUE,   to_json = FALSE,   slurm_prepare = TRUE,   memory_per_cpu = NULL,   job_runtime = NULL,   path_hmsc = NULL,   precision = 64,   ... )"},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_CV_Fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare cross-validated Hmsc models for HPC fitting — mod_cv_fit","text":"path_model Character. Path saved model file (*.qs2). cv_name Character vector. Column name(s) model input data used cross-validate models (see mod_prepare_data mod_cv_prepare). function allows possibility using one way assigning grid cells cross-validation folders. multiple names provided, separate cross-validation models fitted cross-validation type. Currently, three cross-validation strategies: cv_sac, cv_dist, cv_large. Defaults c(\"cv_dist\", \"cv_large\"). partitions vector cross-validation created Hmsc::createPartition similar. Defaults NULL, means use column name(s) provided cv_name argument. partitions vector provided, label used output files cv_custom. env_file Character. Path environment file containing paths data sources. Defaults .env. init_par named list parameter values used initialisation MCMC states. See Hmsc::computePredictedValues information. Default: NULL. job_name Character. Name submitted job(s) SLURM. Default: cv_models. updater named list. conditional updaters omitted? See Hmsc::computePredictedValues information. Defaults list(Gamma2 = FALSE, GammaEta = FALSE) disable following warnings: setting updater$Gamma2=FALSE due specified phylogeny matrix setting updater$GammaEta=FALSE: implemented spatial methods 'GPP' 'NNGP'. align_posterior Logical. Whether posterior chains aligned. See Hmsc::computePredictedValues information. Default: TRUE. to_json Logical. Whether convert unfitted models JSON saving RDS file. Default: FALSE. slurm_prepare Logical. Whether prepare SLURM command files. TRUE (default), SLURM commands saved disk using mod_slurm function. memory_per_cpu Character. Memory allocation per CPU core. Example: \"32G\" 32 gigabytes. Defaults \"64G\". job_runtime Character. Maximum allowed runtime job. Example: \"01:00:00\" one hour. Required — provided, function throws error. path_hmsc Character. Path Hmsc-HPC installation. precision Integer. Must either 32 64 (default). Defines floating-point precision mode Hmsc-HPC sampling (–fp 32 –fp 64). ... Additional arguments passed mod_slurm function.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_CV_Fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare cross-validated Hmsc models for HPC fitting — mod_cv_fit","text":"function copies part Hmsc::computePredictedValues function, currently support performing cross-validation using Hmsc-HPC.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_CV_Fit.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Prepare cross-validated Hmsc models for HPC fitting — mod_cv_fit","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_Merge_Chains.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge model chains into Hmsc and coda objects — mod_merge_chains","title":"Merge model chains into Hmsc and coda objects — mod_merge_chains","text":"functions merge posterior chains multiple runs Hmsc models unified Hmsc coda objects, facilitating analysis. check missing incomplete chains, optionally report issues, save processed results disk. mod_merge_chains handles regular models, mod_merge_chains_cv designed cross-validated models.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_Merge_Chains.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge model chains into Hmsc and coda objects — mod_merge_chains","text":"","code":"mod_merge_chains(   model_dir = NULL,   n_cores = 8L,   strategy = \"multisession\",   future_max_size = 1000L,   model_info_name = NULL,   print_incomplete = TRUE,   from_json = FALSE,   out_extension = \"qs2\" )  mod_merge_chains_cv(   model_dir = NULL,   n_cores = 8L,   strategy = \"multisession\",   future_max_size = 1000L,   cv_names = c(\"cv_dist\", \"cv_large\"),   from_json = FALSE,   out_extension = \"qs2\" )"},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_Merge_Chains.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge model chains into Hmsc and coda objects — mod_merge_chains","text":"model_dir Character. Path root directory model fitted. mod_merge_chains, subdirectories model_fitted model_coda created within path store merged Hmsc coda objects, respectively. mod_merge_chains_cv, merged objects stored model_fitting_cv/model_fitted. n_cores Integer. Number CPU cores use parallel processing. Defaults 8L. strategy Character. parallel processing strategy use. Valid options \"sequential\", \"multisession\" (default), \"multicore\", \"cluster\". See future::plan() ecokit::set_parallel() details. future_max_size Numeric. Maximum allowed total size (megabytes) global variables identified. See future.globals.maxSize argument future::future.options details. model_info_name Character. Name file (without extension) updated model information saved. NULL, overwrites existing model_info.RData file model_dir directory. specified, creates new .RData file name model_dir directory. Applies mod_merge_chains. print_incomplete Logical. TRUE, prints names model variants failed merge due missing incomplete chains. Defaults TRUE. from_json Logical. Whether convert loaded models JSON format reading. Defaults FALSE. out_extension Character. File extension (without dot) output files containing merged Hmsc coda objects. Options qs2 (faster read/write via qs2 package) RData (standard R format). Defaults qs2. cv_names Character vector. Names cross-validation strategies merge, matching used model setup. Defaults c(\"cv_dist\", \"cv_large\"). names one cv_dist, cv_large, cv_sac. Applies mod_merge_chains_cv.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_Merge_Chains.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge model chains into Hmsc and coda objects — mod_merge_chains","text":"functions return invisible(NULL) save processed model information merged objects disk specified locations.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_Merge_Chains.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Merge model chains into Hmsc and coda objects — mod_merge_chains","text":"mod_merge_chains merges posterior chains multiple runs Hmsc model fitted without cross-validation. checks missing incomplete chains, aligns posteriors (using alignPost = TRUE, falling back FALSE alignment fails), saves merged Hmsc object coda object MCMC diagnostics. also records fitting time memory usage progress files. mod_merge_chains_cv performs similar merging process cross-validated Hmsc models, processing fold specified cv_names separately. saves merged Hmsc objects per fold generate coda objects.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_Merge_Chains.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Merge model chains into Hmsc and coda objects — mod_merge_chains","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_SLURM.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare SLURM scripts for Hmsc-HPC model fitting — mod_slurm","title":"Prepare SLURM scripts for Hmsc-HPC model fitting — mod_slurm","text":"mod_slurm function generates SLURM job submission scripts fitting Hmsc-HPC models HPC environment. Additionally, mod_slurm_refit creates SLURM scripts refitting models failed previously fitted.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_SLURM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare SLURM scripts for Hmsc-HPC model fitting — mod_slurm","text":"","code":"mod_slurm(   model_dir = NULL,   job_name = NULL,   cat_job_info = TRUE,   ntasks = 1L,   cpus_per_task = 1L,   gpus_per_node = 1L,   memory_per_cpu = \"64G\",   job_runtime = NULL,   hpc_partition = \"small-g\",   env_file = \".env\",   path_hmsc = NULL,   command_prefix = \"commands_to_fit\",   slurm_prefix = \"bash_fit\",   slurm_path_out = NULL )  mod_slurm_refit(   model_dir = NULL,   n_array_jobs = 210L,   job_name = NULL,   memory_per_cpu = \"64G\",   job_runtime = NULL,   hpc_partition = \"small-g\",   env_file = \".env\",   cat_job_info = TRUE,   ntasks = 1L,   cpus_per_task = 1L,   gpus_per_node = 1L,   slurm_prepare = TRUE,   path_hmsc = NULL,   refit_prefix = \"commands_to_refit\",   slurm_prefix = \"bash_refit\" )"},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_SLURM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare SLURM scripts for Hmsc-HPC model fitting — mod_slurm","text":"model_dir Character. Path root directory fitted model. job_name Character. Name submitted job(s). cat_job_info Logical. TRUE, additional bash commands included print job-related information. Default: TRUE. ntasks Integer. Number tasks allocate job (#SBATCH --ntasks). Default: 1. cpus_per_task Integer. Number CPU cores allocated per task (#SBATCH --cpus-per-task). Default: 1. gpus_per_node Integer. Number GPUs requested per node (#SBATCH --gpus-per-node). Default: 1. memory_per_cpu Character. Memory allocation per CPU core. Example: \"32G\" 32 gigabytes. Defaults \"64G\". job_runtime Character. Maximum allowed runtime job. Example: \"01:00:00\" one hour. Required — provided, function throws error. hpc_partition Character. Name SLURM partition submit job . Default: \"small-g\", running array jobs GPU. env_file Character. Path environment file containing paths data sources. Defaults .env. path_hmsc Character. Path Hmsc-HPC installation. command_prefix Character.Prefix bash commands used job execution. Default: \"commands_to_fit\". slurm_prefix Character. Prefix generated SLURM script filenames. slurm_path_out Character. Directory SLURM script(s) saved. NULL (default), function derives path model_dir. n_array_jobs Integer. Number jobs per SLURM script file. LUMI HPC, limit 210 submitted jobs per user small-g partition. argument used split jobs multiple SLURM scripts needed. Default: 210. See LUMI documentation details. slurm_prepare Logical. Whether prepare SLURM command files. TRUE (default), SLURM commands saved disk using mod_slurm function. refit_prefix Character. Prefix files containing commands refit failed incomplete models.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_SLURM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare SLURM scripts for Hmsc-HPC model fitting — mod_slurm","text":"function return value. Instead, generates writes SLURM script files disk model fitting refitting.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_SLURM.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Prepare SLURM scripts for Hmsc-HPC model fitting — mod_slurm","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_Summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary of Hmsc model parameters — mod_summary","title":"Summary of Hmsc model parameters — mod_summary","text":"function provides comprehensive summary Hmsc model parameters, including Alpha, Beta, Rho, Omega. processes model's output, performs statistical summaries, optionally returns summarised data.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_Summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary of Hmsc model parameters — mod_summary","text":"","code":"mod_summary(   path_coda = NULL,   env_file = \".env\",   return_data = FALSE,   spatial_model = TRUE )"},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_Summary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary of Hmsc model parameters — mod_summary","text":"path_coda Character. Path .qs2 / .RData file containing coda object. env_file Character. Path environment file containing paths data sources. Defaults .env. return_data Logical. Whether summarised data returned R object. TRUE, function returns list containing summaries Alpha, Beta, Rho, Omega parameters. default value FALSE, means function return data save summaries specified directory. spatial_model Logical. Whether model spatial model. TRUE (default), function also process Alpha parameter.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_Summary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary of Hmsc model parameters — mod_summary","text":"return_data FALSE (default), function return anything saves summaries directory. return_data TRUE, also returns data R object.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_Summary.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Summary of Hmsc model parameters — mod_summary","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_inputs.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare initial models for model fitting with Hmsc-HPC — mod_inputs","title":"Prepare initial models for model fitting with Hmsc-HPC — mod_inputs","text":"mod_prepare_hpc function prepares input data initialises models fitting Hmsc-HPC. performs multiple tasks, including data preparation, defining spatial block cross-validation folds, generating Gaussian Predictive Process (GPP) knots (Tikhonov et al.), initialising models, creating HPC execution commands. function supports parallel processing offers option include exclude phylogenetic tree data. mod_prepare_data function used prepare habitat-specific data Hmsc models. function processes environmental species presence data, reads environment variables file, verifies paths, loads filters species data based habitat type minimum presence grid cells per species, merges various environmental layers (e.g., CHELSA Bioclimatic variables, habitat coverage, road railway intensity, sampling efforts) single dataset. Processed data saved disk *.RData file.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_inputs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare initial models for model fitting with Hmsc-HPC — mod_inputs","text":"","code":"mod_prepare_data(   hab_abb = NULL,   directory_name = NULL,   min_efforts_n_species = 100L,   exclude_cultivated = TRUE,   exclude_0_habitat = TRUE,   n_pres_per_species = 80L,   env_file = \".env\",   verbose_progress = TRUE )  mod_prepare_hpc(   hab_abb = NULL,   directory_name = NULL,   min_efforts_n_species = 100L,   n_pres_per_species = 80L,   env_file = \".env\",   gpp = TRUE,   gpp_dists = NULL,   min_lf = NULL,   max_lf = NULL,   alphapw = list(Prior = NULL, Min = 10, Max = 1500, Samples = 101),   efforts_as_predictor = TRUE,   road_rail_as_predictor = TRUE,   habitat_as_predictor = TRUE,   river_as_predictor = FALSE,   soil_as_predictor = TRUE,   wetness_as_predictor = TRUE,   bio_variables = c(\"bio3\", \"bio4\", \"bio11\", \"bio18\", \"bio19\", \"npp\"),   quadratic_variables = c(\"bio4\", \"bio11\"),   n_species_per_grid = 0L,   exclude_cultivated = TRUE,   exclude_0_habitat = TRUE,   cv_n_folds = 4L,   cv_n_grids = 20L,   cv_n_rows = 2L,   cv_n_columns = 2L,   cv_sac = FALSE,   cv_fit = list(cv_type = NULL, cv_fold = NULL, inherit_dir = NULL),   use_phylo_tree = TRUE,   no_phylo_tree = FALSE,   overwrite_rds = TRUE,   n_cores = 8L,   strategy = \"multisession\",   future_max_size = 1000L,   mcmc_n_chains = 4L,   mcmc_thin = NULL,   mcmc_samples = 1000L,   mcmc_transient_factor = 500L,   mcmc_verbose = 200L,   skip_fitted = TRUE,   n_array_jobs = 210L,   model_country = NULL,   verbose_progress = TRUE,   slurm_prepare = TRUE,   memory_per_cpu = \"64G\",   job_runtime = NULL,   job_name = NULL,   path_hmsc = NULL,   check_python = FALSE,   to_json = FALSE,   precision = 64L,   ... )"},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_inputs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare initial models for model fitting with Hmsc-HPC — mod_inputs","text":"hab_abb Character. Abbreviation habitat type (based SynHab) prepare data. Valid values 0, 1, 2, 3, 4a, 4b, 10, 12a, 12b. hab_abb = 0, data prepared irrespective habitat type. details, see Pysek et al.. directory_name Character. Directory name, without parents, models saved. directory created. min_efforts_n_species Integer. Minimum number vascular plant species per grid cell (GBIF data) required inclusion models. exclude grid cells little sampling efforts. Defaults 100. exclude_cultivated Logical. Whether exclude countries cultivated casual observations per species. Defaults TRUE. exclude_0_habitat Logical. Whether exclude grid cells zero percentage habitat coverage. Defaults TRUE. n_pres_per_species Integer. minimum number presence grid cells species included analysis. number presence grid cells per species calculated discarding grid cells low sampling efforts (min_efforts_n_species) zero percentage habitat coverage exclude_0_habitat. Defaults 80. env_file Character. Path environment file containing paths data sources. Defaults .env. verbose_progress Logical. Whether print message upon successful saving files. Defaults FALSE. gpp Logical. Whether fit spatial random effect using Gaussian Predictive Process. Defaults TRUE. FALSE, non-spatial models fitted. gpp_dists Integer. Spacing (kilometres) GPP knots, well minimum allowable distance knot nearest sampling point. knots generated using prepare_knots function, value used knotDist minKnotDist Hmsc::constructKnots. min_lf, max_lf Integer. Minimum maximum number latent factors used. default NULL means number latent factors estimated data. either provided, respective values used arguments Hmsc::setPriors. alphapw Prior alpha parameter. Defaults list Prior = NULL, Min = 10, Max = 1500, Samples = 101. alphapw NULL list NULL list items, default prior used. Prior matrix, used prior. Prior = NULL, prior generated using minimum maximum values alpha parameter (min max, respectively; kilometre)  number samples (Samples). Defaults prior 101 samples ranging 10 1500 km, first value second column set 0.5. efforts_as_predictor Logical. Whether include (log10) sampling efforts predictor model. Default: TRUE. road_rail_as_predictor Logical. Whether include (log10) sum road railway intensity predictor model. Default: TRUE. habitat_as_predictor Logical. Whether include (log10) percentage coverage respective habitat type per grid cell predictor model. Default: TRUE. valid hab_abb equals 0. river_as_predictor Logical. Whether include (log10) total length rivers per grid cell predictor model. Default: FALSE. See river_length details. soil_as_predictor Logical. Whether include soil bulk density depth 15-30 cm predictor model. Default: TRUE. See soil_density_process. wetness_as_predictor Logical. Whether include topographic wetness index predictor model. Default: TRUE. See wetness_index_process. bio_variables Character vector. Variables CHELSA (bioclimatic variables (bio1-bio19) additional predictors (e.g., Net Primary Productivity, npp)) used model. default, six ecologically relevant minimally correlated variables selected: c(\"bio3\", \"bio4\", \"bio11\", \"bio18\", \"bio19\", \"npp\"). quadratic_variables Character vector variables quadratic terms used. Defaults c(\"bio4\", \"bio11\"). quadratic_variables NULL, quadratic terms used. n_species_per_grid Integer. Minimum number species required grid cell included analysis. filtering occurs applying min_efforts_n_species (sampling effort thresholds), n_pres_per_species (minimum species presence thresholds), exclude_0_habitat (exclude 0% habitat coverage). Default (0): Includes grid cells. Positive value (>0): Includes grid cells least n_species_per_grid species present. cv_n_folds Integer. Number cross-validation folds. Default: 4L. cv_n_grids Integer. cv_dist cross-validation strategy (see mod_cv_prepare), argument determines size blocks (many grid cells directions). cv_n_rows, cv_n_columns Integer. Number rows columns used cv_large cross-validation strategy  (see mod_cv_prepare), study area divided large blocks given provided cv_n_rows cv_n_columns values. default 2 means split study area four large blocks median latitude longitude. cv_sac Logical. Whether use spatial autocorrelation determine block size. Defaults FALSE, cv_fit list three elements determining current model specific cross-validation fold full dataset. cv_type (character): type cross-validation use. Valid options \"cv_dist\", \"cv_large\", \"cv_sac\". Default: NULL, means fit models full dataset. can NULL cv_fold provided. cv_fold (integer): id cross-validation fold fit. example, cv_fold = 4 means use fourth fold testing. Default: NULL means cross-validation performed. can NULL cv_type provided. inherit_dir (character): name directory (without parents) inherit (copy) species cross-validation data . Defaults NULL, means data species cross-validation calculated. use_phylo_tree, no_phylo_tree Logical. Whether fit models (use_phylo_tree) without (no_phylo_tree) phylogenetic trees. Defaults use_phylo_tree = TRUE no_phylo_tree = FALSE, meaning models phylogenetic trees fitted default. least one use_phylo_tree no_phylo_tree TRUE. overwrite_rds Logical. Whether overwrite previously exported RDS files initial models. Default: TRUE. n_cores Integer. Number CPU cores use parallel processing. Default: 8. strategy Character. parallel processing strategy use. Valid options \"sequential\", \"multisession\" (default), \"multicore\", \"cluster\". See future::plan() ecokit::set_parallel() details. future_max_size Numeric. Maximum allowed total size (megabytes) global variables identified. See future.globals.maxSize argument future::future.options details. mcmc_n_chains Integer. Number model chains. Default: 4L. mcmc_thin Integer vector. Thinning value(s) MCMC sampling. one value provided, separate model fitted value thinning. mcmc_samples Integer vector. Value(s) number MCMC samples. one value provided, separate model fitted value number samples. Defaults 1000. mcmc_transient_factor Integer. Transient multiplication factor. value transient equal multiplication mcmc_transient_factor mcmc_thin. Default: 500. mcmc_verbose Integer. Interval MCMC sampling progress reported. Default: 200. skip_fitted Logical. Whether skip already fitted models. Default: TRUE. n_array_jobs Integer. Number jobs per SLURM script file. LUMI HPC, limit 210 submitted jobs per user small-g partition. argument used split jobs multiple SLURM scripts needed. Default: 210. See LUMI documentation details. model_country Character. Country countries filter observations . Default: NULL, means prepare data whole Europe. slurm_prepare Logical. Whether prepare SLURM command files. TRUE (default), SLURM commands saved disk using mod_slurm function. memory_per_cpu Character. Memory per CPU SLURM job. value assigned #SBATCH --mem-per-cpu= SLURM argument. Example: \"32G\" request 32 gigabyte. effective slurm_prepare = TRUE. Defaults \"64G\". job_runtime Character. Requested time job SLURM bash arrays. Example: \"01:00:00\" request hour. effective slurm_prepare = TRUE. job_name Character. Name submitted job(s) SLURM. NULL (Default), job name prepared based folder path hab_abb value. effective slurm_prepare = TRUE. path_hmsc Character. Directory path Hmsc-HPC extension installation. provided path_hmsc argument mod_slurm function. check_python Logical. Whether check Python executable exists. to_json Logical. Whether convert unfitted models JSON saving RDS file. Default: FALSE. precision Integer. Must either 32 64 (default). Defines floating-point precision mode Hmsc-HPC sampling (–fp 32 –fp 64). ... Additional parameters provided mod_slurm function.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_inputs.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Prepare initial models for model fitting with Hmsc-HPC — mod_inputs","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_postprocessing.html","id":null,"dir":"Reference","previous_headings":"","what":"Model pipeline for post-processing fitted Hmsc models — mod_postprocessing","title":"Model pipeline for post-processing fitted Hmsc models — mod_postprocessing","text":"functions post-process fitted Hmsc models CPU GPU. main functions pipeline includes mod_postprocess_1_cpu, mod_prepare_tf, mod_postprocess_2_cpu full models without cross-validation, well mod_postprocess_cv_1_cpu mod_postprocess_cv_2_cpu cross-validated models. See details information.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_postprocessing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model pipeline for post-processing fitted Hmsc models — mod_postprocessing","text":"","code":"mod_postprocess_1_cpu(   model_dir = NULL,   hab_abb = NULL,   strategy = \"multisession\",   future_max_size = 1500L,   n_cores = 8L,   n_cores_pred = n_cores,   n_cores_lf = n_cores,   n_cores_vp = n_cores,   env_file = \".env\",   path_hmsc = NULL,   memory_per_cpu = \"64G\",   job_runtime = \"01:00:00\",   from_json = FALSE,   gpp_dist = NULL,   use_trees = \"tree\",   temp_dir = NULL,   mcmc_n_samples = 1000L,   mcmc_thin = NULL,   n_omega = 1000L,   cv_name = c(\"cv_dist\", \"cv_large\"),   n_grid = 50L,   use_tf = TRUE,   tf_use_single = FALSE,   lf_temp_cleanup = TRUE,   lf_check = FALSE,   temp_cleanup = TRUE,   tf_environ = NULL,   pred_new_sites = TRUE,   width_omega = 26,   height_omega = 22.5,   width_beta = 25,   height_beta = 35,   spatial_model = TRUE,   tar_predictions = TRUE,   plot_predictions = TRUE,   is_cv_model = FALSE,   clamp_pred = TRUE,   fix_efforts = \"q90\",   fix_rivers = \"q90\",   climate_models = c(\"GFDL-ESM4\", \"IPSL-CM6A-LR\", \"MPI-ESM1-2-HR\", \"MRI-ESM2-0\",     \"UKESM1-0-LL\"),   climate_scenario = c(\"ssp126\", \"ssp370\", \"ssp585\") )  mod_prepare_tf(   process_vp = TRUE,   process_lf = TRUE,   n_batch_files = 210L,   env_file = \".env\",   working_directory = NULL,   partition_name = \"small-g\",   lf_runtime = \"01:00:00\",   model_prefix = NULL,   vp_runtime = \"02:00:00\" )  mod_postprocess_2_cpu(   model_dir = NULL,   hab_abb = NULL,   strategy = \"multisession\",   future_max_size = 1500L,   n_cores = 8L,   n_cores_pred = n_cores,   n_cores_lf = n_cores,   n_cores_rc = n_cores,   n_cores_vp = n_cores,   env_file = \".env\",   gpp_dist = NULL,   use_trees = \"tree\",   temp_dir = NULL,   mcmc_n_samples = 1000L,   mcmc_thin = NULL,   use_tf = TRUE,   tf_environ = NULL,   tf_use_single = FALSE,   lf_check = FALSE,   lf_temp_cleanup = TRUE,   temp_cleanup = TRUE,   n_grid = 50L,   climate_models = c(\"GFDL-ESM4\", \"IPSL-CM6A-LR\", \"MPI-ESM1-2-HR\", \"MRI-ESM2-0\",     \"UKESM1-0-LL\"),   climate_scenario = c(\"ssp126\", \"ssp370\", \"ssp585\"),   clamp_pred = TRUE,   fix_efforts = \"q90\",   fix_rivers = \"q90\",   pred_new_sites = TRUE,   tar_predictions = TRUE,   rc_prepare = TRUE,   rc_plot = TRUE,   vp_prepare = TRUE,   vp_plot = TRUE,   predict_suitability = TRUE,   plot_predictions = TRUE,   plot_lf = TRUE,   plot_internal_evaluation = TRUE,   spatial_model = TRUE,   is_cv_model = FALSE )  mod_postprocess_cv_1_cpu(   model_dir = NULL,   cv_names = NULL,   n_cores = 8L,   strategy = \"multisession\",   env_file = \".env\",   from_json = FALSE,   use_tf = TRUE,   tf_use_single = FALSE,   tf_environ = NULL,   n_cores_lf = n_cores,   lf_only = TRUE,   lf_temp_cleanup = TRUE,   lf_check = FALSE,   lf_runtime = \"01:00:00\",   temp_cleanup = TRUE,   n_batch_files = 210L,   working_directory = NULL,   partition_name = \"small-g\" )  mod_postprocess_cv_2_cpu(   model_dir = NULL,   cv_names = NULL,   n_cores = 8L,   strategy = \"multisession\",   env_file = \".env\",   use_tf = TRUE,   tf_use_single = FALSE,   temp_cleanup = TRUE,   lf_temp_cleanup = TRUE,   tf_environ = NULL,   n_cores_lf = n_cores,   lf_check = FALSE )"},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_postprocessing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model pipeline for post-processing fitted Hmsc models — mod_postprocessing","text":"model_dir Character. Path root directory fitted model. hab_abb Character. Habitat abbreviation indicating specific SynHab habitat type. Valid values: 0, 1, 2, 3, 4a, 4b, 10, 12a, 12b. See Pysek et al. details. strategy Character. parallel processing strategy use. Valid options \"sequential\", \"multisession\" (default), \"multicore\", \"cluster\". See future::plan() ecokit::set_parallel() details. future_max_size Numeric. Maximum allowed total size (megabytes) global variables identified. See future.globals.maxSize argument future::future.options details. n_cores, n_cores_pred, n_cores_lf, n_cores_vp, n_cores_rc Integer. Number cores use parallel processing. used different processing steps: n_cores merging chains plotting convergence convergence diagnostics; n_cores_pred predicting species' habitat suitability; n_cores_lf predicting latent factors; n_cores_vp processing variance partitioning; n_cores_rc response curve prediction. default 8L. strategy = \"sequential\", arguments set 1L. env_file Character. Path environment file containing paths data sources. Defaults .env. path_hmsc Character. Path Hmsc-HPC installation. memory_per_cpu Character. Memory allocation per CPU core. Example: \"32G\" 32 gigabytes. Defaults \"64G\". job_runtime Character. Maximum allowed runtime jobs refitting models (needed) cross validating models. Defaults \"01:00:00\" one hour. provided, function throws error. from_json Logical. Whether convert loaded models JSON format reading. Defaults FALSE. gpp_dist Integer. Distance kilometres knots selected model. use_trees Character. Whether phylogenetic tree used selected model. Accepts \"tree\" (default) \"no_tree\". temp_dir Character. Path temporary directory store temporary files model predictions. NULL, directory named temp_pred created model_dir. mcmc_thin, mcmc_n_samples Integer. Thinning value number MCMC samples selected model. n_omega Integer. number species sampled Omega parameter transformation. Defaults 100. cv_name NULL character vector. Column name(s) model input data used cross-validate models (see mod_prepare_data mod_cv_prepare). cv_name = NULL, cross-validation data preparation done. See mod_cv_fit valid options. n_grid Integer. Number points along gradient continuous focal variables. Higher values result smoother curves. Default: 50. See Hmsc::constructGradient details. use_tf Logical. Whether use TensorFlow calculations. Defaults TRUE. tf_use_single Logical. Whether use single precision TensorFlow calculations. Defaults FALSE. lf_check Logical. TRUE, function checks output files already created valid. FALSE, function check files exist without checking integrity. Default FALSE. temp_cleanup, lf_temp_cleanup Logical. Whether delete temporary files finishing predicting latent factor species distribution. Default: TRUE. tf_environ Character. Path Python environment. argument required use_tf TRUE Windows. Defaults NULL. pred_new_sites Logical. Whether predict suitability new sites. Default: TRUE. width_omega, height_omega, width_beta, height_beta Integer. width height generated heatmaps Omega Beta parameters centimetres. spatial_model, plot_lf Logical. Whether model spatial (TRUE) (FALSE) whether plot latent factors spatial models JPEG files (using plot_latent_factor). Defaults TRUE. is_cv_model Logical. Whether model cross-validated model (TRUE) fitted full dataset (FALSE; default). TRUE, explanatory predictive power model computed. clamp_pred Logical indicating whether clamp sampling efforts single value. TRUE (default), fix_efforts argument must provided. fix_efforts Numeric character. clamp_pred = TRUE, fixes sampling efforts predictor value predictions. numeric, uses value directly (log10 scale). character, must one identity (.e., fix), median, mean, max, q90 (90% quantile). Using max may reflect extreme sampling efforts highly sampled locations, q90 captures high sampling areas without extremes. Required clamp_pred = TRUE. fix_rivers Numeric, character, NULL. Similar fix_efforts, river length predictor. NULL, river length fixed. Default: q90. climate_models Character vector. Climate models future predictions. Available options c(\"GFDL-ESM4\", \"IPSL-CM6A-LR\", \"MPI-ESM1-2-HR\", \"MRI-ESM2-0\", \"UKESM1-0-LL\") (default). climate_scenario Character vector. Climate scenarios future predictions. Available options : c(\"ssp126\", \"ssp370\", \"ssp585\") (default). process_vp, process_lf Logical. Whether prepares batch scripts variance partitioning computations latent factor predictions GPUs. Defaults TRUE. n_batch_files Integer. Number output batch files create. Must less equal maximum job limit HPC environment. working_directory Character. Optionally sets working directory batch scripts path. NULL, directory remains unchanged. partition_name Character. Name partition submit SLURM jobs . Default small-g. lf_runtime, vp_runtime Character. Time limit latent factor prediction variance partitioning processing jobs, respectively. Defaults 01:00:00 02:00:00 respectively. model_prefix Character. Prefix model name. directory named model_prefix_TF created model_dir store TensorFlow running commands. Defaults NULL. can NULL. rc_prepare, rc_plot Logical. Whether prepare data response curve prediction (using rc_prepare_data) plot response curves JPEG files. (using rc_plot_sr, rc_plot_species, rc_plot_species_all). Defaults TRUE. vp_prepare, vp_plot Logical. Whether prepare data variance partitioning (using variance_partitioning_compute) plot results (using variance_partitioning_plot). Defaults TRUE. predict_suitability, tar_predictions, plot_predictions Logical. Whether predict habitat suitability across different climate options (using predict_maps), compress resulted files single *.tar file (without compression), plot species species richness predictions JPEG files (using plot_prediction). Defaults TRUE. plot_internal_evaluation Logical. Whether compute visualise model internal evaluation (explanatory power) using plot_evaluation. Defaults TRUE. cv_names Character vector. Names cross-validation strategies merge, matching used model setup. Defaults c(\"cv_dist\", \"cv_large\"). names one cv_dist, cv_large, cv_sac. Applies mod_merge_chains_cv. lf_only Logical. Whether predict latent factor. useful distributing processing load GPU CPU. lf_only = TRUE, latent factor prediction needs computed separately GPU. computations finished GPU, function can later rerun lf_only = FALSE (default) predict habitat suitability using already-computed latent factor predictions.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_postprocessing.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model pipeline for post-processing fitted Hmsc models — mod_postprocessing","text":"mod_postprocess_1_cpu function performs initial post-processing step habitat-specific fitted models, automating following tasks: check unsuccessful models: mod_slurm_refit merge chains save R objects (fitted model object coda object) qs2 RData files: mod_merge_chains visualise convergence model variants fitted convergence_plot_all visualise convergence selected model, including plotting Gelman-Rubin-Brooks plot_gelman convergence_plot model convergence diagnostics rho, alpha, omega, beta parameters. extract save model summary: mod_summary plotting model parameters: mod_heatmap_omega, mod_heatmap_beta prepare data cross-validation fit initial cross-validated models: mod_cv_fit Prepare scripts GPU processing, including: predicting latent factors response curves: rc_prepare_data predicting latent factors new sampling units: predict_maps computing variance partitioning: variance_partitioning_compute  mod_prepare_tf running mod_postprocess_1_cpu habitat types, function prepares batch scripts GPU computations habitat types: variance partitioning, function matches files pattern  \"vp_.+command.txt\" (created variance_partitioning_compute merges contents single file (model_prefix_TF/vp_commands.txt). , prepares SLURM script variance partitioning computations (model_prefix_TF/vp_slurm.slurm). latent factor predictions, function matches files pattern \"^lf_new_sites_commands_.+.txt|^lf_rc_commands_.+txt\" split contents multiple scripts model_prefix_TF directory processing batch job. function prepares SLURM script latent factor predictions (lf_slurm.slurm). function tailored LUMI HPC environment assumes tensorflow module installed correctly configured required Python packages. HPC systems, users may need modify function load Python virtual environment install required dependencies TensorFlow related packages.  mod_postprocess_2_cpu function continues running analysis pipeline post-processing Hmsc automating following steps: process visualise response curves: response_curves predict habitat suitability across different climate options: predict_maps plot species & SR predictions JPEG: plot_prediction plot latent factors JPEG: plot_latent_factor process visualise variance partitioning: variance_partitioning_compute variance_partitioning_plot compute visualizing model internal evaluation (explanatory power): plot_evaluation initiate post-processing fitted cross-validated models: prepare commands latent factor predictions GPU — Ongoing function run : completing mod_postprocess_1_cpu mod_prepare_tf CPU, running vp_slurm.slurm lf_slurm.slurm GPU process response curves latent factor predictions (scripts generated mod_prepare_tf). submitting SLURM jobs cross-validated model fitting.  mod_postprocess_cv_1_cpu function similar mod_postprocess_1_cpu, specifically designed cross-validated models. automates merging fitted cross-validated model chains Hmsc model objects prepare scripts latent factor prediction TensorFlow using predict_maps_cv.  mod_postprocess_cv_2_cpu function 1) processes *.feather files resulted Latent Factor predictions (using TensorFlow) saves LF predication disk; 2) predicts species-specific mean habitat suitability testing cross-validation folds calculates testing evaluation metrics; 3) generates plots evaluation metrics.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Mod_postprocessing.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Model pipeline for post-processing fitted Hmsc models — mod_postprocessing","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Parameter_Heatmap.html","id":null,"dir":"Reference","previous_headings":"","what":"Heatmaps for the beta and omega parameters of the Hmsc model — parameter_heatmap","title":"Heatmaps for the beta and omega parameters of the Hmsc model — parameter_heatmap","text":"mod_heatmap_beta() mod_heatmap_omega() functions generate heatmaps using ggplot2 visualise parameter estimates posterior support values species' environmental responses (beta parameters, describes species (Y) respond various covariates (X); see Hmsc::plotBeta) residual associations (omega parameter), respectively.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Parameter_Heatmap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Heatmaps for the beta and omega parameters of the Hmsc model — parameter_heatmap","text":"","code":"mod_heatmap_beta(   path_model = NULL,   support_level = 0.95,   width = 25,   height = 35 )  mod_heatmap_omega(   path_model = NULL,   support_level = 0.95,   width = 26,   height = 22.5 )"},{"path":"https://biodt.github.io/IASDT.R/reference/Parameter_Heatmap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Heatmaps for the beta and omega parameters of the Hmsc model — parameter_heatmap","text":"path_model Character. Path fitted Hmsc model object. support_level Numeric. posterior support threshold determining values considered significant heatmap. Defaults 0.95, indicating 95% posterior support. Values threshold (1 - threshold negative associations) considered significant plotted (see Hmsc::plotBeta). width, height Integer. width height generated heatmaps centimetres. Defaults 26×22.5 omega; 25×35 beta.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Parameter_Heatmap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Heatmaps for the beta and omega parameters of the Hmsc model — parameter_heatmap","text":"functions return value saves heatmap plots JPEG files model_postprocessing/parameters_summary subdirectory.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Parameter_Heatmap.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Heatmaps for the beta and omega parameters of the Hmsc model — parameter_heatmap","text":"functions exports three types visualisations (see Hmsc::plotBeta): mean: posterior mean estimate, support: statistical support level, measured posterior probability positive negative response, sign: indicates whether response positive, negative, neither based chosen support_level. omega parameter, mod_heatmap_omega() function generates two JPEG files: signs mean values. beta parameter, mod_heatmap_beta() function generates four JPEG files : support, signs, mean values (including excluding intercept).","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Parameter_Heatmap.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Heatmaps for the beta and omega parameters of the Hmsc model — parameter_heatmap","text":"Ahmed El-Gabbas. mod_heatmap_beta() function adapted Hmsc::plotBeta","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Predict_Hmsc.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates predicted values from a fitted Hmsc model — predict_hmsc","title":"Calculates predicted values from a fitted Hmsc model — predict_hmsc","text":"function modifies Hmsc:::predict.Hmsc function.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Predict_Hmsc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates predicted values from a fitted Hmsc model — predict_hmsc","text":"","code":"predict_hmsc(   path_model,   Loff = NULL,   x_data = NULL,   X = NULL,   XRRRData = NULL,   XRRR = NULL,   gradient = NULL,   Yc = NULL,   mcmcStep = 1L,   expected = TRUE,   n_cores = 8L,   strategy = \"multisession\",   future_max_size = 1000L,   model_name = \"train\",   temp_dir = \"temp_pred\",   temp_cleanup = TRUE,   prediction_type = NULL,   use_tf = TRUE,   tf_environ = NULL,   tf_use_single = FALSE,   lf_out_file = NULL,   lf_return = FALSE,   lf_input_file = NULL,   lf_only = FALSE,   n_cores_lf = n_cores,   lf_check = FALSE,   lf_temp_cleanup = TRUE,   lf_commands_only = FALSE,   pred_directory = NULL,   pred_pa = NULL,   pred_xy = NULL,   evaluate = FALSE,   evaluation_name = NULL,   evaluation_directory = \"evaluation\",   verbose = TRUE,   spatial_model = TRUE )"},{"path":"https://biodt.github.io/IASDT.R/reference/Predict_Hmsc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates predicted values from a fitted Hmsc model — predict_hmsc","text":"path_model Character. Path saved model object. Loff See Hmsc::predict.Hmsc details. x_data data.frame. unpreprocessed covariates predictions made. Works XFormula argument specified Hmsc::Hmsc model constructor call. Requirements similar Hmsc model constructor. X matrix. Covariates predictions made. one x_data X arguments may provided. XRRRData data.frame. Covariates reduced-rank regression. XRRR matrix. Covariates reduced-rank regression. gradient object returned Hmsc::constructGradient. Providing gradient alternative providing x_data, studyDesign ranLevels. used together Yc. Yc matrix. Outcomes assumed known conditional predictions. used together gradient. mcmcStep Integer. Number extra mcmc steps used updating random effects. Defaults 1L. expected Logical. Whether return location parameter observation models sample values . Defaults TRUE. n_cores Integer. Number CPU cores use parallel processing. Default: 8. strategy Character. parallel processing strategy use. Valid options \"sequential\", \"multisession\" (default), \"multicore\", \"cluster\". See future::plan() ecokit::set_parallel() details. future_max_size Numeric. Maximum allowed total size (megabytes) global variables identified. See future.globals.maxSize argument future::future.options details. model_name Character. Prefix temporary file names. Defaults NULL, case prefix used. temp_dir Character. Path temporary storage intermediate files. temp_cleanup Logical. Whether clean temporary files. Defaults TRUE. prediction_type Character. Type predictions made. NULL (default), predictions made latent factors. c, predictions made response curves mean coordinates. , predictions made response curves infinite coordinates. use_tf Logical. Whether use TensorFlow calculations. Defaults TRUE. tf_environ Character. Path Python environment. argument required use_tf TRUE Windows. Defaults NULL. tf_use_single Logical. Whether use single precision TensorFlow calculations. Defaults FALSE. lf_out_file Character. Path save outputs. NULL (default), predicted latent factors saved file. end either *.qs2 *.RData. lf_return Logical. Whether output returned. Defaults FALSE. lf_out_file NULL, parameter set FALSE function needs return result saved file. lf_input_file Character. File name latent factor predictions saved. NULL (default), latent factor predictions computed. specified, latent factor predictions read path. allows predicting latent factors new sites . lf_only Logical. Whether return latent factor predictions . Defaults FALSE. helps predicting new sites, allowing predicting latent factors , output can loaded predictions needed. n_cores_lf Integer. Number cores use parallel processing latent factor prediction. Defaults 8L. lf_check Logical. TRUE, function checks output files already created valid. FALSE, function check files exist without checking integrity. Default FALSE. lf_temp_cleanup Logical. Whether delete temporary files temp_dir directory finishing LF predictions. lf_commands_only Logical. TRUE, returns command run Python script. Default FALSE. pred_directory Character. Directory path indicating predictions saved. Defaults NULL, saves model predictions \"model_prediction\" folder current working directory. pred_pa matrix. Presence-absence data evaluation. NULL (default), presence-absence data model object used. argument used evaluate TRUE. pred_xy matrix. Coordinates added predicted values. NULL (default), coordinates model object used. evaluate Logical. Whether evaluate model predictions. Defaults FALSE. evaluation_name Character. Name evaluation results. NULL, default name used (eval_[model_name].qs2). evaluation_directory Character. Directory evaluation results saved. Defaults evaluation. verbose Logical. Whether print message upon successful saving files. Defaults FALSE. spatial_model Logical. Whether fitted model spatial model. Defaults TRUE.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Predict_Maps.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict habitat suitability of Hmsc models — predict_maps","title":"Predict habitat suitability of Hmsc models — predict_maps","text":"package provides two functions predicting habitat suitability Hmsc models IASDT framework. predict_maps generates current future habitat suitability maps (mean, sd, cov) full Hmsc model fit. predict_maps_cv predicts evaluates cross-validated Hmsc models current climate conditions. details, see respective function documentation details section .","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Predict_Maps.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict habitat suitability of Hmsc models — predict_maps","text":"","code":"predict_maps(   path_model = NULL,   hab_abb = NULL,   env_file = \".env\",   n_cores = 8L,   strategy = \"multisession\",   future_max_size = 1000L,   clamp_pred = TRUE,   fix_efforts = \"q90\",   fix_rivers = \"q90\",   pred_new_sites = TRUE,   use_tf = TRUE,   tf_environ = NULL,   tf_use_single = FALSE,   n_cores_lf = n_cores,   lf_check = FALSE,   lf_temp_cleanup = TRUE,   lf_only = FALSE,   lf_commands_only = FALSE,   temp_dir = \"temp_pred\",   temp_cleanup = TRUE,   tar_predictions = TRUE,   spatial_model = TRUE,   n_cores_pred = n_cores,   climate_models = c(\"GFDL-ESM4\", \"IPSL-CM6A-LR\", \"MPI-ESM1-2-HR\", \"MRI-ESM2-0\",     \"UKESM1-0-LL\"),   climate_scenario = c(\"ssp126\", \"ssp370\", \"ssp585\") )  predict_maps_cv(   model_dir = NULL,   cv_name = NULL,   cv_fold = NULL,   n_cores = 8L,   strategy = \"multisession\",   env_file = \".env\",   use_tf = TRUE,   tf_environ = NULL,   tf_use_single = FALSE,   n_cores_lf = n_cores,   lf_check = FALSE,   lf_temp_cleanup = TRUE,   lf_only = FALSE,   lf_commands_only = FALSE,   temp_cleanup = TRUE )"},{"path":"https://biodt.github.io/IASDT.R/reference/Predict_Maps.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict habitat suitability of Hmsc models — predict_maps","text":"path_model Character. Path fitted Hmsc model object. hab_abb Character. Habitat abbreviation indicating specific SynHab habitat type. Valid values: 0, 1, 2, 3, 4a, 4b, 10, 12a, 12b. See Pysek et al. details. env_file Character. Path environment file containing paths data sources. Defaults .env. n_cores Integer. Number CPU cores use parallel processing. Default: 8. strategy Character. parallel processing strategy use. Valid options \"sequential\", \"multisession\" (default), \"multicore\", \"cluster\". See future::plan() ecokit::set_parallel() details. future_max_size Numeric. Maximum allowed total size (megabytes) global variables identified. See future.globals.maxSize argument future::future.options details. clamp_pred Logical indicating whether clamp sampling efforts single value. TRUE (default), fix_efforts argument must provided. fix_efforts Numeric character. clamp_pred = TRUE, fixes sampling efforts predictor value predictions. numeric, uses value directly (log10 scale). character, must one identity (.e., fix), median, mean, max, q90 (90% quantile). Using max may reflect extreme sampling efforts highly sampled locations, q90 captures high sampling areas without extremes. Required clamp_pred = TRUE. fix_rivers Numeric, character, NULL. Similar fix_efforts, river length predictor. NULL, river length fixed. Default: q90. pred_new_sites Logical. Whether predict suitability new sites. Default: TRUE. use_tf Logical. Whether use TensorFlow calculations. Defaults TRUE. tf_environ Character. Path Python environment. argument required use_tf TRUE Windows. Defaults NULL. tf_use_single Logical. Whether use single precision TensorFlow calculations. Defaults FALSE. n_cores_lf Integer. Number cores use parallel processing latent factor prediction. Defaults 8L. lf_check Logical. TRUE, function checks output files already created valid. FALSE, function check files exist without checking integrity. Default FALSE. lf_temp_cleanup Logical. Whether delete temporary files temp_dir directory finishing LF predictions. lf_only Logical. Whether predict latent factor. useful distributing processing load GPU CPU. lf_only = TRUE, latent factor prediction needs computed separately GPU. computations finished GPU, function can later rerun lf_only = FALSE (default) predict habitat suitability using already-computed latent factor predictions. lf_commands_only Logical. TRUE, returns command run Python script. Default FALSE. temp_dir Character. Path temporary storage intermediate files. temp_cleanup Logical. Whether clean temporary files. Defaults TRUE. tar_predictions Logical. Whether compress tiff files predicted habitat suitability single *.tar file (without compression). Default: TRUE. spatial_model Logical. Whether fitted model spatial model. Defaults TRUE. n_cores_pred Integer. Number cores use predicting species' habitat suitability. climate_models Character vector. Climate models future predictions. Available options c(\"GFDL-ESM4\", \"IPSL-CM6A-LR\", \"MPI-ESM1-2-HR\", \"MRI-ESM2-0\", \"UKESM1-0-LL\") (default). climate_scenario Character vector. Climate scenarios future predictions. Available options : c(\"ssp126\", \"ssp370\", \"ssp585\") (default). model_dir Character. Path directory containing cross-validated models. cv_name Character. Cross-validation strategy. Valid values cv_dist, cv_large, cv_sac. cv_fold Integer. cross-validation fold number.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Predict_Maps.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict habitat suitability of Hmsc models — predict_maps","text":"predict_maps: Generates habitat suitability maps Hmsc models fitted full dataset, current future climate options. produces maps mean, standard deviation (sd), coefficient variation (cov) suitability species overall species richness. evaluate model's explanatory power using various metrics. future predictions, also generates anomaly maps (future - current). function supports ensemble predictions across multiple climate models prepares data upload OPeNDAP server use IASDT Shiny App. predict_maps_cv: Computes predictions cross-validated Hmsc models using testing folds. evaluates model performance (predictive power) various metrics plots evaluation results predictive explanatory power. Unlike predict_maps, function perform clamping generate future climate predictions.","code":""},{"path":[]},{"path":"https://biodt.github.io/IASDT.R/reference/Predict_Maps.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predict habitat suitability of Hmsc models — predict_maps","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Railway_Intensity.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate railway intensity based on OpenStreetMap data — railway_intensity","title":"Calculate railway intensity based on OpenStreetMap data — railway_intensity","text":"function downloads, processes, analyses railway data extracted OpenRailwayMap available OpenStreetMap Data Extracts. supports parallel processing faster execution can calculate total length railways distance nearest railway grid cell Europe.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Railway_Intensity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate railway intensity based on OpenStreetMap data — railway_intensity","text":"","code":"railway_intensity(   env_file = \".env\",   n_cores = 6L,   strategy = \"multisession\",   delete_processed = TRUE )"},{"path":"https://biodt.github.io/IASDT.R/reference/Railway_Intensity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate railway intensity based on OpenStreetMap data — railway_intensity","text":"env_file Character. Path environment file containing paths data sources. Defaults .env. n_cores Integer. Number CPU cores use parallel processing. Default: 8. strategy Character. parallel processing strategy use. Valid options \"sequential\", \"multisession\" (default), \"multicore\", \"cluster\". See future::plan() ecokit::set_parallel() details. delete_processed Logical indicating whether delete raw downloaded railways files processing . helps free large unnecessary file space (> 55 GB). Defaults TRUE.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Railway_Intensity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate railway intensity based on OpenStreetMap data — railway_intensity","text":"NULL. Outputs processed files directories specified environment file.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Railway_Intensity.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate railway intensity based on OpenStreetMap data — railway_intensity","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Response_curves.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare and plot response curve data for Hmsc models — response_curves","title":"Prepare and plot response curve data for Hmsc models — response_curves","text":"rc_*() functions process visualise response curves Hmsc models. support parallel computation optionally return processed data. four functions group: rc_prepare_data(): Prepares response curve data analysis rc_plot_species(): Generates response curve plots individual species rc_plot_species_all(): Generates response curves species together single plot rc_plot_sr(): Plots response curves species richness.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Response_curves.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare and plot response curve data for Hmsc models — response_curves","text":"","code":"rc_prepare_data(   path_model = NULL,   n_grid = 50L,   n_cores = 8L,   strategy = \"multisession\",   future_max_size = 1500L,   return_data = FALSE,   probabilities = c(0.025, 0.5, 0.975),   use_tf = TRUE,   tf_environ = NULL,   tf_use_single = FALSE,   n_cores_lf = n_cores,   lf_check = FALSE,   lf_temp_cleanup = TRUE,   lf_commands_only = FALSE,   temp_dir = \"temp_pred\",   temp_cleanup = TRUE,   verbose = TRUE )  rc_plot_species(   model_dir = NULL,   n_cores = 20,   env_file = \".env\",   return_data = FALSE )  rc_plot_species_all(   model_dir = NULL,   n_cores = 8L,   return_data = FALSE,   plotting_alpha = 0.3 )  rc_plot_sr(   model_dir,   verbose = TRUE,   n_cores = 8L,   future_max_size = 1000L,   strategy = \"multisession\" )"},{"path":"https://biodt.github.io/IASDT.R/reference/Response_curves.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare and plot response curve data for Hmsc models — response_curves","text":"path_model Character. Path file containing fitted Hmsc model. n_grid Integer. Number points along gradient continuous focal variables. Higher values result smoother curves. Default: 50. See Hmsc::constructGradient details. n_cores Integer. Number CPU cores use parallel processing. Defaults 8L functions, except rc_plot_species, defaults 20L. strategy Character. parallel processing strategy use. Valid options \"sequential\", \"multisession\" (default), \"multicore\", \"cluster\". See future::plan() ecokit::set_parallel() details. future_max_size Numeric. Maximum allowed total size (megabytes) global variables identified. See future.globals.maxSize argument future::future.options details. return_data Logical. TRUE, function returns processed data R object. Default: FALSE. probabilities Numeric vector. Quantiles calculate response curve predictions. Default: c(0.025, 0.5, 0.975). See stats::quantile details. use_tf Logical. Whether use TensorFlow calculations. Defaults TRUE. tf_environ Character. Path Python environment. argument required use_tf TRUE Windows. Defaults NULL. tf_use_single Logical. Whether use single precision TensorFlow calculations. Defaults FALSE. n_cores_lf Integer. Number cores use parallel processing latent factor prediction. Defaults 8L. lf_check Logical. TRUE, function checks output files already created valid. FALSE, function check files exist without checking integrity. Default FALSE. lf_temp_cleanup Logical. Whether delete temporary files temp_dir directory finishing LF predictions. lf_commands_only Logical. TRUE, returns command run Python script. Default FALSE. temp_dir Character. Path temporary storage intermediate files. temp_cleanup Logical. Whether clean temporary files. Defaults TRUE. verbose Logical. Whether print message upon successful saving files. Defaults FALSE. model_dir Character. Path root directory containing fitted models. function reads data response_curves_data subdirectory, created rc_prepare_data. env_file Character. Path environment file containing paths data sources. Defaults .env. plotting_alpha Numeric. Opacity level response curve lines (0 = fully transparent, 1 = fully opaque). Default: 0.3.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Response_curves.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Prepare and plot response curve data for Hmsc models — response_curves","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/River_Length.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the length of rivers in each Strahler order per grid cell — river_length","title":"Calculate the length of rivers in each Strahler order per grid cell — river_length","text":"function processes EU-Hydro River Network Database calculate length rivers Strahler number. Strahler number used index river network classification, higher numbers representing larger, significant river segments. function reads processes zip-compressed geographic data (GPKG files), extracts relevant information river segments, computes length rivers Strahler order per grid cell, outputs results raster files RData objects. calculated length represents total length rivers Strahler number larger (e.g., STRAHLER_5, length rivers Strahler values 5 higher).","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/River_Length.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the length of rivers in each Strahler order per grid cell — river_length","text":"","code":"river_length(env_file = \".env\", cleanup = FALSE)"},{"path":"https://biodt.github.io/IASDT.R/reference/River_Length.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the length of rivers in each Strahler order per grid cell — river_length","text":"env_file Character. Path environment file containing paths data sources. Defaults .env. cleanup Logical indicating whether clean temporary files Interim directory finishing calculations. Default: FALSE.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/River_Length.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the length of rivers in each Strahler order per grid cell — river_length","text":"NULL. function outputs processed files specified directories.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/River_Length.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate the length of rivers in each Strahler order per grid cell — river_length","text":"data provides pan-European level photo-interpreted river network, consistent surface interpretation water bodies (lakes wide rivers), drainage model (also called Drainage Network), derived EU-DEM, catchments drainage lines nodes. Data source: EU-Hydro River Network Database v013 | Temporal extent: 2006-2012; Format: Vector (GPKG); Size: 4 GB","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/River_Length.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate the length of rivers in each Strahler order per grid cell — river_length","text":"DOI: https://doi.org/10.2909/393359a7-7ebd-4a52-80ac-1a18d5f3db9c Download link: https://land.copernicus.eu/en/products/eu-hydro/eu-hydro-river-network-database","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/River_Length.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate the length of rivers in each Strahler order per grid cell — river_length","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Road_Intensity.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate road intensity per grid cell — road_intensity","title":"Calculate road intensity per grid cell — road_intensity","text":"function downloads, processes, analyses GRIP global roads data (Meijer et al. 2018). function calculates total road lengths distance nearest road per grid cell (road type per road type).","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Road_Intensity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate road intensity per grid cell — road_intensity","text":"","code":"road_intensity(env_file = \".env\")"},{"path":"https://biodt.github.io/IASDT.R/reference/Road_Intensity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate road intensity per grid cell — road_intensity","text":"env_file Character. Path environment file containing paths data sources. Defaults .env.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Road_Intensity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate road intensity per grid cell — road_intensity","text":"NULL. function outputs processed files specified directories.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Road_Intensity.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Calculate road intensity per grid cell — road_intensity","text":"function downloads recent version Global Roads Inventory Project (GRIP) data URL specified environment variable DP_R_roads_url. Original data format zipped file containing global road data form fgdb (EPSG:3246). LUMI HPC, loading libarchive module necessary use archive R package: module load libarchive/3.6.2-cpeGNU-23.09 distance roads calculated determining distance grid cell nearest grid cell overlaps road (nearest road line). Note different calculating actual distance nearest road line, computationally intensive performed function.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Road_Intensity.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate road intensity per grid cell — road_intensity","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Variance_partitioning.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes and visualise variance partitioning of Hmsc models — variance_partitioning","title":"Computes and visualise variance partitioning of Hmsc models — variance_partitioning","text":"Computes plots variance components respect given grouping fixed effects levels random effects. variance_partitioning_compute() function inherits main functionality Hmsc::computeVariancePartitioning function, added functionality parallel computation using TensorFlow. variance_partitioning_plot() function generates plots variance partitioning JPEG files. allows sorting predictors species; e.g., mean value per predictor; original species order. also plots raw variance partitioning (relative variance partitioning multiplied training testing (supported) Tjur-R2 value).","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Variance_partitioning.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes and visualise variance partitioning of Hmsc models — variance_partitioning","text":"","code":"variance_partitioning_compute(   path_model,   group = NULL,   group_names = NULL,   start = 1L,   na.ignore = FALSE,   n_cores = 8L,   use_tf = TRUE,   tf_environ = NULL,   tf_use_single = FALSE,   temp_cleanup = TRUE,   chunk_size = 50L,   verbose = TRUE,   vp_file = \"varpar\",   vp_commands_only = FALSE,   temp_dir = NULL )  variance_partitioning_plot(   path_model = NULL,   env_file = \".env\",   vp_file = \"varpar\",   use_tf = TRUE,   tf_environ = NULL,   n_cores = 1L,   width = 30,   height = 15,   axis_text = 4,   spatial_model = TRUE,   is_cv_model = FALSE,   temp_dir = NULL )"},{"path":"https://biodt.github.io/IASDT.R/reference/Variance_partitioning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes and visualise variance partitioning of Hmsc models — variance_partitioning","text":"path_model Character. Path fitted Hmsc model object. group vector numeric values corresponding group identifiers groupnames. model defined XData XFormula, default use model terms. group_names vector names group fixed effect. match group. model defined XData XFormula, default use labels model terms. start index first MCMC sample included. Default: 1L. na.ignore Logical. TRUE, covariates ignored sites focal species NA computing variance-covariance matrices species. n_cores Integer. Number CPU cores use computing variance partitioning using TensorFlow. effective use_tf TRUE. Default: 1. use_tf Logical. Whether use TensorFlow calculations. Defaults TRUE. tf_environ Character. Path Python environment. argument required use_tf TRUE Windows. Defaults NULL. tf_use_single Logical. Whether use single precision TensorFlow calculations. Defaults FALSE. temp_cleanup Logical. Whether delete temporary files processing. Default: TRUE. chunk_size Integer. Size chunk samples process parallel. relevant TensorFlow. Default: 50. verbose Logical. Whether print progress messages. Default: TRUE. vp_file Character. Name output file save results. Default: varpar. vp_commands_only Logical. TRUE, returns commands run Python script. Default FALSE. relevant use_tf TRUE. temp_dir Character. Path temporary directory store intermediate files. Default: NULL, creates temporary directory parent directory model file. env_file Character. Path environment file containing paths data sources. Defaults .env. width, height Numeric. Width height output plot centimetres. Default: 30 15, respectively. axis_text Numeric. Size axis text. Default: 4. spatial_model Logical. Whether fitted model spatial model. Defaults TRUE. is_cv_model Logical. Whether model cross-validated model (TRUE) fitted full dataset (FALSE; default). TRUE, explanatory predictive power model used estimate raw variance partitioning.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/Variance_partitioning.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Computes and visualise variance partitioning of Hmsc models — variance_partitioning","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/cpp_functions.html","id":null,"dir":"Reference","previous_headings":"","what":"helper C++ functions for fast matrix computations — cpp_functions","title":"helper C++ functions for fast matrix computations — cpp_functions","text":"collection efficient C++ functions using RcppArmadillo common matrix operations, including solving linear systems, computing matrix inverses, approximating normal CDFs, fast elementwise transformations.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/cpp_functions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"helper C++ functions for fast matrix computations — cpp_functions","text":"","code":"solve1(x)  solve2(A, B)  solve2vect(A, B)  fast_pnorm(x)  exp_neg_div(A, x)"},{"path":"https://biodt.github.io/IASDT.R/reference/cpp_functions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"helper C++ functions for fast matrix computations — cpp_functions","text":"x numeric matrix, numeric vector, numeric scalar (depending function). numeric matrix (solving, exponential operations, etc.). B numeric matrix vector (right-hand side solving linear systems).","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/cpp_functions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"helper C++ functions for fast matrix computations — cpp_functions","text":"solve1(): numeric matrix, inverse x. solve2(): numeric matrix, solution * X = B. solve2vect(): numeric vector, solution * x = B. fast_pnorm(): numeric vector, CDF values approximated standard normal distribution. exp_neg_div(): numeric matrix, elementwise exponential -/x.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/cpp_functions.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"helper C++ functions for fast matrix computations — cpp_functions","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/cpp_functions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"helper C++ functions for fast matrix computations — cpp_functions","text":"","code":"# ----------------------------------------- # Example for solve1 # -----------------------------------------  N <- 100 set.seed(1000) Matrix <- matrix(rnorm(N * N), N, N) all.equal(solve(Matrix), IASDT.R::solve1(Matrix)) #> [1] TRUE  # ----------------------------------------- # Example for solve2 # -----------------------------------------  N <- 100 set.seed(1000) A <- matrix(rnorm(N * N), N, N) set.seed(2000) B <- matrix(rnorm(N * N), N, N) all.equal(solve(A, B), IASDT.R::solve2(A, B)) #> [1] TRUE  set.seed(2000) B <- matrix(rnorm(N), N, 1) all.equal(solve(A, B), IASDT.R::solve2(A, B)) #> [1] TRUE  # ----------------------------------------- # Example for solve2vect # -----------------------------------------  N <- 100 set.seed(1000) A <- matrix(rnorm(N * N), N, N) set.seed(2000) B <- rnorm(N) all.equal(solve(A, B), as.vector(IASDT.R::solve2vect(A, B))) #> [1] TRUE  # ----------------------------------------- # Example for fast_pnorm # -----------------------------------------  set.seed(1000) A <- rnorm(100) all.equal(pnorm(A), IASDT.R::fast_pnorm(A)) #> [1] TRUE  # ----------------------------------------- # Example for exp_neg_div # -----------------------------------------  N <- 1000 set.seed(1000) A <- matrix(rnorm(N * N), N, N) set.seed(2000) x <- rnorm(1) all.equal(exp(-A / x), IASDT.R::exp_neg_div(A, x)) #> [1] TRUE"},{"path":"https://biodt.github.io/IASDT.R/reference/eLTER_Process.html","id":null,"dir":"Reference","previous_headings":"","what":"Process eLTER data for the IASDT — elter_process","title":"Process eLTER data for the IASDT — elter_process","text":"function processes pre-cleaned pre-standardized Integrated European Long-Term Ecosystem, critical zone socio-ecological Research (eLTER) data Invasive Alien Species Digital Twin (IASDT).","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/eLTER_Process.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process eLTER data for the IASDT — elter_process","text":"","code":"elter_process(env_file = \".env\", start_year = 1981)"},{"path":"https://biodt.github.io/IASDT.R/reference/eLTER_Process.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process eLTER data for the IASDT — elter_process","text":"env_file Character. Path environment file containing paths data sources. Defaults .env. start_year Numeric. starting year occurrence data. records year onward processed. Default 1981, matches year ranges CHELSA current climate data.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/eLTER_Process.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process eLTER data for the IASDT — elter_process","text":"Returns NULL invisibly saving processed data.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/eLTER_Process.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Process eLTER data for the IASDT — elter_process","text":"function processes pre-cleaned vascular plants data eLTER sites, harmonized Ahmed El-Gabbas. original eLTER biodiversity data highly heterogeneous format structure, requiring standardization cleaning use. Taxonomic standardization GBIF backbone performed Marina Golivets (Feb. 2024).","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/eLTER_Process.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Process eLTER data for the IASDT — elter_process","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/fit_sdm_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Species Distribution Modelling Workflow for Single-Species Models — fit_sdm_models","title":"Species Distribution Modelling Workflow for Single-Species Models — fit_sdm_models","text":"comprehensive workflow implements single-species species distribution models (sSDMs) invasive alien plant species Europe habitat level. orchestrates entire process data preparation model fitting, evaluation, prediction across current future climate scenarios. workflow employs sdm R package model fitting handles cross-validation, parallel processing, various environmental predictors.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/fit_sdm_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Species Distribution Modelling Workflow for Single-Species Models — fit_sdm_models","text":"","code":"fit_sdm_models(   sdm_method = NULL,   model_settings = NULL,   model_dir = NULL,   hab_abb = NULL,   cv_type = \"cv_dist\",   n_cores = 8L,   future_max_size = 2000L,   selected_species = NULL,   excluded_species = NULL,   env_file = \".env\",   clamp_pred = TRUE,   fix_efforts = \"q90\",   fix_rivers = \"q90\",   climate_models = \"all\",   climate_scenarios = \"all\",   climate_periods = \"all\",   copy_maxent_html = TRUE )"},{"path":"https://biodt.github.io/IASDT.R/reference/fit_sdm_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Species Distribution Modelling Workflow for Single-Species Models — fit_sdm_models","text":"sdm_method Character. single SDM algorithm use fitting models. Valid values: \"glm\", \"glmpoly\", \"gam\", \"glmnet\", \"mars\", \"gbm\", \"rf\", \"ranger\", \"cart\", \"rpart\", \"maxent\", \"mlp\", \"rbf\", \"svm\", \"mda\", \"fda\". correspond selected methods supported sdm package. details supported options, see sdm::getmethodNames(). model_settings List NULL. List model-specific settings. NULL, defaults custom settings defined within workflow. model_dir Character. Path directory containing model data outputs results saved. Model data prepared using mod_prepare_hpc() mod_prepare_data() functions. hab_abb Character. Abbreviation single SynHab habitat type. Valid values: \"0\", \"1\", \"2\", \"3\", \"4a\", \"4b\", \"10\", \"12a\", \"12b\". See mod_prepare_hpc() details. cv_type Character. Cross-validation type. One cv_dist (default) cv_large. See mod_cv_fit() details. n_cores Integer. Number CPU cores parallel processing. Default 8. future_max_size Numeric. Maximum allowed total size (megabytes) global variables identified. See ecokit::set_parallel() future.globals.maxSize argument future::future.options() details. selected_species, excluded_species Character vector NULL. Names species include exclude modelling. env_file Character. Path file environment variable definitions spatial datasets. Default \".env\". clamp_pred Logical. clamping applied sampling efforts river length predictors prediction? Default TRUE. fix_efforts, fix_rivers Character numeric (length 1). Method fixed value sampling effort river length (log-scale) clamping enabled (clamp_pred = TRUE). Valid methods: \"identity\" (use observed, clamping), summary statistics sampling efforts layer (\"median\", \"mean\", \"max\", \"q90\" (default; 90th percentile)), single numeric value within observed range. climate_models Character vector \"\". climate change models use future projections. Valid values (case-sensitive): \"GFDL-ESM4\", \"IPSL-CM6A-LR\", \"MPI-ESM1-2-HR\", \"MRI-ESM2-0\", \"UKESM1-0-LL\", \"\" (default, meaning available models). subset, must subset listed valid models. climate_scenarios Character vector \"\". climate change scenarios use future projections. Valid values: \"ssp126\", \"ssp370\", \"ssp585\", \"\" (default, meaning available scenarios). subset, must subset listed valid scenarios. climate_periods Character vector \"\". Time periods prediction. Valid values \"2011-2040\", \"2041-2070\", \"2071-2100\", \"\" (default), subset supported periods. copy_maxent_html Logical. Whether copy directory containing HTML results Maxent modelling directory. Default TRUE.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/fit_sdm_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Species Distribution Modelling Workflow for Single-Species Models — fit_sdm_models","text":"tibble summarizing model results species, including: Evaluation metrics training testing data (AUC, TSS, Kappa, etc.) Variable importance scores Response curves environmental variable Prediction summaries current future climate scenarios Paths generated model files prediction rasters Additionally, function saves various outputs disk future use: Fitted model objects (.RData files) Extracted model information (evaluation metrics, variable importance, etc.) Prediction rasters species, cross-validation fold, climate scenario Summary statistics across CV folds (mean, weighted mean, SD, coefficient variation) Species richness maps climate scenario","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/fit_sdm_models.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Species Distribution Modelling Workflow for Single-Species Models — fit_sdm_models","text":"fit_sdm_models function orchestrates comprehensive workflow handles aspects single-species distribution modelling invasive alien plant species Europe. workflow integrates several internal components manage different stages modelling process: Overall workflow: Input validation: Checks parameters validity Data preparation: Loads processes model data Parallel processing setup: Configures computational resources Model fitting prediction: species CV fold Results summarization: Compiles metrics, variable importance, predictions Species richness calculation: Across modelled species Core capabilities: Data preparation: workflow validates prepares necessary input data including modelling data, environmental predictors, prediction datasets. handles species selection, data loading, preprocessing spatial predictors (including clamping sampling efforts river length required). Model parameterization: function provides carefully selected default settings various SDM algorithms, ensuring consistent parameterization across models. Model information extraction: fitting, workflow automatically extracts key information fitted SDM objects, including evaluation metrics, variable importance, response curves. Model optimization: Technical improvements like optimizing SDM model object size setting formula environments base environment address known issues sdm package. Parallel prediction: workflow efficiently generates predictions species cross-validation fold, handling model fitting, information extraction, prediction, file saving parallel. Statistical summarization: Summary statistics calculated across cross-validation folds, including mean, weighted mean (test AUC), standard deviation, coefficient variation predictions.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/fit_sdm_models.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Species Distribution Modelling Workflow for Single-Species Models — fit_sdm_models","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/get_species_name.html","id":null,"dir":"Reference","previous_headings":"","what":"Get species name or information of an IASDT species ID — get_species_name","title":"Get species name or information of an IASDT species ID — get_species_name","text":"function retrieves detailed information IASDT species list, optionally filtered specific IASDT species ID (species_id).","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/get_species_name.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get species name or information of an IASDT species ID — get_species_name","text":"","code":"get_species_name(species_id = NULL, env_file = \".env\")"},{"path":"https://biodt.github.io/IASDT.R/reference/get_species_name.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get species name or information of an IASDT species ID — get_species_name","text":"species_id optional IASDT species ID detailed information required. provided, function returns entire species list. env_file Character. Path environment file containing paths data sources. Defaults .env.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/get_species_name.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get species name or information of an IASDT species ID — get_species_name","text":"data frame containing species information. species ID species_id provided, returns species information listed species, otherwise return full list IAS.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/get_species_name.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get species name or information of an IASDT species ID — get_species_name","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/install_hmsc_windows.html","id":null,"dir":"Reference","previous_headings":"","what":"Install Hmsc-HPC in a python virtual environment on Windows — install_hmsc_windows","title":"Install Hmsc-HPC in a python virtual environment on Windows — install_hmsc_windows","text":"function sets Python virtual environment installs Hmsc-HPC package specified Git repository. also installs TensorFlow performs checks verify installation Python, virtual environment, packages.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/install_hmsc_windows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Install Hmsc-HPC in a python virtual environment on Windows — install_hmsc_windows","text":"","code":"install_hmsc_windows(   path_python,   path_ve,   url_hmsc = \"https://github.com/trossi/hmsc-hpc.git@simplify-io\",   url_rdata = \"https://github.com/trossi/rdata.git@test-for-hmsc-v2\",   force = FALSE )"},{"path":"https://biodt.github.io/IASDT.R/reference/install_hmsc_windows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Install Hmsc-HPC in a python virtual environment on Windows — install_hmsc_windows","text":"path_python Character. Path Python executable. path_ve Character. Path virtual environment created. can existing folder. url_hmsc Character. URL Git repository (branch name) Hmsc-HPC package. url_rdata Character. URL Git repository (branch name) rdata package. temporary allow rdata package write rds file. near future, functionality pushed main branch rdata. force Logical. Whether force installation Hmsc-HPC; .e. using --force-reinstall suffix pip install command","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/install_hmsc_windows.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Install Hmsc-HPC in a python virtual environment on Windows — install_hmsc_windows","text":"function performs installation steps returns NULL invisibly.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/install_hmsc_windows.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Install Hmsc-HPC in a python virtual environment on Windows — install_hmsc_windows","text":"function performs following steps: Checks virtual environment directory already exists stops error Verifies Python version installation Creates new Python virtual environment Upgrades pip virtual environment Installs Hmsc-HPC package provided Git URL Installs TensorFlow version 2.15 Installs rdata Checks TensorFlow Hmsc-HPC package installations","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/install_hmsc_windows.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Install Hmsc-HPC in a python virtual environment on Windows — install_hmsc_windows","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/install_hmsc_windows.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Install Hmsc-HPC in a python virtual environment on Windows — install_hmsc_windows","text":"","code":"if (FALSE) { # \\dontrun{ install_hmsc_windows(    path_python = \"C:/Python/Python310/python.exe\",    path_ve = \"D:/Hmsc-HPC\") } # }"},{"path":"https://biodt.github.io/IASDT.R/reference/mod_CV_prepare.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare spatial-block cross-validation folds for spatial analysis — mod_cv_prepare","title":"Prepare spatial-block cross-validation folds for spatial analysis — mod_cv_prepare","text":"function assign modelling input data spatial-block cross-validation folds using three strategies (see ) using blockCV::cv_spatial. function planned used inside mod_prepare_hpc function.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_CV_prepare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare spatial-block cross-validation folds for spatial analysis — mod_cv_prepare","text":"","code":"mod_cv_prepare(   input_data = NULL,   env_file = \".env\",   x_vars = NULL,   cv_n_folds = 4L,   cv_n_grids = 20L,   cv_n_rows = 2L,   cv_n_columns = 2L,   cv_sac = FALSE,   out_path = NULL )"},{"path":"https://biodt.github.io/IASDT.R/reference/mod_CV_prepare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare spatial-block cross-validation folds for spatial analysis — mod_cv_prepare","text":"input_data data.frame. data frame tibble containing input dataset. data frame include two columns x y coordinates long columns matching names predictors listed x_vars argument. argument mandatory can empty. env_file Character. Path environment file containing paths data sources. Defaults .env. x_vars Character vector. Variables used model. argument mandatory can empty. cv_n_folds Integer. Number cross-validation folds. Default: 4L. cv_n_grids Integer. Number grid cells directions used cv_dist cross-validation strategy (see ). Default: 20L. cv_n_rows, cv_n_columns Integer. Number rows columns used cv_large cross-validation strategy (see ), study area divided large blocks given provided cv_n_rows cv_n_columns values. default 2L means split study area four large blocks median latitude longitude. cv_sac Logical. Whether use spatial autocorrelation determine block size. Defaults FALSE, out_path Character. Path directory save cross-validation results. argument mandatory can empty.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_CV_prepare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare spatial-block cross-validation folds for spatial analysis — mod_cv_prepare","text":"function returns modified version input dataset additional numeric columns (integer) indicating cross-validation strategy used.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_CV_prepare.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Prepare spatial-block cross-validation folds for spatial analysis — mod_cv_prepare","text":"function uses following cross-validation strategies: cv_dist size spatial cross-validation blocks determined cv_n_grids argument. default cv_n_grids value 20L, means blocks 20×20 grid cell . cv_large splits study area large blocks, determined  cv_n_rows cv_n_columns arguments. cv_n_rows = cv_n_columns = 2L (default), four large blocks used, split study area median coordinates. cv_sac size blocks determined median spatial autocorrelation range predictor data (estimated using blockCV::cv_spatial_autocor). requires availability automap R package. strategy currently skipped default.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_CV_prepare.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Prepare spatial-block cross-validation folds for spatial analysis — mod_cv_prepare","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_cv_evaluate.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-validation Model Evaluation and Plotting — mod_cv_evaluate","title":"Cross-validation Model Evaluation and Plotting — mod_cv_evaluate","text":"performs model evaluation using cross-validation results, calculates multiple metrics (AUC, Tjur R2, Boyce index, RMSE), generates summary plots explanatory predictive power.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_cv_evaluate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-validation Model Evaluation and Plotting — mod_cv_evaluate","text":"","code":"mod_cv_evaluate(model_dir = NULL, cv_type = \"cv_dist\")"},{"path":"https://biodt.github.io/IASDT.R/reference/mod_cv_evaluate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-validation Model Evaluation and Plotting — mod_cv_evaluate","text":"model_dir Character. Path root directory fitted model. cv_type Character. Cross-validation type. One cv_dist (default) cv_large. See mod_cv_fit() details.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_cv_evaluate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-validation Model Evaluation and Plotting — mod_cv_evaluate","text":"Invisibly returns path saved evaluation data file.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_cv_evaluate.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Cross-validation Model Evaluation and Plotting — mod_cv_evaluate","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_cv_evaluate_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Evaluation Results for Cross-Validated Hmsc Models — mod_cv_evaluate_plot","title":"Plot Evaluation Results for Cross-Validated Hmsc Models — mod_cv_evaluate_plot","text":"function evaluates cross-validation results Hmsc models (spatial /non-spatial), summarizes performance metrics (RMSE, TjurR2, AUC, Boyce), generates diagnostic plots comparing model types.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_cv_evaluate_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Evaluation Results for Cross-Validated Hmsc Models — mod_cv_evaluate_plot","text":"","code":"mod_cv_evaluate_plot(   model_prefix = NULL,   hab_abb = NULL,   n_cv_folds = 4,   spatial_model = c(\"gpp\", \"nonspatial\") )"},{"path":"https://biodt.github.io/IASDT.R/reference/mod_cv_evaluate_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Evaluation Results for Cross-Validated Hmsc Models — mod_cv_evaluate_plot","text":"model_prefix Character. Prefix model directory name. hab_abb Character. Habitat abbreviation. n_cv_folds Integer. Number cross-validation folds (default: 4L). spatial_model Character vector. Specifies models evaluate: \"gpp\", \"nonspatial\", .","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_cv_evaluate_plot.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot Evaluation Results for Cross-Validated Hmsc Models — mod_cv_evaluate_plot","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_cv_merge_predictions.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge Cross-Validation Predictions and Generate Summary Maps — mod_cv_merge_predictions","title":"Merge Cross-Validation Predictions and Generate Summary Maps — mod_cv_merge_predictions","text":"Merges prediction results cross-validated Hmsc models, processes clamped -clamped model outputs, generates summary maps anomaly visualizations species invasion predictions. saves processed prediction data map images disk.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_cv_merge_predictions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge Cross-Validation Predictions and Generate Summary Maps — mod_cv_merge_predictions","text":"","code":"mod_cv_merge_predictions(   model_prefix = NULL,   n_cv_folds = 4L,   hab_abb = NULL,   spatial_model = NULL,   n_cores = 8L,   clamp = c(\"clamp\", \"no_clamp\") )"},{"path":"https://biodt.github.io/IASDT.R/reference/mod_cv_merge_predictions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge Cross-Validation Predictions and Generate Summary Maps — mod_cv_merge_predictions","text":"model_prefix Character. Prefix model output directories files. n_cv_folds Integer. Number cross-validation folds. Default 4L. hab_abb Character. Habitat abbreviation process. spatial_model Logical. Whether use spatial model (TRUE) non-spatial (FALSE). n_cores Integer. Number cores parallel processing. Default 8L. clamp Character vector. Indicates prediction types process: \"clamp\", \"no_clamp\", .","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_cv_merge_predictions.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Merge Cross-Validation Predictions and Generate Summary Maps — mod_cv_merge_predictions","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_fit_windows.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit Hmsc-HPC models on UFZ Windows Server — mod_fit_windows","title":"Fit Hmsc-HPC models on UFZ Windows Server — mod_fit_windows","text":"function fits Hmsc models UFZ Windows Server. reads model configurations specified path, loads environment variables, checks input arguments validity, executes model fitting parallel required.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_fit_windows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit Hmsc-HPC models on UFZ Windows Server — mod_fit_windows","text":"","code":"mod_fit_windows(   path_model = NULL,   python_ve = NULL,   n_cores = NULL,   strategy = \"multisession\" )"},{"path":"https://biodt.github.io/IASDT.R/reference/mod_fit_windows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit Hmsc-HPC models on UFZ Windows Server — mod_fit_windows","text":"path_model Character. Path model files. argument can empty. python_ve Character. Path valid Python virtual environment. Defaults NULL. argument can empty. n_cores Integer. Number CPU cores use parallel processing. strategy Character. parallel processing strategy use. Valid options \"sequential\", \"multisession\" (default), \"multicore\", \"cluster\". See future::plan() ecokit::set_parallel() details.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_fit_windows.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit Hmsc-HPC models on UFZ Windows Server — mod_fit_windows","text":"function return anything prints messages console regarding progress completion model fitting.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_fit_windows.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fit Hmsc-HPC models on UFZ Windows Server — mod_fit_windows","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_get_posteriors.html","id":null,"dir":"Reference","previous_headings":"","what":"Combines posteriors exported by Hmsc-HPC into an Hmsc object — mod_get_posteriors","title":"Combines posteriors exported by Hmsc-HPC into an Hmsc object — mod_get_posteriors","text":"function converts posterior files exported Hmsc-HPC Hmsc object. can either read data directly RDS files convert JSON format specified.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_get_posteriors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combines posteriors exported by Hmsc-HPC into an Hmsc object — mod_get_posteriors","text":"","code":"mod_get_posteriors(path_posterior = NULL, from_json = FALSE)"},{"path":"https://biodt.github.io/IASDT.R/reference/mod_get_posteriors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combines posteriors exported by Hmsc-HPC into an Hmsc object — mod_get_posteriors","text":"path_posterior Character vector. Path RDS files containing exported posterior files. argument mandatory empty. from_json Logical. Whether loaded models converted JSON format. Defaults FALSE, meaning data read directly RDS files without conversion.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_get_posteriors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combines posteriors exported by Hmsc-HPC into an Hmsc object — mod_get_posteriors","text":"Depending from_json parameter, returns Hmsc object either directly RDS files converting JSON format.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/mod_get_posteriors.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Combines posteriors exported by Hmsc-HPC into an Hmsc object — mod_get_posteriors","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/naps_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Process and map Naturalized Alien Plant Species (NAPS) data for the IASDT — naps_data","title":"Process and map Naturalized Alien Plant Species (NAPS) data for the IASDT — naps_data","text":"Processes visualises Naturalized Alien Plant Species (NAPS) distribution data GBIF, EASIN, eLTER Invasive Alien Species Digital Twin (IASDT). Merges pre-processed data, creates presence-absence rasters, summarises distributions, generates maps using helper functions.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/naps_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process and map Naturalized Alien Plant Species (NAPS) data for the IASDT — naps_data","text":"","code":"naps_process(env_file = \".env\", n_cores = 6L, strategy = \"multisession\")  naps_distribution(   species = NULL,   env_file = \".env\",   verbose = FALSE,   dist_citizen = 100L )  naps_plot(species = NULL, env_file = \".env\")  naps_standardisation(env_file = \".env\")"},{"path":"https://biodt.github.io/IASDT.R/reference/naps_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process and map Naturalized Alien Plant Species (NAPS) data for the IASDT — naps_data","text":"env_file Character. Path environment file containing paths data sources. Defaults .env. n_cores Integer. Number CPU cores use parallel processing. Default: 6. strategy Character. parallel processing strategy use. Valid options \"sequential\", \"multisession\" (default), \"multicore\", \"cluster\". See future::plan() ecokit::set_parallel() details. species Character. Species name distribution mapping. verbose Logical. TRUE, prints progress messages. Default: FALSE. dist_citizen Numeric. Distance km spatial filtering citizen science data GBIF grid cells countries species yet recognized \"naturalized\". Default 100L km.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/naps_data.html","id":"functions-details","dir":"Reference","previous_headings":"","what":"Functions details","title":"Process and map Naturalized Alien Plant Species (NAPS) data for the IASDT — naps_data","text":"naps_standardisation(): Load pre-prepared standardisation information NAPS verbatim names prepare unique ias_id standardised NAPS. function called per workflow version. ias_id determined based sorting standardised species names alphabetically within taxonomic hierarchy (order : class, order, family, taxon_name). new standardisation file used, new ias_id values generated, break reproducibility prevent consistent data linkage across analyses. naps_process(): Merges pre-processed GBIF (gbif_process), EASIN (easin_process), eLTER (elter_process) data (run first). Outputs SpatRaster distribution rasters, summary tables, JPEG maps using naps_distribution() naps_plot(). naps_distribution(): Generates presence-absence maps (.RData, .tif)  species, including grid cells study area set excluding cultivated/casual-countries. Returns file path tibble presence counts (total, source) summary statistics biogeographical regions naps_plot(): Creates JPEG distribution maps GBIF, EASIN, eLTER data using ggplot2.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/naps_data.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Process and map Naturalized Alien Plant Species (NAPS) data for the IASDT — naps_data","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://biodt.github.io/IASDT.R/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/plot_evaluation.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate plots for the explanatory power of Hmsc models — plot_evaluation","title":"Generate plots for the explanatory power of Hmsc models — plot_evaluation","text":"function generates four diagnostic plots (RMSE, AUC, Continuous Boyce Index, Tjur-R²) evaluate performance (explanatory power without cross-validation) Hmsc models.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/plot_evaluation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate plots for the explanatory power of Hmsc models — plot_evaluation","text":"","code":"plot_evaluation(model_dir)"},{"path":"https://biodt.github.io/IASDT.R/reference/plot_evaluation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate plots for the explanatory power of Hmsc models — plot_evaluation","text":"model_dir Character. Path model directory containing predictions.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/plot_evaluation.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Generate plots for the explanatory power of Hmsc models — plot_evaluation","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/plot_gelman.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Gelman-Rubin-Brooks — plot_gelman","title":"Plot Gelman-Rubin-Brooks — plot_gelman","text":"plot_gelman_*() functions generate plots visualising evolution Gelman-Rubin-Brooks shrink factor different model parameters number iterations increases. plots help assess whether MCMC chains converged common distribution. plot includes: median (solid line) 97.5th percentile (dashed line) shrink factor dashed horizontal line 1.1, representing common convergence threshold. primary function users plot_gelman(), internally calls: plot_gelman_alpha(): Plots shrink factor Alpha parameter plot_gelman_beta(): Plots shrink factor Beta parameters plot_gelman_omega(): Plots shrink factor Omega parameter plot_gelman_rho(): Plots shrink factor Rho parameter","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/plot_gelman.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Gelman-Rubin-Brooks — plot_gelman","text":"","code":"plot_gelman(   path_coda = NULL,   alpha = TRUE,   beta = TRUE,   omega = TRUE,   rho = TRUE,   n_omega = 1000L,   plotting_alpha = 0.25,   env_file = \".env\" )  plot_gelman_alpha(coda_object, plotting_alpha = 0.25)  plot_gelman_beta(coda_object, env_file = \".env\", plotting_alpha = 0.25)  plot_gelman_omega(coda_object, n_omega = 1000L, plotting_alpha = 0.25)  plot_gelman_rho(coda_object)"},{"path":"https://biodt.github.io/IASDT.R/reference/plot_gelman.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Gelman-Rubin-Brooks — plot_gelman","text":"path_coda Character. Path file containing coda object, representing MCMC samples. alpha, beta, omega, rho Logical. TRUE, plots Gelman-Rubin statistic respective model parameters (alpha, beta, omega, rho). Default: TRUE parameters. n_omega Integer. Number species sampled omega parameter. Default: 1000L. plotting_alpha Numeric. Transparency level (alpha) plot lines (0 = fully transparent, 1 = fully opaque). Default: 0.25. env_file Character. Path environment file containing paths data sources. Defaults .env. coda_object mcmc.list. MCMC sample object containing posterior distributions Hmsc model.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/plot_gelman.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot Gelman-Rubin-Brooks — plot_gelman","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/plot_latent_factor.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot spatial variation in site loadings of HMSC models — plot_latent_factor","title":"Plot spatial variation in site loadings of HMSC models — plot_latent_factor","text":"Generate save spatial variation site loadings HMSC models' latent factors JPEG file.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/plot_latent_factor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot spatial variation in site loadings of HMSC models — plot_latent_factor","text":"","code":"plot_latent_factor(path_model = NULL, env_file = \".env\")"},{"path":"https://biodt.github.io/IASDT.R/reference/plot_latent_factor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot spatial variation in site loadings of HMSC models — plot_latent_factor","text":"path_model Character. Path fitted Hmsc model object. env_file Character. Path environment file containing paths data sources. Defaults .env.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/plot_latent_factor.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot spatial variation in site loadings of HMSC models — plot_latent_factor","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/plot_prediction.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot species and level of invasion predictions as JPEG files using ggplot2 — plot_prediction","title":"Plot species and level of invasion predictions as JPEG files using ggplot2 — plot_prediction","text":"Generate predictions species habitat models saves output JPEG files.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/plot_prediction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot species and level of invasion predictions as JPEG files using ggplot2 — plot_prediction","text":"","code":"plot_prediction(   model_dir = NULL,   env_file = \".env\",   n_cores = 8L,   is_cv_model = FALSE )"},{"path":"https://biodt.github.io/IASDT.R/reference/plot_prediction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot species and level of invasion predictions as JPEG files using ggplot2 — plot_prediction","text":"model_dir Character. Path model directory containing predictions. env_file Character. Path environment file containing paths data sources. Defaults .env. n_cores Integer. Number CPU cores use parallel processing. Default: 8. is_cv_model Logical. Whether model cross-validated model (TRUE) fitted full dataset (FALSE; default).","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/plot_prediction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot species and level of invasion predictions as JPEG files using ggplot2 — plot_prediction","text":"Saves prediction plots JPEG files specified output directory.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/plot_prediction.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot species and level of invasion predictions as JPEG files using ggplot2 — plot_prediction","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/predict_latent_factor.html","id":null,"dir":"Reference","previous_headings":"","what":"Draws samples from the conditional predictive distribution of latent factors — predict_latent_factor","title":"Draws samples from the conditional predictive distribution of latent factors — predict_latent_factor","text":"function optimized speed using parallel processing optionally TensorFlow matrix operations. function adapted Hmsc::predictLatentFactor equivalent results original function predictMean = TRUE.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/predict_latent_factor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Draws samples from the conditional predictive distribution of latent factors — predict_latent_factor","text":"","code":"predict_latent_factor(   units_pred,   units_model,   post_eta,   post_alpha,   lf_rl,   n_cores_lf = 8L,   strategy = \"multisession\",   future_max_size = 1000L,   temp_dir = \"temp_pred\",   lf_temp_cleanup = TRUE,   model_name = NULL,   use_tf = TRUE,   tf_environ = NULL,   tf_use_single = FALSE,   lf_out_file = NULL,   lf_return = FALSE,   lf_check = FALSE,   lf_commands_only = FALSE,   solve_max_attempts = 5L,   solve_chunk_size = 50L,   verbose = TRUE )"},{"path":"https://biodt.github.io/IASDT.R/reference/predict_latent_factor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Draws samples from the conditional predictive distribution of latent factors — predict_latent_factor","text":"units_pred factor vector random level units predictions made units_model factor vector random level units conditioned post_eta Character. Path post_eta; list containing samples random factors conditioned units post_alpha list containing samples range (lengthscale) parameters latent factors lf_rl HmscRandomLevel-class object describes random level structure n_cores_lf Integer. Number cores use parallel processing latent factor prediction. Defaults 8L. strategy Character. parallel processing strategy use. Valid options \"sequential\", \"multisession\" (default), \"multicore\", \"cluster\". See future::plan() ecokit::set_parallel() details. future_max_size Numeric. Maximum allowed total size (megabytes) global variables identified. See future.globals.maxSize argument future::future.options details. temp_dir Character. Path temporary storage intermediate files. lf_temp_cleanup Logical. Whether delete temporary files temp_dir directory finishing LF predictions. model_name Character. Prefix temporary file names. Defaults NULL, case prefix used. use_tf Logical. Whether use TensorFlow calculations. Defaults TRUE. tf_environ Character. Path Python environment. argument required use_tf TRUE Windows. Defaults NULL. tf_use_single Logical. Whether use single precision TensorFlow calculations. Defaults FALSE. lf_out_file Character. Path save outputs. NULL (default), predicted latent factors saved file. end either *.qs2 *.RData. lf_return Logical. Whether output returned. Defaults FALSE. lf_out_file NULL, parameter set FALSE function needs return result saved file. lf_check Logical. TRUE, function checks output files already created valid. FALSE, function check files exist without checking integrity. Default FALSE. lf_commands_only Logical. TRUE, returns command run Python script. Default FALSE. solve_max_attempts Integer. Maximum number attempts run solve crossprod internal function run_crossprod_solve. Default 5L. solve_chunk_size Integer. Chunk size solve_and_multiply Python function. Default 50L. verbose Logical. TRUE, logs detailed information execution. Default TRUE.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/predict_latent_factor.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Draws samples from the conditional predictive distribution of latent factors — predict_latent_factor","text":"function expected faster original function Hmsc package, especially using TensorFlow calculations working parallel. main difference function: allow parallel processing (n_cores_lf argument); TensorFlow used (use_tf = TRUE), matrix calculations much faster, particularly used GPU. following Python modules needed: numpy, tensorflow, rdata, xarray, pandas. use TensorFlow Windows, argument tf_environ set path Python environment TensorFlow installed; use_tf set FALSE, function uses R (supported relatively faster CPP functions) calculations; d11 d12 matrices processed saved disk called needed.","code":""},{"path":[]},{"path":"https://biodt.github.io/IASDT.R/reference/prepare_knots.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare knot locations for Hmsc GPP models — prepare_knots","title":"Prepare knot locations for Hmsc GPP models — prepare_knots","text":"Prepare locations knots use Gaussian Predictive Process (GPP) models within HMSC framework. ensures knots spaced minimum specified distance applies jitter identical coordinates avoid overlap.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/prepare_knots.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare knot locations for Hmsc GPP models — prepare_knots","text":"","code":"prepare_knots(   coordinates = NULL,   min_distance = NULL,   jitter_distance = 100,   min_lf = NULL,   max_lf = NULL,   alphapw = list(Prior = NULL, Min = 10, Max = 1500, Samples = 101) )"},{"path":"https://biodt.github.io/IASDT.R/reference/prepare_knots.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare knot locations for Hmsc GPP models — prepare_knots","text":"coordinates Numeric matrix data frame containing (x, y) coordinates sampling units. min_distance Numeric. Minimum distance knots meters. distance used knotDist minKnotDist parameters Hmsc::constructKnots function. jitter_distance Numeric. jitter distance applied overlapping coordinates avoid exact duplicates. Defaults 100 meters. min_lf, max_lf Integer. Minimum maximum number latent factors used. default NULL means number latent factors estimated data. either provided, respective values used arguments Hmsc::setPriors. alphapw Prior alpha parameter. Defaults list Prior = NULL, Min = 10, Max = 1500, Samples = 101. alphapw NULL list NULL list items, default prior used. Prior matrix, used prior. Prior = NULL, prior generated using minimum maximum values alpha parameter (min max, respectively; kilometre)  number samples (Samples). Defaults prior 101 samples ranging 10 1500 km, first value second column set 0.5.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/prepare_knots.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare knot locations for Hmsc GPP models — prepare_knots","text":"object class HmscRandomLevel, suitable specifying random level HMSC GPP models. object contains prepared knot locations data frame columns var_1 var_2 (numeric coordinates), possible jittering conversion avoid overlap.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/prepare_knots.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Prepare knot locations for Hmsc GPP models — prepare_knots","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/run_crossprod_solve.html","id":null,"dir":"Reference","previous_headings":"","what":"run_crossprod_solve — run_crossprod_solve","title":"run_crossprod_solve — run_crossprod_solve","text":"Internal function executes Python script performs matrix computations using TensorFlow provided inputs. Retries three times output file validation fails.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/run_crossprod_solve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"run_crossprod_solve — run_crossprod_solve","text":"","code":"run_crossprod_solve(   tf_environ,   s1,   s2,   post_eta,   path_out,   denom,   chunk_size = 1000L,   threshold_mb = 2000L,   tf_use_single = TRUE,   verbose = TRUE,   solve_chunk_size = 50L,   solve_max_attempts = 5L,   lf_commands_only = FALSE )"},{"path":"https://biodt.github.io/IASDT.R/reference/run_crossprod_solve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"run_crossprod_solve — run_crossprod_solve","text":"tf_environ Character. Path Python environment. argument required use_tf TRUE Windows. Defaults NULL. s1 Character. Path input file containing s1 coordinates. s2 Character Path input file containing s2 coordinates. post_eta Character. Path file containing post_eta matrix data. path_out Character. Path rds file output results saved. denom Numeric. denominator value used computation. chunk_size Numeric (Optional). Size chunks process time. Default 1000. threshold_mb Numeric (Optional). Memory threshold (MB) manage processing. Default 2000. tf_use_single Logical. Whether use single precision TensorFlow calculations. Defaults FALSE. verbose Logical. TRUE, logs detailed information execution. Default TRUE. solve_chunk_size Integer. Chunk size solve_and_multiply Python function. Default 50L. solve_max_attempts Integer. Maximum number attempts run solve crossprod internal function run_crossprod_solve. Default 5L. lf_commands_only Logical. TRUE, returns command run Python script. Default FALSE.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/run_crossprod_solve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"run_crossprod_solve — run_crossprod_solve","text":"Returns path_out successful. Returns NULL attempts fail.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/run_crossprod_solve.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"run_crossprod_solve — run_crossprod_solve","text":"function checks existence required input files Python executable specified virtual environment. Executes Python script using system2. Verifies output file validity using ecokit::check_data. Retries 3 times output invalid. Generates detailed logs verbose set TRUE.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/run_crossprod_solve.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"run_crossprod_solve — run_crossprod_solve","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/soil_density_process.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve and project soil bulk density data — soil_density_process","title":"Retrieve and project soil bulk density data — soil_density_process","text":"Downloads projects SoilGrids' bulk density fine earth fraction (bdod) layers specified depth intervals. See details. function project original data different depths intervals modelling reference grid.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/soil_density_process.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve and project soil bulk density data — soil_density_process","text":"","code":"soil_density_process(depths = NULL, env_file = \".env\", n_cores = 6L)"},{"path":"https://biodt.github.io/IASDT.R/reference/soil_density_process.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve and project soil bulk density data — soil_density_process","text":"depths Character vector depth intervals (e.g. c(\"0-5\",\"5-15\")). NULL (default) valid depths processed. Valid depths SoilGrids BDOD standard horizons (cm): \"0-5\", \"5-15\", \"15-30\", \"30-60\", \"60-100\", \"100-200\". depth string omit trailing cm. env_file Character. Path environment file containing paths data sources. Defaults .env. n_cores Integer. Number CPU cores use parallel processing. Default: 6.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/soil_density_process.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve and project soil bulk density data — soil_density_process","text":"(Invisibly) path RData file containing processed soil bulk density data.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/soil_density_process.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Retrieve and project soil bulk density data — soil_density_process","text":"SoilGrids: https://soilgrids.org SoilGrids250m 2.0 - Bulk density Poggio et al. (2021): SoilGrids 2.0: producing soil information globe quantified spatial uncertainty. Soil. 10.5194/soil-7-217-2021","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/soil_density_process.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Retrieve and project soil bulk density data — soil_density_process","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/trim_hmsc.html","id":null,"dir":"Reference","previous_headings":"","what":"Trim an Hmsc Model Object by Removing Specified Components — trim_hmsc","title":"Trim an Hmsc Model Object by Removing Specified Components — trim_hmsc","text":"Removes specified components Hmsc model object one time. used keep smaller Hmsc object, containing needed list items. useful situations fitted Hmsc model large (e.g. GBs) downstream computations implemented parallel.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/trim_hmsc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trim an Hmsc Model Object by Removing Specified Components — trim_hmsc","text":"","code":"trim_hmsc(model, names_to_remove = NULL)"},{"path":"https://biodt.github.io/IASDT.R/reference/trim_hmsc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trim an Hmsc Model Object by Removing Specified Components — trim_hmsc","text":"model object class Hmsc, containing fitted Hmsc model. Must NULL. names_to_remove character vector specifying names components remove model (e.g., \"postList\", \"Y\"). NULL, trimming implemented. Must non-empty match names model.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/trim_hmsc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trim an Hmsc Model Object by Removing Specified Components — trim_hmsc","text":"Hmsc object specified components removed.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/trim_hmsc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Trim an Hmsc Model Object by Removing Specified Components — trim_hmsc","text":"function used reduce memory footprint Hmsc models removing unnecessary components processing. converts model plain list avoid S3 method overhead, removes components iteratively, restores Hmsc class. iterative approach slower vectorized subsetting may preferred specific use cases requiring step--step removal. simple trimming list items; e.g. model[c(\"postList\", X\")] <- NULL took comparably much time trimming done inside functions jobs submitted via SLURM","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/trim_hmsc.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Trim an Hmsc Model Object by Removing Specified Components — trim_hmsc","text":"Ahmed El-Gabbas","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/trim_hmsc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Trim an Hmsc Model Object by Removing Specified Components — trim_hmsc","text":"","code":"require(Hmsc)  (model <- Hmsc::TD$m) #> Hmsc object with 50 sampling units, 4 species, 3 covariates, 3 traits and 2 random levels #> Posterior MCMC sampling with 2 chains each with 100 samples, thin 1 and transient 50   (trimmed_model <- trim_hmsc(   model, names_to_remove = c(\"postList\", \"rL\", \"ranLevels\"))) #> Hmsc object with 50 sampling units, 4 species, 3 covariates, 3 traits and 2 random levels #> Posterior MCMC sampling with 100 samples, thin 1 and transient 50   setdiff(names(model), names(trimmed_model)) #> [1] \"ranLevels\" \"rL\"        \"postList\"   lobstr::obj_size(model) #> 937.01 kB lobstr::obj_size(trimmed_model) #> 44.06 kB"},{"path":"https://biodt.github.io/IASDT.R/reference/wetness_index_process.html","id":null,"dir":"Reference","previous_headings":"","what":"Download and Process Topographic Wetness Index Data — wetness_index_process","title":"Download and Process Topographic Wetness Index Data — wetness_index_process","text":"Downloads, extracts, processes global topographic wetness index data 30 arc-second resolution (Title & Bemmels, 2018). function checks existing processed data, downloads required dataset necessary, extracts relevant TIFF file, re-projects masks match reference grid, saves processed raster TIFF RData formats.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/wetness_index_process.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download and Process Topographic Wetness Index Data — wetness_index_process","text":"","code":"wetness_index_process(env_file = \".env\")"},{"path":"https://biodt.github.io/IASDT.R/reference/wetness_index_process.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download and Process Topographic Wetness Index Data — wetness_index_process","text":"env_file Character. Path environment file containing paths data sources. Defaults .env.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/wetness_index_process.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download and Process Topographic Wetness Index Data — wetness_index_process","text":"(Invisibly) path processed wetness index RData file.","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/wetness_index_process.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Download and Process Topographic Wetness Index Data — wetness_index_process","text":"Title & Bemmels (2018): ENVIREM: expanded set bioclimatic topographic variables increases flexibility improves performance ecological niche modeling. Ecography. 10.1111/ecog.02880 ENVIREM: ENVIronmental Rasters Ecological Modeling version 1.0","code":""},{"path":"https://biodt.github.io/IASDT.R/reference/wetness_index_process.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Download and Process Topographic Wetness Index Data — wetness_index_process","text":"Ahmed El-Gabbas","code":""}]
