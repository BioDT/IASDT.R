---
title: "Model Fitting and Post-Processing Workflow"
output: html_document
author: "Ahmed El-Gabbas"
date: "`r format(Sys.time(), '%d %B, %Y')`"
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
execute: 
  eval: false
format:
  html:
    embed-resources: true
    code-overflow: wrap
    page-layout: full
    # toc: false
    # toc-title: "AAAA"
    # toc-expand: true
    # toc-depth: 1
    # number-sections: true
    link-external-icon: true
    link-external-newwindow: true
---

```{css, echo=FALSE}
.ShortBlock {
height: 400px;
width: 100%;
}

summary {
color: blue;
padding-left: 40px;
}

ff {
  background-color: #def5ff;
  color: black;
  font-style: oblique;
}

cc {
  background-color: #f0f7eb;
  color: black;
  font-style: oblique;
}

```

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE,
  warning = FALSE,
  message = FALSE,
  dev = "ragg_png",
  dpi = 300,
  tidy = "styler",
  out.width = "100%",
  fig.show = "hold")

library(kableExtra)
library(knitr)
library(tibble)
```

# Overview

This document outlines the workflow (currently under development) for modelling the distribution and level of invasion (species richness) of invasive alien plant species (IAS) in Europe. The IAS prototype Digital Twin (`IAS-pDT`) is a component of the European project <a href="https:/www.biodt.eu/" target="_blank">BioDT</a>, which aims to develop a Digital Twin for biodiversity in Europe. For more information on `IAS-pDT`, see <a href="https://doi.org/10.3897/rio.10.e124579" target="_blank"><img src="https://img.shields.io/badge/10.3897/rio.10.e124579-blue" alt="DOI: 10.3897/rio.10.e124579"/></a>. The full workflow of the `IAS-pDT` can be found here <a href="https://doi.org/10.5281/zenodo.14834385" target="_blank"><img src="https://img.shields.io/badge/10.5281/zenodo.14834385-blue" alt="DOI: 10.5281/zenodo.14834385"/></a>.

The `IAS-pDT` utilizes the `IASDT.R` R package (<a href="https://doi.org/10.5281/zenodo.14756907" target="_blank"><img src="https://img.shields.io/badge/10.5281/zenodo.14756907-blue" alt="DOI: 10.5281/zenodo.14756907"/></a>) to execute a workflow for model fitting, post-processing, and data preparation for the <a href="https://app.biodt.eu/app/biodtshiny" target="_blank">Shiny app</a>. Model outputs from the `IAS-pDT` are publicly accessible to end user and stakeholders via an <a href="http://opendap.biodt.eu/ias-pdt/" target="_blank">OPeNDAP</a> server, and prediction maps can be retrieved directly in R using the `IASDT.R` package (currently in preparation).


Models are fitted using the Hierarchical Modelling of Species Communities (<a href="https://www.helsinki.fi/en/researchgroups/statistical-ecology/software/hmsc" target="_blank">Hmsc</a>) R package. Due to the high computational cost of fitting these spatial models at the European scale, we employ the `Hmsc-HPC` extension (<a href="https://doi.org/10.1371/journal.pcbi.1011914" target="_blank"><img src="https://img.shields.io/badge/10.1371/journal.pcbi.1011914-blue" alt="DOI: 10.1371/journal.pcbi.1011914"/></a>) to fit models on GPUs. Models are fitted at the habitat level, meaning that a separate model is created for each habitat type, and only IAS species associated with that habitat are included. We adopted the habitat classification by Py≈°ek et al. (<a href="https://doi.org/10.23855/preslia.2022.447" target="_blank"><img src="https://img.shields.io/badge/10.23855/preslia.2022.447-blue" alt="DOI: 10.23855/preslia.2022.447"/></a>) and fitted eight distinct models (see table below).

```{r echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
DT <- tibble::tribble(
  ~Abbreviation, ~`Habitat Type`, ~Description,
  "1", "Forests", "closed vegetation dominated by deciduous or evergreen trees",
  "2", "Open forests", "woodlands with canopy openings created by environmental stress or disturbance, including forest edges",
  "3", "Scrub", "shrublands maintained by environmental stress (aridity) or disturbance",
  "4a", "Natural grasslands", "grasslands maintained by climate (aridity, unevenly distributed precipitation), herbivores or environmental stress (aridity, instability or toxicity of substrate)",
  "4b", "Human-maintained grasslands", "grasslands dependent on regular human-induced management (mowing, grazing by livestock, artificial burning)",
  "10", "Wetlands", "sites with the permanent or seasonal influence of moisture, ranging from oligotrophic to eutrophic",
  "12a", "Ruderal habitats", "anthropogenically disturbed or eutrophicated sites, where the anthropogenic disturbance orfertilization is typically a side-product and not the aim of the management",
  "12b", "Agricultural habitats", "synanthropic habitats directly associated with growing of agricultural products, thus dependent on specific type of management (ploughing, fertilization)"
)

DT %>% 
  knitr::kable(format = "html")  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"), font_size = 16, full_width = TRUE)
```

For each habitat type, predictions of individual species and species richness are generated across multiple climate scenarios: five CMIP6 climate models &times; three Shared Socioeconomic Pathways (SSPs) &times; three future time frames, totaling 45 scenarios (see CHELSA <a href="https://chelsa-climate.org/wp-admin/download-page/CHELSA_tech_specification_V2.pdf" target="_blank">technical specifications</a> for details). Additionally, we aggregate future climate model outputs to provide an ensemble prediction for each SSP scenario and time frame combination.

```{r echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
DT <- tibble::tribble(
  ~`Climate model`, ~Institution,
  "mpi-esm1-2-hr", "Max Planck Institute for Meteorology, Germany",
  "ipsl-cm6a-lr", "Institut Pierre Simon Laplace, France",
  "ukesm1-0-ll", "Met Office Hadley Centre, UK",
  "gfdl-esm4", "National Oceanic and Atmospheric Administration, USA",
  "mri-esm2-0", "Meteorological Research Institute, Japan")

DT %>% 
  knitr::kable(format = "html")  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"), font_size = 16, full_width = TRUE)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
DT <- tibble::tribble(
  ~"Shared Socioeconomic Pathway", ~Description,
  "ssp126", "SSP1-RCP2.6 climate as simulated by the GCMs",
  "ssp370", "SSP3-RCP7 climate as simulated by the GCMs",
  "ssp585", "SSP5-RCP8.5 climate as simulated by the GCMs")

DT %>% 
  knitr::kable(format = "html")  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"), font_size = 16, full_width = TRUE)
```


---

# Abiotic predictors

> Under preparation

---

# Species-distribution data

> Under preparation

---

# Model input data

The primary function for preparing model-fitting data and initial models is **`Mod_Prep4HPC()`**. It organizes data for each habitat-specific model into separate directories (e.g., <ff>datasets/processed/model_fitting/HabX</ff>, where <i>X</i> denotes a habitat type listed above). Internally, it invokes other functions for specific tasks:

- `Mod_PrepData()`: prepare modelling input data. Important arguments are:

```{r echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
DT <- tibble::tribble(
  ~"Scenario", ~Description,
  "<cc>Hab_Abb</cc>", "abbreviation of a single habitat type to be modelled",
  "<cc>Path_Model</cc>", "directory path to where all model files will be saved",
  "<cc>MinEffortsSp</cc>", "minimum number of vascular plant species per grid cell required for inclusion in model fitting. This represents the count of *all* vascular plant species (including native ones) recorded in GBIF across Europe. The count is calculated during the sampling effort preparation step (`Efforts_Process()`). This argument helps exclude grid cells with minimal sampling efforts",
  "<cc>ExcludeCult</cc>", "whether to exclude countries with cultivated or casual observations for each species",
  "<cc>ExcludeZeroHabitat</cc>", "whether to exclude grid cells with zero habitat coverage of the respective habitat type",
  "<cc>PresPerSpecies</cc>", "minimum number of presence grid cells for a species to be included in the models. This is calculated after discarding grid cells with low sampling efforts (<cc>MinEffortsSp</cc>), zero percentage habitat coverage (<cc>ExcludeZeroHabitat</cc>), and countries with cultivated or casual observations (<cc>ExcludeCult</cc>)")

DT %>% 
  knitr::kable(col.names = NULL, format = "html", escape = FALSE)  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"), 
    font_size = 16, full_width = TRUE) %>%
  add_indent(positions = seq_len(nrow(DT)), level_of_indent = 1)
```

- `Mod_GetCV()`: prepare and visualize spatial-block cross-validation options:
  - <i>CV_Dist</i>: the size of cross-validation blocks is determined by the <cc>CV_NGrids</cc> argument
  - <i>CV_Large</i>: the study area is split into large blocks, as determined by the <cc>CV_NR</cc> and <cc>CV_NC</cc> arguments

```{r echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
DT <- tibble::tribble(
  ~"Scenario", ~Description,
  "<cc>CV_NFolds</cc>", "number of cross-validation folds",
  "<cc>CV_NGrids</cc>", "number of grid cells in both directions used in the <i>CV_Dist</i> cross-validation strategy. The default value is 20, representing blocks of 20&times;20 grid cell each",
  "<cc>CV_NR</cc> / <br/><cc>CV_NC</cc>", "number of rows and columns used in the <i>CV_Large</i> cross-validation strategy, in which the study area is divided into large blocks. if <cc>CV_NR</cc> = <cc>CV_NC</cc> = 2 (default), will divide the study area into four large blocks, split at the median coordinates")

DT %>% 
  knitr::kable(col.names = NULL, format = "html", escape = FALSE)  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"), 
    font_size = 16, full_width = TRUE) %>%
  add_indent(positions = seq_len(nrow(DT)), level_of_indent = 1)
```

- `Mod_PrepKnots()`: prepare and visualize knots for the Gaussian Predictive Process (GPP) models (<a href="https://doi.org/10.1371/10.1002/ecy.2929" target="_blank"><img src="https://img.shields.io/badge/10.1371/10.1002/ecy.2929-blue" alt="DOI: 10.1371/10.1002/ecy.2929"/></a>)

```{r echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
DT <- tibble::tribble(
  ~"Scenario", ~Description,
  "<cc>GPP</cc>", "whether to fit spatial random effect using GPP",
  "<cc>GPP_Dists</cc>", "distance in kilometers used both for the spacing between knots and the minimum allowable distance between a knot and the nearest sampling point",
  "<cc>GPP_Plot</cc>", "whether to plot the coordinates of the sampling units and the knots",
  "<cc>MinLF</cc> / <br/><cc>MaxLF</cc>", "minimum and maximum number of latent factors to be used",
  "<cc>Alphapw</cc>", "prior for the alpha parameter")

DT %>% 
  knitr::kable(col.names = NULL, format = "html", escape = FALSE)  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"), font_size = 16, full_width = TRUE) %>%
  add_indent(positions = seq_len(nrow(DT)), level_of_indent = 1)
```

- `Mod_SLURM()`: prepare SLURM scripts for model fitting on GPU using the `Hmsc-HPC` extension. 

```{r echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
DT <- tibble::tribble(
  ~"Scenario", ~Description,
  "<cc>JobName</cc>", "name of the job",
  "<cc>ntasks</cc>", "number of tasks",
  "<cc>CpusPerTask</cc> / <br/><cc>GpusPerNode</cc>", "number of CPUs/GPUs per node",
  "<cc>MemPerCpu</cc>", "memory per CPU",
  "<cc>Time</cc>", "duration for which the job should run",
  "<cc>Partition</cc>", "HPC partition name",
  "<cc>NumArrayJobs</cc>", "number of jobs per SLURM script")

DT %>% 
  knitr::kable(col.names = NULL, format = "html", escape = FALSE)  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"), font_size = 16, full_width = TRUE) %>%
  add_indent(positions = seq_len(nrow(DT)), level_of_indent = 1)
```

Other arguments are:

- selection of predictors: 
```{r echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
DT <- tibble::tribble(
  ~"Scenario", ~Description,
  "<cc>BioVars<cc>", "names of CHELSA variables to be used in the model",
  "<cc>QuadraticVars<cc>", "names of variables, for which quadratic terms are used",
  "<cc>EffortsAsPredictor<cc>", "whether to include the (log<sub>10</sub>) sampling efforts as predictor",
  "<cc>RoadRailAsPredictor<cc>", "whether to include the (log<sub>10</sub>) sum of road and railway intensity as predictor",
  "<cc>HabAsPredictor<cc>", "whether to include the (log<sub>10</sub>) percentage coverage of respective habitat type per grid cell as predictor",
  "<cc>RiversAsPredictor<cc>", "whether to include the (log<sub>10</sub>) total length of rivers per grid cell as predictor")

DT %>% 
  knitr::kable(col.names = NULL, format = "html", escape = FALSE)  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"), font_size = 16, full_width = TRUE)
```

- model fitting options
```{r echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
DT <- tibble::tribble(
  ~"Scenario", ~Description,
  "<cc>NChains</cc>", "number of MCMC chains",
  "<cc>thin</cc>", "thinning value(s) in MCMC sampling",
  "<cc>samples</cc>", "number of MCMC samples per chain",
  "<cc>transientFactor</cc>", "transient multiplication factor. The value of transient will equal the multiplication of transientFactor and thin",
  "<cc>verbose</cc>", "interval at which MCMC sampling progress is reported",
  "<cc>Precision</cc>", "floating-point precision mode for Hmsc-HPC sampling")

DT %>% 
  knitr::kable(col.names = NULL, format = "html", escape = FALSE)  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"), font_size = 16, full_width = TRUE)
```
- <cc>NspPerGrid</cc>: minimum number of IAS per grid cell for a grid cell to be included in the analysis
- <cc>ModelCountry</cc>: fit the model for a specific country or countries
- whether or not to use phylogenetic trees: <cc>PhyloTree</cc> and <cc>NoPhyloTree</cc>
- <cc>Path_Hmsc</cc>: directory path to `Hmsc-HPC` extension installation
- <cc>PrepSLURM</cc>: whether to prepare SLURM script for model fitting on GPU via `Mod_SLURM()`

-----

# Model fitting on GPUs

Once the model input data and initial models are prepared, the next step is model fitting on GPUs. For each habitat type, the `Mod_Prep4HPC()` function generates:

- python commands (<ff>Commands2Fit.txt</ff>) for fitting model chains for all model variants on GPU (one line per chain)

<details>
<summary>Example model fitting commands</summary>
```{bash, file = "Example_python.txt"}
```
</details>

- one or more SLURM script files (<ff>Bash_Fit.slurm</ff>) that can be used to submit all model-fitting commands (<ff>Commands2Fit.txt</ff>) as a batch job on an HPC system

<details>
<summary>Example SLURM script</summary>
```{bash, file = "Example_SLURM.txt"}
#| class-source: ShortBlock
```
</details>

Batch job(s) for model fitting can be submitted using the `sbatch` command; e.g. 
```{bash}
sbatch datasets/processed/model_fitting/Hab1/Bash_Fit.slurm
```

----

# Post-processing of fitted models

Post-processing of fitted models is carried out in multiple steps, alternating computations between CPU and GPU.

## Step 1: CPU

The **`Mod_Postprocess_1_CPU()`** function serves as the initial post-processing step. For each habitat type, it automates the following tasks:

```{r echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
DT <- tibble::tribble(
  ~"Scenario", ~Description,
  "`Mod_SLURM_Refit()`", "check unsuccessful models",
  "`Mod_Merge_Chains()`", "merge MCMC chains and save fitted model and coda objects to <i>qs2</i> or <i>RData</i> files",
  "`Convergence_Plot_All()`", "visualize convergence of the `rho`, `alpha`, `omega`, and `beta` parameters across all fitted model variants. This function is unnecessary if only one model variant is used. It compares convergence for models fitted with e.g., multiple thinning values, both with and without phylogenetic relationships, or with varying distances between GPP knots",
  "`Convergence_Plot()`", "visualize model convergence of the `rho`, `alpha`, `omega`, and `beta` parameters of the selected model. This is similar to `Convergence_Plot_All`, but providing detailed overview of the convergence of a single selected model",
  "`PlotGelman()`", "visualize Gelman-Rubin-Brooks diagnostics of the selected model",
  "`Mod_Summary()`", "extract and save model summary",
  "`Mod_Heatmap_Beta()`", "plotting `beta` parameters as heatmaps",
  "`Mod_Heatmap_Omega()`", "plotting the `omega` parameter (residual associations) as heatmaps",
  "`Mod_CV_Fit()`", "prepare input data for model fitting on spatial-block cross-validation")

  
DT %>% 
  knitr::kable(col.names = NULL, format = "html", escape = FALSE)  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"), font_size = 16, full_width = TRUE)
```

Previous attempts to prepare response curve data, predict at new sites, and compute variance partitioning failed when using R and CPU on the UFZ Windows server and LUMI HPC. Consequently, these data are prepared on GPUs. `Mod_Postprocess_1_CPU()` function invokes the following functions:

```{r echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
DT <- tibble::tribble(
  ~"Scenario", ~Description,
  "`RespCurv_PrepData()`", "predict latent factors for response curves",
  "`Predict_Maps()`", "predict latent factors at new sampling units",
  "`VarPar_Compute()`", "compute variance partitioning")

DT %>% 
  knitr::kable(col.names = NULL, format = "html", escape = FALSE)  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"), font_size = 16, full_width = TRUE)
```

<br/>Following running `Mod_Postprocess_1_CPU()` for all habitat types, the **`Mod_Prep_TF()`** function prepares batch scripts for GPU computations of *all* habitat types: predicting latent factor predictions for response curves and new sampling units and for computing variance partitioning.


- predict latent factors
  - `RespCurv_PrepData()` inherits the core functionalities of `Hmsc::constructGradient()` and `Hmsc::plotGradient()`, while adding capabilities to prepare data for response curves on GPUs [<cc>LF_Commands_Only = TRUE</cc>]. To predict at the mean coordinate (see <i>coordinates</i> argument of `Hmsc::constructGradient()`), latent factor predictions must be computed at that coordinate, a memory-intensive task using `Hmsc::predictLatentFactor()`. The function exports execution commands to <ff>LF_RC_Commands_&#42;.txt</ff> files(s) on the main directory, along with many <i>&#42;.qs2</i> and <i>&#42;.feather</i> data files in the <ff>TEMP_Pred</ff> directory.
  - Similarly, `Predict_Maps()` [<cc>LF_Only = TRUE, LF_Commands_Only = TRUE</cc>] exports execution commands to <ff>LF_NewSites_Commands_&#42;.txt</ff> files(s) on the main directory, along with many <i>&#42;.qs2</i> and <i>&#42;.feather</i> data files in the <ff>TEMP_Pred</ff> directory.
  - `Mod_Prep_TF()` matches all files with the pattern <cc>&#x5e;LF_NewSites_Commands_.+.txt|&#x5e;LF_RC_Commands_.+txt</cc> (both for response curves and latent factor predictions) and split their contents into multiple scripts at the <ff>TF_postprocess</ff> directory for processing as a batch job. The function prepares a SLURM script for latent factor predictions (<ff>LF_SLURM.slurm</ff>).

<details>
<summary>Example <ff>LF_SLURM.slurm</ff> file</summary>
```{bash, file = "Example_LF_SLURM.slurm"}
#| class-source: ShortBlock
```
</details>

- `VarPar_Compute()` function exports files for processing variance partitioning on GPU to the <ff>TEMP_VP</ff> directory: <ff>VP_A_Command.txt</ff>, <ff>VP_F_Command.txt</ff>, <ff>VP_mu_Command.txt</ff>, and <ff>VP_SLURM.slurm</ff>. `Mod_Prep_TF()` scans for files matching the pattern <cc>VP_.+Command.txt</cc> for different habitat types and consolidates them into a single file (<ff>TF_postprocess/VP_Commands.txt</ff>). Then, it prepares a SLURM script, <ff>TF_postprocess/VP_SLURM.slurm</ff>, for variance partitioning computations.

<details>
<summary>Example <ff>Example_VP_Commands.txt</ff> file</summary>
```{bash, file = "Example_VP_Commands.txt"}
#| class-source: ShortBlock
```
</details>

<details>
<summary>Example <ff>VP_SLURM.slurm</ff> file</summary>
```{bash, file = "Example_VP_SLURM.slurm"}
#| class-source: ShortBlock
```
</details>

<br/>

------

## Step 2: GPU

- Computations for latent factor predictions and variance partitioning are performed on GPUs. Batch jobs can be submitted using the `sbatch` command.

```{bash}
sbatch datasets/processed/model_fitting/TF_postprocess/VP_SLURM.slurm
sbatch datasets/processed/model_fitting/TF_postprocess/LF_SLURM.slurm
```

- cross-validated models are fitted by submitting respective SLURM commands --- [In preparation]

```{bash}
source datasets/processed/model_fitting/HabX/Model_Fitting_CV/CV_Bash_Fit.slurm
```

------

<br/>

## Step 3: CPU

The **`Mod_Postprocess_2_CPU()`** function continues the analysis pipeline for post-processing Hmsc models on the CPU. It automates the following tasks:

```{r echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
DT <- tibble::tribble(
  ~"Scenario", ~Description,
  " `RespCurv_PrepData()`,<br/>`RespCurv_PlotSR()`,<br/>`RespCurv_PlotSp()`,<br/>`RespCurv_PlotSpAll()`", "process and visualize response curves",
  "`Predict_Maps()`", "- predict habitat suitability across different climate options\n- compute model internal evaluation (explanatory power)\n- prepare maps for the shiny app",
  "`Mod_Predict_Plot()`", "visualize species & species richness predictions as JPEG",
  "`Mod_Plot_LF()`", "visualize spatial variation in site loadings of HMSC Models as JPEG",
  "`VarPar_Compute()`,<br/>`VarPar_Plot()`", "process and visualize variance partitioning",
  "`Mod_Eval_Plot()`", "visualize model internal evaluation (explanatory power)",
    "`In preparation...`", "initiate post-processing of fitted cross-validated models and prepare commands for latent factor predictions on GPU")

DT %>% 
  knitr::kable(col.names = NULL, format = "html", escape = FALSE)  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"), font_size = 16, full_width = TRUE)
```

------

## Step 4: GPU

Predicting latent factors for cross-validated models on GPUs --- [In preparation]

------

## Step 5: CPU

Evaluating model performance for cross-validated models --- [In preparation]
