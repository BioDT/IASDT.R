---
title: "IAS-pDT modelling workflow — 5. Model post-processing"
output: html_document
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
execute: 
  eval: false
format:
  html:
    self_contained: true
    mode: selfcontained
    embed-resources: true
    code-overflow: wrap
    page-layout: full
    # toc-expand: true
    # toc-depth: 1
    # number-sections: true
    link-external-icon: true
    link-external-newwindow: true
vignette: >
  %\VignetteIndexEntry{IAS-pDT modelling workflow — 5. Model post-processing}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r, results='asis', eval=TRUE, echo=FALSE}
cat('
<script>
function toggleScript(id) {
  var content = document.getElementById(id);
  if (content.style.display === "none") {
    content.style.display = "block";
  } else {
    content.style.display = "none";
  }
}
</script>
')
```

```{r, include = FALSE, eval=TRUE}
library(kableExtra)
library(knitr)
library(tibble)
knitr::opts_chunk$set(
  collapse = TRUE, comment = "#>", eval = FALSE, warning = FALSE,
  message = FALSE, dev = "ragg_png", dpi = 300, tidy = "styler",
  out.width = "100%", fig.show = "hold", echo = FALSE)

collapsible_script_section <- function(
    file_path, button_label, section_id, 
    button_class = "ShortBlockButton", 
    div_class = "ShortBlock") {
  if (!file.exists(file_path)) {
    script_content <- "Error: File not found."
  } else {
    script_content <- readLines(file_path) %>% 
      paste(collapse = "\n") %>% 
      htmltools::htmlEscape()
  }
  html_output <- paste0(
    '<button onclick="toggleScript(\'', section_id, '\')" class="', button_class, '">', button_label, '</button>\n',
    '<div id="', section_id, '" class="', div_class, '" style="display: none;">\n',
    '  <pre><code class="bash">', script_content, '</code></pre>\n',
    '</div>\n'
  )
  cat(html_output)
}
```

```{r, eval=TRUE, results="asis"}
c("<style>", readLines("style.css"), "</style>") %>% 
  cat(sep = "\n")
```

<br/>

Post-processing of fitted models within the `IAS-pDT` workflow is conducted across multiple steps, leveraging both CPU and GPU computations to optimize performance and address memory constraints.

<br/>

## Step 1: CPU

The **`mod_postprocess_1_CPU()`** function initiates the post-processing phase for each habitat type, automating the following tasks:

```{r echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
DT <- tibble::tribble(
  ~Scenario, ~Description,
  "`mod_SLURM_refit()`", "check for unsuccessful model fits",
  "`mod_merge_chains()`", "merge MCMC chains and saves fitted model and coda objects to <i>.qs2</i> or <i>.RData</i> files",
  "`convergence_plot_all()`", "visualize convergence of `rho`, `alpha`, `omega`, and `beta` parameters across all model variants; unnecessary for a single variant, this function compares convergence across models with varying thinning values, both with and without phylogenetic relationships, or GPP knot distances",
  "`convergence_plot()`", "convergence of `rho`, `alpha`, `omega`, and `beta` parameters for a selected model, offering a detailed view compared to `convergence_plot_all()`",
  "`plot_gelman()`", "visualize Gelman-Rubin-Brooks diagnostics for the selected model",
  "`mod_summary()`", "extract and save a summary of the model",
  "`mod_heatmap_beta()`", "generate heatmaps of the `beta` parameters",
  "`mod_heatmap_omega()`", "generates heatmaps of the `omega` parameter (residual species associations)",
  "`mod_CV_fit()`", "prepare input data for spatial-block cross-validation model fitting") %>% 
  dplyr::mutate(Scenario = paste0("<cc>", Scenario, "</cc><tab0>"))

DT %>% 
  knitr::kable(col.names = NULL, format = "html", escape = FALSE)  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"), 
    font_size = 16, full_width = TRUE) %>%
  kableExtra::add_indent(
    positions = seq_len(nrow(DT)), level_of_indent = 2) %>% 
  kableExtra::column_spec(
    column = 1, extra_css = "white-space: nowrap;")
```

<br/><hr class="hr2"><br/>

### Computationally intensive tasks that are offloaded to GPU

Previous attempts to prepare response curve data, predict at new sites, and compute variance partitioning using R on CPUs (UFZ Windows server and LUMI HPC) were hindered by memory limitations. Consequently, these tasks are offloaded to GPU-based computations using Python and TensorFlow. The `mod_postprocess_1_CPU()` function invokes the following sub-functions to generate commands for GPU execution:

```{r echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
DT <- tibble::tribble(
  ~Scenario, ~Description,
  "`resp_curv_prepare_data()`", "prepare data for predicting latent factors for response curves",
  "`predict_maps()`", "prepare data for predicting latent factors at new sampling units",
  "`variance_partitioning_compute()`", "prepare data for computing variance partitioning") %>% 
  dplyr::mutate(Scenario = paste0("<cc>", Scenario, "</cc><tab0>"))

DT %>% 
  knitr::kable(col.names = NULL, format = "html", escape = FALSE)  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"),
    font_size = 16, full_width = TRUE) %>%
  kableExtra::add_indent(
    positions = seq_len(nrow(DT)), level_of_indent = 2) %>% 
  kableExtra::column_spec(
    column = 1, extra_css = "white-space: nowrap;")
```

<br/><hr class="hr2"><br/>

### Prepare commands for GPU computations

> <u><i>Predicting latent factors:</i></u>

- Latent factor predictions for response curves and new sampling units are executed via a <i>TensorFlow</i> script located at <a href="https://github.com/BioDT/IASDT.R/blob/main/inst/crossprod_solve.py" target="_blank" class="ll">inst/crossprod_solve.py</a>.
- For these tasks, the respective R functions export numerous <i>.qs2</i> and <i>.feather</i> data files to the <ff>TEMP_Pred</ff> subdirectory, essential for GPU computations. They also generate execution commands saved as <ff>LF_RC_Commands_*.txt</ff> (for response curves) and <ff>LF_NewSites_Commands_*.txt</ff> (for new sites).

```{r, results='asis', eval=TRUE, echo=FALSE}
collapsible_script_section(
  "Example_LF_RC_Commands.txt", 
  "&#187;&#187; Example LF_RC_Commands.txt file", 
  "scriptContentNS1")
```

<br/>

```{r, results='asis', eval=TRUE, echo=FALSE}
collapsible_script_section(
  "Example_LF_NewSites_Commands.txt", 
  "&#187;&#187; Example LF_NewSites_Commands.txt file", 
  "scriptContentNS2")
```

<br/><br/>

- Response curves
  - `resp_curv_prepare_data()` extends `Hmsc::constructGradient()` and `Hmsc::plotGradient()`, enabling GPU-based response curve data preparation when <cc>`LF_commands_only = TRUE`</cc>.
  - To predict at mean coordinates (per the <i>coordinates</i> argument of `Hmsc::constructGradient()`), latent factor predictions—typically memory-intensive with `Hmsc::predictLatentFactor()`—are computed on GPUs.
- Predicting at new sites
  - `predict_maps()` prepares GPU computations for new site predictions when <cc>`LF_only = TRUE`</cc> and <cc>`LF_commands_only = TRUE`</cc>.
  
<br/><br/>

> <u><i>Computing variance partitioning:</i></u>

- Variance partitioning computations on GPUs are executed using TensorFlow scripts at <a href="https://github.com/BioDT/IASDT.R/blob/main/inst/VP_geta.py" target="_blank" class="ll">inst/VP_geta.py</a>, <a href="https://github.com/BioDT/IASDT.R/blob/main/inst/VP_getf.py" target="_blank" class="ll">inst/VP_getf.py</a>, and <a href="https://github.com/BioDT/IASDT.R/blob/main/inst/VP_gemu.py" target="_blank" class="ll">inst/VP_gemu.py</a>. The functionality of these scripts was taken from `Hmsc::computeVariancePartitioning()` (see the `geta`, `getf`, and `gemu` internal functions identified inside of the computeVariancePartitioning R function).
- `variance_partitioning_compute()` exports required files to the <ff>TEMP_VP</ff> subdirectory, including numerous <i>.qs2</i> and <i>.feather</i> files, and generates execution commands saved as <ff>VP_A_Command.txt</ff>, <ff>VP_F_Command.txt</ff>, and <ff>VP_mu_Command.txt</ff>.

<br/><br/>

### Combining commands for GPU computations

After executing `mod_postprocess_1_CPU()` for all habitat types, the **`mod_prepare_TF()`** function consolidates batch scripts for GPU computations across all habitat types:

- It aggregates script files containing commands for response curves and latent factor predictions, splitting them into multiple scripts (<ff>TF_Chunk_*.txt</ff>) for batch processing, and generates a SLURM script (<ff>LF_SLURM.slurm</ff>) for latent factor predictions.

```{r, results='asis', eval=TRUE, echo=FALSE}
collapsible_script_section(
  "Example_TF_Chunk.txt", 
  "&#187;&#187; Example TF_Chunk_*.txt file", 
  "scriptContentNS3")
```

<br/>

```{r, results='asis', eval=TRUE, echo=FALSE}
collapsible_script_section(
  "Example_LF_SLURM.slurm", 
  "&#187;&#187; Example LF_SLURM.slurm file", 
  "scriptContentNS4")
```

<br/>

- It consolidates variance partitioning command files into a single <ff>VP_Commands.txt</ff> and prepares a SLURM script (<ff>VP_SLURM.slurm</ff>) for variance partitioning computations.

```{r, results='asis', eval=TRUE, echo=FALSE}
collapsible_script_section(
  "Example_VP_Commands.txt", 
  "&#187;&#187; Example VP_Commands.txt file", 
  "scriptContentNS5")
```

<br/>

```{r, results='asis', eval=TRUE, echo=FALSE}
collapsible_script_section(
  "Example_VP_SLURM.slurm", 
  "&#187;&#187; Example VP_SLURM.slurm file", 
  "scriptContentNS6")
```

<hr class="hr1">

## Step 2: GPU

Latent factor predictions and variance partitioning are computed on GPUs. Batch jobs can be submitted using the `sbatch` command:

```{bash, echo = TRUE}
sbatch datasets/processed/model_fitting/Mod_Q_Hab_TF/VP_SLURM.slurm
sbatch datasets/processed/model_fitting/Mod_Q_Hab_TF/LF_SLURM.slurm
```

Cross-validated models are fitted by submitting corresponding SLURM commands (*in preparation*):

```{bash, echo = TRUE}
source datasets/processed/model_fitting/HabX/Model_Fitting_CV/CV_Bash_Fit.slurm
```

<hr class="hr1">

## Step 3: CPU

The `mod_postprocess_2_CPU()` function advances the post-processing pipeline for HMSC models on the CPU, automating the following tasks:

```{r echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
DT <- tibble::tribble(
  ~Function, ~Description,
  " `resp_curv_prepare_data()`,<br/>`resp_curv_plot_SR()`,<br/>`resp_curv_plot_species()`,<br/>`resp_curv_plot_species_all()`", "continue processing and visualizing response curves",
  "`predict_maps()`", "predict habitat suitability across climate scenarios, compute model explanatory power (internal evaluation), and prepare maps for the Shiny app",
  "`plot_prediction()`", "visualize species and species richness predictions as JPEG images",
  "`plot_latent_factor()`", "visualize spatial variation in site loadings of HMSC models as JPEG images",
  "`variance_partitioning_compute()`,<br/>`variance_partitioning_plot()`", "continue processing and visualize variance partitioning",
  "`plot_evaluation()`", "visualize explanatory power (internal model evaluation)",
  "*In preparation...*", "initiate post-processing of fitted cross-validated models and prepare GPU commands for latent factor predictions") %>% 
  dplyr::mutate(Function = paste0("<cc>", Function, "</cc><tab0>"))

DT %>% 
  knitr::kable(col.names = NULL, format = "html", escape = FALSE)  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"), 
    font_size = 16, full_width = TRUE) %>%
  kableExtra::add_indent(
    positions = seq_len(nrow(DT)), level_of_indent = 1) %>% 
  kableExtra::column_spec(
    column = 1, extra_css = "white-space: nowrap;")
```

<hr class="hr1">

## Step 4: GPU

Predicting latent factors for cross-validated models on GPUs (*in preparation*).

<hr class="hr1">

## Step 5: CPU

Evaluating the performance of cross-validated models (*in preparation*).

<hr class="hr1">

<span style="font-size: 1.2em; line-height: 0.8;">
<b>Previous articles:</b><br/>
<tab>&#8608;<tab><a href="workflow_1_overview.html" class="ll">1. Overview</a><br/>
<tab>&#8608;<tab><a href="workflow_2_abiotic_data.html" class="ll">2. Processing abiotic data</a><br/>
<tab>&#8608;<tab><a href="workflow_3_biotic_data.html" class="ll">3. Processing biotic data</a><br/>
<tab>&#8608;<tab><a href="workflow_4_model_fitting.html" class="ll">4. Model fitting</a><br/>
</span>
