---
title: "IASDT modelling workflow — 5. Model post-processing"
output: html_document
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
execute: 
  eval: false
format:
  html:
    self_contained: true
    mode: selfcontained
    embed-resources: true
    code-overflow: wrap
    code-link: true
    page-layout: full
    # toc-expand: true
    # toc-depth: 1
    # number-sections: true
    link-external-icon: true
    link-external-newwindow: true
vignette: >
  %\VignetteIndexEntry{IASDT modelling workflow — 5. Model post-processing}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r}
#| results = "asis",
#| eval = TRUE,
#| echo = FALSE
cat('
<script>
function toggleScript(id) {
  var content = document.getElementById(id);
  if (content.style.display === "none") {
    content.style.display = "block";
  } else {
    content.style.display = "none";
  }
}
</script>
')
```

```{r}
#| include = FALSE,
#| eval = TRUE
library(kableExtra)
library(knitr)
library(tibble)
knitr::opts_chunk$set(
  collapse = TRUE, comment = "#>", eval = FALSE, warning = FALSE,
  message = FALSE, dev = "ragg_png", dpi = 300, tidy = "styler",
  out.width = "100%", fig.show = "hold", echo = FALSE)

collapsible_script_section <- function(
    file_path, button_label, section_id, 
    button_class = "ShortBlockButton", 
    div_class = "ShortBlock") {
  if (!file.exists(file_path)) {
    script_content <- "Error: File not found."
  } else {
    script_content <- readLines(file_path) %>% 
      paste(collapse = "\n") %>% 
      htmltools::htmlEscape()
  }
  html_output <- paste0(
    '<button onclick="toggleScript(\'', section_id, '\')" class="', button_class, '">', button_label, '</button>\n',
    '<div id="', section_id, '" class="', div_class, '" style="display: none;">\n',
    '  <pre><code class="bash">', script_content, '</code></pre>\n',
    '</div>\n'
  )
  cat(html_output)
}
```

```{r}
#| eval = TRUE,
#| results = "asis"
c("<style>", readLines("style.css"), "</style>") %>% 
  cat(sep = "\n")
```

<br/>

Post-processing of fitted models in the `IASDT` workflow involves multiple steps, utilising both CPU and GPU computations to optimize performance and manage memory constraints effectively.

<br/>

## Step 1: CPU

The **`mod_postprocess_1_CPU()`** function begins the post-processing phase for each habitat type by automating the following tasks:


```{r}
#| echo = FALSE,
#| message = FALSE,
#| warning = FALSE,
#| eval = TRUE
DT <- tibble::tribble(
  ~R_function, ~Description,
  "`mod_SLURM_refit()`", "checks for unsuccessful model fits.",
  "`mod_merge_chains()`", "merges MCMC chains and saves the fitted model and coda objects to `.qs2` or `.RData` files.",
  "`convergence_plot_all()`", "visualises the convergence of `rho`, `alpha`, `omega`, and `beta` parameters across all model variants. This function is particularly useful for comparing convergence across models with different thinning values, with and without phylogenetic relationships, or varying GPP knot distances. It is unnecessary when only a single model variant is considered.",
  "`convergence_plot()`", "displays the convergence of `rho`, `alpha`, `omega`, and `beta` parameters for a selected model, providing a more detailed view compared to `convergence_plot_all()`.",
  "`plot_gelman()`", "visualises the Gelman-Rubin-Brooks diagnostics for the selected model.",
  "`mod_summary()`", "extracts and saves a summary of the model.",
  "`mod_heatmap_beta()`", "generates heatmaps of the `beta` parameters.",
  "`mod_heatmap_omega()`", "generates heatmaps of the `omega` parameter, which represents residual species associations.",
  "`mod_CV_fit()`", "prepares the necessary data for fitting cross-validated models. 
  <ul><li>output files are saved in the `Model_Fitting_CV` subdirectory.</li><li>the type of cross-validation strategy is controlled by the `CV_name` argument, which defaults to both `CV_Dist` and `CV_Large`.</li><li>unfitted model objects are saved in the `Model_Init` subdirectory.</li><li>commands for model fitting are saved as text files, with a separate file for each cross-validation strategy (e.g., `Commands2Fit_CV_Dist.txt`, `Commands2Fit_CV_Large.txt`).</li><li>model fitting commands are submitted as batch jobs using SLURM scripts, with a separate script for each strategy (e.g., `CV_Bash_Fit_Dist.slurm`, `CV_Bash_Fit_Large.slurm`).</li></ul>") %>% 
  dplyr::mutate(R_function = paste0("<cc>", R_function, "</cc><tab0>"))

DT %>% 
  knitr::kable(col.names = NULL, format = "html", escape = FALSE)  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"), 
    font_size = 16, full_width = TRUE) %>%
  kableExtra::add_indent(
    positions = seq_len(nrow(DT)), level_of_indent = 2) %>% 
  kableExtra::column_spec(
    column = 1, extra_css = "white-space: nowrap;")
```


<br/><hr class="hr2"><br/>

### Computationally intensive tasks offloaded to GPU

Previous attempts to prepare response curve data, predict at new sites, and compute variance partitioning using R on CPUs (such as the UFZ Windows server and LUMI HPC) were limited by memory constraints. As a result, these tasks are now offloaded to GPU-based computations using `Python` and `TensorFlow`. The `mod_postprocess_1_CPU()` function calls the following sub-functions to generate the necessary commands for GPU execution:


```{r}
#| echo = FALSE,
#| message = FALSE,
#| warning = FALSE,
#| eval = TRUE
DT <- tibble::tribble(
  ~R_function, ~Description,
  "`resp_curv_prepare_data()`", "prepares data for predicting latent factors for response curves",
  "`predict_maps()`", "prepares data for predicting latent factors at new sampling units",
  "`variance_partitioning_compute()`", "prepares data for computing variance partitioning") %>% 
  dplyr::mutate(R_function = paste0("<cc>", R_function, "</cc><tab0>"))

DT %>% 
  knitr::kable(col.names = NULL, format = "html", escape = FALSE)  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"),
    font_size = 16, full_width = TRUE) %>%
  kableExtra::add_indent(
    positions = seq_len(nrow(DT)), level_of_indent = 2) %>% 
  kableExtra::column_spec(
    column = 1, extra_css = "white-space: nowrap;")
```

<br/><hr class="hr2"><br/>

### Preparing commands for GPU computations

> <u><i>Predicting latent factors:</i></u>

- Predictions of latent factors for response curves and new sampling units are performed using a `TensorFlow` script located at <a href="https://github.com/BioDT/IASDT.R/blob/main/inst/crossprod_solve.py" target="_blank" class="ll">inst/crossprod_solve.py</a>.
- For these tasks, the corresponding R functions export multiple `.qs2` and `.feather` data files to the <ff>TEMP_Pred</ff> subdirectory, which are essential for GPU computations. Additionally, they generate execution commands saved as <ff>LF_RC_Commands_*.txt</ff> (for response curves) and <ff>LF_NewSites_Commands_*.txt</ff> (for new sites).


```{r}
#| results = "asis",
#| eval = TRUE,
#| echo = FALSE
collapsible_script_section(
  "Example_LF_RC_Commands.txt", 
  "&#187;&#187; Example LF_RC_Commands.txt file", 
  "scriptContentNS1")
```

<br/>

```{r}
#| results = "asis",
#| eval = TRUE,
#| echo = FALSE
collapsible_script_section(
  "Example_LF_NewSites_Commands.txt", 
  "&#187;&#187; Example LF_NewSites_Commands.txt file", 
  "scriptContentNS2")
```

<br/><br/>

- Response curves
  - `resp_curv_prepare_data()` extends the functionality of `Hmsc::constructGradient()` and `Hmsc::plotGradient()` by enabling the preparation of response curve data on GPUs when `LF_commands_only = TRUE`.
  - For predictions at mean coordinates (as specified by the `coordinates` argument in `Hmsc::constructGradient()`), latent factor predictions — which are typically memory-intensive when using `Hmsc::predictLatentFactor()` — are computed on GPUs.
- Predicting at new sites
  - `predict_maps()` sets up GPU computations for predictions at new sites when both `LF_only = TRUE` and `LF_commands_only = TRUE`.
  
<br/><br/>


> <u><i>Computing variance partitioning:</i></u>

- Variance partitioning computations are performed on GPUs using `TensorFlow` scripts located at <a href="https://github.com/BioDT/IASDT.R/blob/main/inst/VP_geta.py" target="_blank" class="ll">inst/VP_geta.py</a>, <a href="https://github.com/BioDT/IASDT.R/blob/main/inst/VP_getf.py" target="_blank" class="ll">inst/VP_getf.py</a>, and <a href="https://github.com/BioDT/IASDT.R/blob/main/inst/VP_gemu.py" target="_blank" class="ll">inst/VP_gemu.py</a>. These scripts implement functionality derived from `Hmsc::computeVariancePartitioning()`, specifically the internal functions `geta`, `getf`, and `gemu`.
- The `variance_partitioning_compute()` function exports the necessary files to the <ff>TEMP_VP</ff> subdirectory, including numerous `.qs2` and `.feather` files. It also generates execution commands saved as <ff>VP_A_Command.txt</ff>, <ff>VP_F_Command.txt</ff>, and <ff>VP_mu_Command.txt</ff>.

<br/><br/>

### Combining commands for GPU computations

Once `mod_postprocess_1_CPU()` has been executed for all habitat types, the **`mod_prepare_TF()`** function consolidates the batch scripts for GPU computations across all habitat types:

- It aggregates the script files that contain commands for response curves and latent factor predictions, splitting them into multiple scripts (<ff>TF_Chunk_*.txt</ff>) for batch processing. Additionally, it generates a SLURM script (<ff>LF_SLURM.slurm</ff>) for executing the latent factor predictions.

```{r}
#| results = "asis",
#| eval = TRUE,
#| echo = FALSE
collapsible_script_section(
  "Example_TF_Chunk.txt", 
  "&#187;&#187; Example TF_Chunk_*.txt file", 
  "scriptContentNS3")
```

<br/>

```{r}
#| results = "asis",
#| eval = TRUE,
#| echo = FALSE
collapsible_script_section(
  "Example_LF_SLURM.slurm", 
  "&#187;&#187; Example LF_SLURM.slurm file", 
  "scriptContentNS4")
```

<br/>

- It combines the variance partitioning command files into a single <ff>VP_Commands.txt</ff> file and prepares a SLURM script (<ff>VP_SLURM.slurm</ff>) for the variance partitioning computations.

```{r}
#| results = "asis",
#| eval = TRUE,
#| echo = FALSE
collapsible_script_section(
  "Example_VP_Commands.txt", 
  "&#187;&#187; Example VP_Commands.txt file", 
  "scriptContentNS5")
```

<br/>

```{r}
#| results = "asis",
#| eval = TRUE,
#| echo = FALSE
collapsible_script_section(
  "Example_VP_SLURM.slurm", 
  "&#187;&#187; Example VP_SLURM.slurm file", 
  "scriptContentNS6")
```

<hr class="hr1">

## Step 2: GPU

In this step, latent factor predictions and variance partitioning are computed on GPUs. The batch jobs for these computations can be submitted using the `sbatch` command:

```{bash}
#| echo = TRUE
# Submit SLURM jobs for variance partitioning and latent factor predictions
sbatch datasets/processed/model_fitting/Mod_Q_Hab_TF/VP_SLURM.slurm
sbatch datasets/processed/model_fitting/Mod_Q_Hab_TF/LF_SLURM.slurm
```

Additionally, cross-validated models are fitted by submitting the corresponding SLURM scripts for each cross-validation strategy:

```{bash}
#| echo = TRUE
# Submit SLURM jobs for cross-validated model fitting
#
# cross-validation method "CV_Dist"
sbatch datasets/processed/model_fitting/HabX/Model_Fitting_CV/CV_Bash_Fit_Dist.slurm
# cross-validation method "CV_Large"
sbatch datasets/processed/model_fitting/HabX/Model_Fitting_CV/CV_Bash_Fit_Large.slurm
```

<hr class="hr1">

## Step 3: CPU

To continue the post-processing of the fitted models on CPUs, two functions need to be executed:


### 1. **`mod_postprocess_2_CPU()`**

The `mod_postprocess_2_CPU()` function progresses the post-processing pipeline for HMSC models on the CPU by automating the following tasks:

```{r}
#| echo = FALSE,
#| message = FALSE,
#| warning = FALSE,
#| eval = TRUE

DT <- tibble::tribble(
  ~R_function, ~Description,
  "`resp_curv_prepare_data()`,<br/>`resp_curv_plot_SR()`,<br/>`resp_curv_plot_species()`,<br/>`resp_curv_plot_species_all()`", "continues the processing and visualisation of response curves.",
  "`predict_maps()`", "<ul><li>predicts habitat suitability across various climate scenarios.</li><li>computes the model's explanatory power (internal evaluation without cross-validation) using four metrics: AUC (area under the ROC curve), RMSE (root mean square error), continuous Boyce index, and Tjur R<sup>2</sup>.</li><li>prepares GeoTIFF maps for free access via the `IASDT` <a href='http://opendap.biodt.eu/ias-pdt/' target='_blank' class='ll'>OPeNDAP server</a> and the `IASDT` <a href='https://app.biodt.eu/app/biodtshiny' target='_blank' class='ll'>Shiny app</a>.</li></ul>",
  "`plot_prediction()`", "visualises predictions of species and species richness as JPEG images.",
  "`plot_latent_factor()`", "visualises the spatial variation in site loadings of HMSC models as JPEG images.",
  "`variance_partitioning_compute()`,<br/>`variance_partitioning_plot()`", "continues the processing and visualisation of variance partitioning.",
  "`plot_evaluation()`", "visualises the explanatory power of the model (internal evaluation).") %>% 
  dplyr::mutate(R_function = paste0("<cc>", R_function, "</cc><tab0>"))

DT %>% 
  knitr::kable(col.names = NULL, format = "html", escape = FALSE)  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"), 
    font_size = 16, full_width = TRUE) %>%
  kableExtra::add_indent(
    positions = seq_len(nrow(DT)), level_of_indent = 1) %>% 
  kableExtra::column_spec(
    column = 1, extra_css = "white-space: nowrap;")
```

<br/>

### 2. **`mod_postprocess_CV_1_CPU()`**

The `mod_postprocess_CV_1_CPU()` function begins the post-processing of cross-validated models on the CPU by automating the following tasks:

```{r}
#| echo = FALSE,
#| message = FALSE,
#| warning = FALSE,
#| eval = TRUE
DT <- tibble::tribble(
  ~R_function, ~Description,
  "`mod_merge_chains_CV()`", "merges fitted cross-validated model chains into `Hmsc` model objects and saves them to disk.",
  "`predict_maps_CV()`", "prepares scripts for predicting latent factors for each cross-validation strategy at new sampling units (evaluation folds). The arguments `LF_only` and `LF_commands_only` are set to `TRUE` to prepare only the necessary script files.") %>% 
  dplyr::mutate(R_function = paste0("<cc>", R_function, "</cc><tab0>"))

DT %>% 
  knitr::kable(col.names = NULL, format = "html", escape = FALSE)  %>%
  kableExtra::kable_styling(
    c("basic", "condensed", "hover"), 
    font_size = 16, full_width = TRUE) %>%
  kableExtra::add_indent(
    positions = seq_len(nrow(DT)), level_of_indent = 1) %>% 
  kableExtra::column_spec(
    column = 1, extra_css = "white-space: nowrap;")
```

Once `predict_maps_CV()` has been completed, the function combines the computation commands into multiple text script files (<ff>TF_Chunk_*.txt</ff>) in each model's <ff>Model_Fitting_CV/LF_TF_commands</ff> subdirectory. These scripts need to be executed on GPUs using a single batch job submitted via a SLURM script in the same directory, <ff>LF_SLURM.slurm</ff>.

<hr class="hr1">

## Step 4: GPU

In this step, the computation of latent factors for cross-validated models is performed on GPUs using SLURM scripts.

```{bash}
#| echo = TRUE
sbatch datasets/processed/model_fitting/HabX/Model_Fitting_CV/LF_TF_commands/LF_SLURM.slurm
```

<hr class="hr1">

## Step 5: CPU

The final step of the post-processing pipeline is carried out on CPUs using the `mod_postprocess_CV_2_CPU()` function. This function automates the following tasks:

- Predicting habitat suitability at the testing cross-validation folds using the `predict_maps_CV()` function.
- Computing the model's predictive power (using spatially independent testing data) with the same function, based on four metrics: AUC (area under the ROC curve); RMSE (root mean square error); continuous Boyce index; and Tjur R<sup>2</sup>.
- Plotting the model's evaluation, including:
  - Predictive power values for each evaluation metric versus the mean number of testing presences
  - Explanatory versus predictive power for each evaluation metric
  
<hr class="hr1">

<span style="font-size: 1.2em; line-height: 0.8;">
<b>Previous articles:</b><br/>
<tab>&#8608;<tab><a href="workflow_1_overview.html" class="ll">1. Overview</a><br/>
<tab>&#8608;<tab><a href="workflow_2_abiotic_data.html" class="ll">2. Processing abiotic data</a><br/>
<tab>&#8608;<tab><a href="workflow_3_biotic_data.html" class="ll">3. Processing biotic data</a><br/>
<tab>&#8608;<tab><a href="workflow_4_model_fitting.html" class="ll">4. Model fitting</a><br/>
</span>
